translation_unit: /* Copyright 2015 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "tensorflow/c/c_api.h"

#include <algorithm>
#include <cstring>
#include <limits>
#include <memory>
#include <optional>
#include <unordered_set>
#include <utility>
#include <vector>

#include "absl/strings/match.h"
// Required for IS_MOBILE_PLATFORM
#include "tensorflow/core/platform/platform.h"  // NOLINT

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
#include "tensorflow/c/experimental/filesystem/modular_filesystem.h"
#include "tensorflow/cc/framework/gradients.h"
#include "tensorflow/cc/framework/ops.h"
#include "tensorflow/cc/framework/scope_internal.h"
#include "tensorflow/cc/ops/while_loop.h"
#include "tensorflow/cc/saved_model/loader.h"
#include "tensorflow/core/distributed_runtime/server_lib.h"
#include "tensorflow/core/framework/logging.h"
#include "tensorflow/core/framework/op_gen_lib.h"
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
#include "tensorflow/c/c_api_internal.h"
#include "tensorflow/c/tf_buffer_internal.h"
#include "tensorflow/c/tf_status_internal.h"
#include "tensorflow/c/tf_tensor.h"
#include "tensorflow/core/common_runtime/device_mgr.h"
#include "tensorflow/core/common_runtime/eval_const_tensor.h"
#include "tensorflow/core/common_runtime/graph_constructor.h"
#include "tensorflow/core/common_runtime/shape_refiner.h"
#include "tensorflow/core/config/flag_defs.h"
#include "tensorflow/core/config/flags.h"
#include "tensorflow/core/framework/allocation_description.pb.h"
#include "tensorflow/core/framework/attr_value_util.h"
#include "tensorflow/core/framework/cpp_shape_inference.pb.h"
#include "tensorflow/core/framework/full_type.pb.h"
#include "tensorflow/core/framework/kernel_def.pb.h"
#include "tensorflow/core/framework/log_memory.h"
#include "tensorflow/core/framework/node_def_util.h"
#include "tensorflow/core/framework/op_kernel.h"
#include "tensorflow/core/framework/partial_tensor_shape.h"
#include "tensorflow/core/framework/shape_inference.h"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/framework/tensor.pb.h"  // NOLINT
#include "tensorflow/core/framework/tensor_shape.h"
#include "tensorflow/core/framework/tensor_shape.pb.h"
#include "tensorflow/core/framework/types.h"
#include "tensorflow/core/framework/versions.pb.h"
#include "tensorflow/core/graph/graph.h"
#include "tensorflow/core/graph/node_builder.h"
#include "tensorflow/core/graph/validate.h"
#include "tensorflow/core/lib/gtl/array_slice.h"
#include "tensorflow/core/platform/coding.h"
#include "tensorflow/core/platform/errors.h"
#include "tensorflow/core/platform/mem.h"
#include "tensorflow/core/platform/mutex.h"
#include "tensorflow/core/platform/protobuf.h"
#include "tensorflow/core/platform/status.h"
#include "tensorflow/core/platform/str_util.h"
#include "tensorflow/core/platform/strcat.h"
#include "tensorflow/core/platform/stringpiece.h"
#include "tensorflow/core/platform/thread_annotations.h"
#include "tensorflow/core/platform/types.h"
#include "tensorflow/core/public/release_version.h"
#include "tensorflow/core/public/session.h"

// The implementation below is at the top level instead of the
// brain namespace because we are defining 'extern "C"' functions.
using tensorflow::AttrValueMap;
using tensorflow::DataType;
using tensorflow::ExtendSessionGraphHelper;
using tensorflow::FullTypeDef;
using tensorflow::Graph;
using tensorflow::GraphDef;
using tensorflow::mutex_lock;
using tensorflow::NameRangeMap;
using tensorflow::NameRangesForNode;
using tensorflow::NewSession;
using tensorflow::Node;
using tensorflow::NodeBuilder;
using tensorflow::OpDef;
using tensorflow::PartialTensorShape;
using tensorflow::RunMetadata;
using tensorflow::RunOptions;
using tensorflow::Session;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::Tensor;
using tensorflow::TensorId;
using tensorflow::TensorShapeProto;
using tensorflow::VersionDef;
using tensorflow::errors::FailedPrecondition;
using tensorflow::errors::InvalidArgument;
using tensorflow::errors::OutOfRange;
using tensorflow::gtl::ArraySlice;
using tensorflow::strings::StrCat;

extern "C" {

// --------------------------------------------------------------------------
const char* TF_Version() { return TF_VERSION_STRING; }

// --------------------------------------------------------------------------

// --------------------------------------------------------------------------
TF_SessionOptions* TF_NewSessionOptions() {
  TF_SessionOptions* out = new TF_SessionOptions;
  // Disable optimizations for static graph to allow calls to Session::Extend.
  out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
  return out;
}
void TF_DeleteSessionOptions(TF_SessionOptions* opt) { delete opt; }

void TF_SetTarget(TF_SessionOptions* options, const char* target) {
  options->options.target = target;
}

void TF_SetConfig(TF_SessionOptions* options, const void* proto,
                  size_t proto_len, TF_Status* status) {
  if (!options->options.config.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
  // Disable optimizations for static graph to allow calls to Session::Extend.
  options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
}

void TF_TensorFromProto(const TF_Buffer* from, TF_Tensor* to,
                        TF_Status* status) {
  TF_SetStatus(status, TF_OK, "");
  tensorflow::TensorProto from_tensor_proto;
  status->status = BufferToMessage(from, &from_tensor_proto);
  if (!status->status.ok()) {
    return;
  }
  status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto);
}
// --------------------------------------------------------------------------

TF_DeprecatedSession* TF_NewDeprecatedSession(const TF_SessionOptions* opt,
                                              TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}

void TF_CloseDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = s->session->Close();
}

void TF_DeleteDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  delete s->session;
  delete s;
}

void TF_ExtendGraph(TF_DeprecatedSession* s, const void* proto,
                    size_t proto_len, TF_Status* status) {
  GraphDef g;
  if (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  status->status = s->session->Extend(g);
}

}  // end extern "C"

// Reset helper for converting character arrays to string vectors.
static void TF_Reset_Helper(const TF_SessionOptions* opt,
                            const char** containers, int ncontainers,
                            TF_Status* status) {
  std::vector<string> container_names(ncontainers);
  for (int i = 0; i < ncontainers; ++i) {
    container_names[i] = containers[i];
  }

  status->status = Reset(opt->options, container_names);
}

extern "C" {

void TF_Reset(const TF_SessionOptions* opt, const char** containers,
              int ncontainers, TF_Status* status) {
  TF_Reset_Helper(opt, containers, ncontainers, status);
}

}  // end extern "C"

namespace tensorflow {

void RecordMutation(TF_Graph* graph, const TF_Operation& op,
                    const char* mutation_type) {
  // If any session has already run this node_id, mark this session as
  // unrunnable.
  for (auto it : graph->sessions) {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
}

namespace {

// Helper method that creates a shape handle for a shape described by dims.
tensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims) {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}

}  // namespace

void TF_GraphSetOutputHandleShapesAndTypes(TF_Graph* graph, TF_Output output,
                                           int num_shapes_and_types,
                                           const int64_t** shapes,
                                           const int* ranks,
                                           const TF_DataType* types,
                                           TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  auto shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types);
  for (int i = 0; i < num_shapes_and_types; ++i) {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }

  ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec);
}

// Helpers for loading a TensorFlow plugin (a .so file).
absl::Status LoadDynamicLibrary(const char* library_filename, void** result,
                                const void** buf, size_t* len);

// TODO(josh11b,mrry): Change Session to be able to use a Graph*
// directly, instead of requiring us to serialize to a GraphDef and
// call Session::Extend().
bool ExtendSessionGraphHelper(TF_Session* session, TF_Status* status) {
  if (session->graph != nullptr) {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
  return true;
}

}  // namespace tensorflow

static void TF_Run_Setup(int noutputs, TF_Tensor** c_outputs,
                         TF_Status* status) {
  status->status = absl::OkStatus();
  for (int i = 0; i < noutputs; ++i) {
    c_outputs[i] = nullptr;
  }
}

// TF_TensorToTensorV1 decodes a string serialization to DT_RESOURCE.
// In the TFv1 convention, TF_Tensor can hold a string serialization of
// DT_RESOURCE. The string serialization is converted back to a
// ResourceHandle during Session run where the TF_Tensor is converted to a
// Tensor.
// TFv2 does not depend on this conversion. There is no matching
// TF_TensorFromTensorV1 because the conversion to string is performed by the
// python side of Session.
static Status TF_TensorToTensorV1(const TF_Tensor* src, Tensor* dst) {
  Status status = TF_TensorToTensor(src, dst);
  if (!status.ok()) {
    return status;
  }
  if (dst->dtype() == tensorflow::DT_RESOURCE) {
    const auto tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor);

    if (dst->dims() != 0) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
    *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape());
    if (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
    return absl::OkStatus();
  }
  return absl::OkStatus();
}

static bool TF_Run_Inputs(TF_Tensor* const* c_inputs,
                          std::vector<std::pair<string, Tensor>>* input_pairs,
                          TF_Status* status) {
  const int ninputs = input_pairs->size();
  for (int i = 0; i < ninputs; ++i) {
    status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second);
    if (!status->status.ok()) return false;
  }
  return true;
}

// Create an empty tensor of type 'dtype'. 'shape' can be arbitrary, but has to
// result in a zero-sized tensor.
static TF_Tensor* EmptyTensor(TF_DataType dtype,
                              const tensorflow::TensorShape& shape) {
  static char empty;
  int64_t nelems = 1;
  std::vector<int64_t> dims;
  dims.reserve(shape.dims());
  for (int i = 0; i < shape.dims(); ++i) {
    dims.push_back(shape.dim_size(i));
    nelems *= shape.dim_size(i);
  }
  CHECK_EQ(nelems, 0);
  return TF_NewTensor(
      dtype, reinterpret_cast<const int64_t*>(dims.data()), shape.dims(),
      reinterpret_cast<void*>(&empty), 0, [](void*, size_t, void*) {}, nullptr);
}

static void TF_Run_Helper(
    Session* session, const char* handle, const TF_Buffer* run_options,
    // Input tensors
    const std::vector<std::pair<string, Tensor>>& input_pairs,
    // Output tensors
    const std::vector<string>& output_tensor_names, TF_Tensor** c_outputs,
    // Target nodes
    const std::vector<string>& target_oper_names, TF_Buffer* run_metadata,
    TF_Status* status) {
  const int noutputs = output_tensor_names.size();
  std::vector<Tensor> outputs(noutputs);
  Status result;

  if (handle == nullptr) {
    RunOptions run_options_proto;
    if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)) {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
    if (run_metadata != nullptr && run_metadata->data != nullptr) {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }

    RunMetadata run_metadata_proto;
    result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto);

    // Serialize back to upstream client, who now owns the new buffer
    if (run_metadata != nullptr) {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
  } else {
    // NOTE(zongheng): PRun does not support RunOptions yet.
    result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
  }
  if (!result.ok()) {
    status->status = result;
    return;
  }

  // Store results in c_outputs[]
  for (int i = 0; i < noutputs; ++i) {
    const Tensor& src = outputs[i];
    if (!src.IsInitialized() || src.NumElements() == 0) {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
    c_outputs[i] = TF_TensorFromTensor(src, &status->status);
    if (!status->status.ok()) return;
  }
}

extern "C" {

void TF_Run(TF_DeprecatedSession* s, const TF_Buffer* run_options,
            // Input tensors
            const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
            // Output tensors
            const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
            // Target nodes
            const char** c_target_oper_names, int ntargets,
            TF_Buffer* run_metadata, TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status);
}

void TF_PRunSetup(TF_DeprecatedSession* s,
                  // Input names
                  const char** c_input_names, int ninputs,
                  // Output names
                  const char** c_output_names, int noutputs,
                  // Target nodes
                  const char** c_target_oper_names, int ntargets,
                  const char** handle, TF_Status* status) {
  *handle = nullptr;

  std::vector<string> input_names(ninputs);
  std::vector<string> output_names(noutputs);
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = c_input_names[i];
  }
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  string new_handle;
  status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}

void TF_PRun(TF_DeprecatedSession* s, const char* handle,
             // Input tensors
             const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
             // Output tensors
             const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
             // Target nodes
             const char** c_target_oper_names, int ntargets,
             TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status);
}

TF_Library* TF_LoadLibrary(const char* library_filename, TF_Status* status) {
  TF_Library* lib_handle = new TF_Library;
  status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length);
  if (!status->status.ok()) {
    delete lib_handle;
    return nullptr;
  }
  return lib_handle;
}

TF_Buffer TF_GetOpList(TF_Library* lib_handle) { return lib_handle->op_list; }

void TF_DeleteLibraryHandle(TF_Library* lib_handle) {
  if (lib_handle == nullptr) return;
  tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data));
  delete lib_handle;
}

TF_Buffer* TF_GetAllOpList() {
  std::vector<tensorflow::OpDef> op_defs;
  tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs);
  tensorflow::OpList op_list;
  for (const auto& op : op_defs) {
    *(op_list.add_op()) = op;
  }
  TF_Buffer* ret = TF_NewBuffer();
  TF_CHECK_OK(MessageToBuffer(op_list, ret));
  return ret;
}

// --------------------------------------------------------------------------
// ListDevices & SessionListDevices API

void TF_DeleteDeviceList(TF_DeviceList* list) { delete list; }

TF_DeviceList* TF_SessionListDevices(TF_Session* session, TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}

TF_DeviceList* TF_DeprecatedSessionListDevices(TF_DeprecatedSession* session,
                                               TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}

int TF_DeviceListCount(const TF_DeviceList* list) {
  return list->response.size();
}

#define TF_DEVICELIST_METHOD(return_type, method_name, accessor, err_val) \
  return_type method_name(const TF_DeviceList* list, const int index,     \
                          TF_Status* status) {                            \
    if (list == nullptr) {                                                \
      status->status = InvalidArgument("list is null!");                  \
      return err_val;                                                     \
    }                                                                     \
    if (index < 0 || index >= list->response.size()) {                    \
      status->status = InvalidArgument("index out of bounds");            \
      return err_val;                                                     \
    }                                                                     \
    status->status = ::tensorflow::OkStatus();                            \
    return list->response[index].accessor;                                \
  }

TF_DEVICELIST_METHOD(const char*, TF_DeviceListName, name().c_str(), nullptr);
TF_DEVICELIST_METHOD(const char*, TF_DeviceListType, device_type().c_str(),
                     nullptr);
TF_DEVICELIST_METHOD(int64_t, TF_DeviceListMemoryBytes, memory_limit(), -1);
TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output) {
  return TensorId(output.oper->node.name(), output.index);
}

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
std::vector<tensorflow::Output> OutputsFromTFOutputs(TF_Output* tf_outputs,
                                                     int n) {
  std::vector<tensorflow::Output> outputs(n);
  for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
  return outputs;
}

void TFOutputsFromOutputs(const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs) {
  for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
}
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

}  // namespace

// Shape functions -----------------------------------------------------------

void TF_GraphSetTensorShape(TF_Graph* graph, TF_Output output,
                            const int64_t* dims, const int num_dims,
                            TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
  tensorflow::shape_inference::ShapeHandle new_shape =
      tensorflow::ShapeHandleFromDims(ic, num_dims, dims);
  status->status = graph->refiner.SetShape(node, output.index, new_shape);
}

int TF_GraphGetTensorNumDims(TF_Graph* graph, TF_Output output,
                             TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return -1;
  }

  tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);

  // Unknown rank means the number of dimensions is -1.
  if (!ic->RankKnown(shape)) {
    return -1;
  }

  return ic->Rank(shape);
}

void TF_GraphGetTensorShape(TF_Graph* graph, TF_Output output, int64_t* dims,
                            int num_dims, TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);

  int rank = -1;
  if (ic->RankKnown(shape)) {
    rank = ic->Rank(shape);
  }

  if (num_dims != rank) {
    status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank);
    return;
  }

  if (num_dims == 0) {
    // Output shape is a scalar.
    return;
  }

  // Rank is greater than 0, so fill in the values, if known, and
  // -1 for unknown values.
  for (int i = 0; i < num_dims; ++i) {
    auto dim = ic->Dim(shape, i);
    int64_t value = -1;
    if (ic->ValueKnown(dim)) {
      value = ic->Value(dim);
    }
    dims[i] = value;
  }
}

// TF_OperationDescription functions ------------------------------------------

extern "C" {

TF_OperationDescription* TF_NewOperationLocked(TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
    TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu) {
  return new TF_OperationDescription(graph, op_type, oper_name);
}

TF_OperationDescription* TF_NewOperation(TF_Graph* graph, const char* op_type,
                                         const char* oper_name) {
  mutex_lock l(graph->mu);
  return TF_NewOperationLocked(graph, op_type, oper_name);
}

void TF_SetDevice(TF_OperationDescription* desc, const char* device) {
  desc->node_builder.Device(device);
}

void TF_AddInput(TF_OperationDescription* desc, TF_Output input) {
  desc->node_builder.Input(&input.oper->node, input.index);
}

void TF_AddInputList(TF_OperationDescription* desc, const TF_Output* inputs,
                     int num_inputs) {
  std::vector<NodeBuilder::NodeOut> input_list;
  input_list.reserve(num_inputs);
  for (int i = 0; i < num_inputs; ++i) {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
  desc->node_builder.Input(input_list);
}

void TF_AddControlInput(TF_OperationDescription* desc, TF_Operation* input) {
  desc->node_builder.ControlInput(&input->node);
}

void TF_ColocateWith(TF_OperationDescription* desc, TF_Operation* op) {
  desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()));
}

void TF_SetAttrString(TF_OperationDescription* desc, const char* attr_name,
                      const void* value, size_t length) {
  absl::string_view s(static_cast<const char*>(value), length);
  desc->node_builder.Attr(attr_name, s);
}

void TF_SetAttrStringList(TF_OperationDescription* desc, const char* attr_name,
                          const void* const* values, const size_t* lengths,
                          int num_values) {
  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  } else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
}

void TF_SetAttrInt(TF_OperationDescription* desc, const char* attr_name,
                   int64_t value) {
  desc->node_builder.Attr(attr_name, static_cast<int64_t>(value));
}

void TF_SetAttrIntList(TF_OperationDescription* desc, const char* attr_name,
                       const int64_t* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values));
}

void TF_SetAttrFloat(TF_OperationDescription* desc, const char* attr_name,
                     float value) {
  desc->node_builder.Attr(attr_name, value);
}

void TF_SetAttrFloatList(TF_OperationDescription* desc, const char* attr_name,
                         const float* values, int num_values) {
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values));
}

void TF_SetAttrBool(TF_OperationDescription* desc, const char* attr_name,
                    unsigned char value) {
  desc->node_builder.Attr(attr_name, static_cast<bool>(value));
}

void TF_SetAttrBoolList(TF_OperationDescription* desc, const char* attr_name,
                        const unsigned char* values, int num_values) {
  std::unique_ptr<bool[]> b(new bool[num_values]);
  for (int i = 0; i < num_values; ++i) {
    b[i] = values[i];
  }
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values));
}

void TF_SetAttrType(TF_OperationDescription* desc, const char* attr_name,
                    TF_DataType value) {
  desc->node_builder.Attr(attr_name, static_cast<DataType>(value));
}

void TF_SetAttrTypeList(TF_OperationDescription* desc, const char* attr_name,
                        const TF_DataType* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values));
}

void TF_SetAttrPlaceholder(TF_OperationDescription* desc, const char* attr_name,
                           const char* placeholder) {
  tensorflow::AttrValue attr_value;
  attr_value.set_placeholder(placeholder);
  desc->node_builder.Attr(attr_name, attr_value);
}

void TF_SetAttrFuncName(TF_OperationDescription* desc, const char* attr_name,
                        const char* value, size_t length) {
  tensorflow::NameAttrList func_name;
  func_name.set_name(string(value, value + length));
  desc->node_builder.Attr(attr_name, func_name);
}

void TF_SetAttrShape(TF_OperationDescription* desc, const char* attr_name,
                     const int64_t* dims, int num_dims) {
  PartialTensorShape shape;
  if (num_dims >= 0) {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
  desc->node_builder.Attr(attr_name, shape);
}

void TF_SetAttrShapeList(TF_OperationDescription* desc, const char* attr_name,
                         const int64_t* const* dims, const int* num_dims,
                         int num_shapes) {
  std::vector<PartialTensorShape> shapes;
  shapes.reserve(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
}

void TF_SetAttrTensorShapeProto(TF_OperationDescription* desc,
                                const char* attr_name, const void* proto,
                                size_t proto_len, TF_Status* status) {
  // shape.ParseFromArray takes an int as length, this function takes size_t,
  // make sure there is no information loss.
  if (proto_len > std::numeric_limits<int>::max()) {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
  TensorShapeProto shape;
  if (shape.ParseFromArray(proto, static_cast<int>(proto_len))) {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  } else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
}

void TF_SetAttrTensorShapeProtoList(TF_OperationDescription* desc,
                                    const char* attr_name,
                                    const void* const* protos,
                                    const size_t* proto_lens, int num_shapes,
                                    TF_Status* status) {
  std::vector<TensorShapeProto> shapes;
  shapes.resize(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
  status->status = absl::OkStatus();
}

void TF_SetAttrTensor(TF_OperationDescription* desc, const char* attr_name,
                      TF_Tensor* value, TF_Status* status) {
  Tensor t;
  status->status = TF_TensorToTensor(value, &t);
  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}

void TF_SetAttrTensorList(TF_OperationDescription* desc, const char* attr_name,
                          TF_Tensor* const* values, int num_values,
                          TF_Status* status) {
  status->status = absl::OkStatus();
  std::vector<Tensor> t;
  t.reserve(num_values);

  for (int i = 0; i < num_values && status->status.ok(); ++i) {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }

  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}

void TF_SetAttrValueProto(TF_OperationDescription* desc, const char* attr_name,
                          const void* proto, size_t proto_len,
                          TF_Status* status) {
  tensorflow::AttrValue attr_value;
  if (!attr_value.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }

  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  } else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }

  status->status = absl::OkStatus();
}

TF_Operation* TF_FinishOperationLocked(TF_OperationDescription* desc,
                                       TF_Status* status)
    TF_EXCLUSIVE_LOCKS_REQUIRED(desc->graph->mu) {
  Node* ret = nullptr;

  if (desc->graph->name_map.count(desc->node_builder.node_name())) {
    status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
  } else {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }

  delete desc;

  return ToOperation(ret);
}

TF_Operation* TF_FinishOperation(TF_OperationDescription* desc,
                                 TF_Status* status) {
  mutex_lock l(desc->graph->mu);
  return TF_FinishOperationLocked(desc, status);
}

// TF_Operation functions
// ----------------------------------------------------------

const char* TF_OperationName(TF_Operation* oper) {
  return oper->node.name().c_str();
}

const char* TF_OperationOpType(TF_Operation* oper) {
  return oper->node.type_string().c_str();
}

const char* TF_OperationDevice(TF_Operation* oper) {
  return oper->node.requested_device().c_str();
}

int TF_OperationNumOutputs(TF_Operation* oper) {
  return oper->node.num_outputs();
}

TF_DataType TF_OperationOutputType(TF_Output oper_out) {
  return static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index));
}

int TF_OperationOutputListLength(TF_Operation* oper, const char* arg_name,
                                 TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}

int TF_OperationNumInputs(TF_Operation* oper) {
  return oper->node.num_inputs();
}

TF_DataType TF_OperationInputType(TF_Input oper_in) {
  return static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index));
}

int TF_OperationInputListLength(TF_Operation* oper, const char* arg_name,
                                TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}

TF_Output TF_OperationInput(TF_Input oper_in) {
  const tensorflow::Edge* edge;
  Status s = oper_in.oper->node.input_edge(oper_in.index, &edge);
  if (!s.ok()) {
    return {nullptr, -1};
  }

  return {ToOperation(edge->src()), edge->src_output()};
}

void TF_OperationAllInputs(TF_Operation* oper, TF_Output* inputs,
                           int max_inputs) {
  for (auto* edge : oper->node.in_edges()) {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
}

int TF_OperationOutputNumConsumers(TF_Output oper_out) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
  return count;
}

int TF_OperationOutputConsumers(TF_Output oper_out, TF_Input* consumers,
                                int max_consumers) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
  return count;
}

int TF_OperationNumControlInputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
  return count;
}

int TF_OperationGetControlInputs(TF_Operation* oper,
                                 TF_Operation** control_inputs,
                                 int max_control_inputs) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
  return count;
}

int TF_OperationNumControlOutputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
  return count;
}

int TF_OperationGetControlOutputs(TF_Operation* oper,
                                  TF_Operation** control_outputs,
                                  int max_control_outputs) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
  return count;
}

TF_AttrMetadata TF_OperationGetAttrMetadata(TF_Operation* oper,
                                            const char* attr_name,
                                            TF_Status* status) {
  TF_AttrMetadata metadata;
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return metadata;
  switch (attr->value_case()) {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
  return metadata;
}

void TF_OperationGetAttrString(TF_Operation* oper, const char* attr_name,
                               void* value, size_t max_length,
                               TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kS) {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
  if (max_length <= 0) {
    return;
  }
  const auto& s = attr->s();
  std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length));
}

void TF_OperationGetAttrStringList(TF_Operation* oper, const char* attr_name,
                                   void** values, size_t* lengths,
                                   int max_values, void* storage,
                                   size_t storage_size, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().s_size());
  char* p = static_cast<char*>(storage);
  for (int i = 0; i < len; ++i) {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
}

#define DEFINE_GETATTR(func, c_type, cpp_type, list_field)                   \
  void func(TF_Operation* oper, const char* attr_name, c_type* value,        \
            TF_Status* status) {                                             \
    cpp_type v;                                                              \
    status->status =                                                         \
        tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &v);          \
    if (!status->status.ok()) return;                                        \
    *value = static_cast<c_type>(v);                                         \
  }                                                                          \
  void func##List(TF_Operation* oper, const char* attr_name, c_type* values, \
                  int max_values, TF_Status* status) {                       \
    const auto* attr = GetAttrValue(oper, attr_name, status);                \
    if (!status->status.ok()) return;                                        \
    if (attr->value_case() != tensorflow::AttrValue::kList) {                \
      status->status =                                                       \
          InvalidArgument("Value for '", attr_name, "' is not a list.");     \
      return;                                                                \
    }                                                                        \
    const auto len = std::min(max_values, attr->list().list_field##_size()); \
    for (int i = 0; i < len; ++i) {                                          \
      values[i] = static_cast<c_type>(attr->list().list_field(i));           \
    }                                                                        \
  }
DEFINE_GETATTR(TF_OperationGetAttrInt, int64_t, int64_t, i);
DEFINE_GETATTR(TF_OperationGetAttrFloat, float, float, f);
DEFINE_GETATTR(TF_OperationGetAttrBool, unsigned char, bool, b);
DEFINE_GETATTR(TF_OperationGetAttrType, TF_DataType, DataType, type);
#undef DEFINE_GETATTR

void TF_OperationGetAttrShape(TF_Operation* oper, const char* attr_name,
                              int64_t* value, int num_dims, TF_Status* status) {
  PartialTensorShape shape;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape);
  if (!status->status.ok()) return;
  auto len = std::min(shape.dims(), num_dims);
  for (int i = 0; i < len; ++i) {
    value[i] = shape.dim_size(i);
  }
}

void TF_OperationGetAttrShapeList(TF_Operation* oper, const char* attr_name,
                                  int64_t** dims, int* num_dims, int num_shapes,
                                  int64_t* storage, int storage_size,
                                  TF_Status* status) {
  std::vector<PartialTensorShape> shapes;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes);
  if (!status->status.ok()) return;
  auto len = std::min(static_cast<int>(shapes.size()), num_shapes);
  int64_t* p = storage;
  int storage_left = storage_size;
  for (int i = 0; i < len; ++i) {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
}

void TF_OperationGetAttrTensorShapeProto(TF_Operation* oper,
                                         const char* attr_name,
                                         TF_Buffer* value, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kShape) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
  status->status = MessageToBuffer(attr->shape(), value);
}

void TF_OperationGetAttrTensorShapeProtoList(TF_Operation* oper,
                                             const char* attr_name,
                                             TF_Buffer** values, int max_values,
                                             TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().shape_size());
  for (int i = 0; i < len; ++i) {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
}

void TF_OperationGetAttrTensor(TF_Operation* oper, const char* attr_name,
                               TF_Tensor** value, TF_Status* status) {
  *value = nullptr;
  Tensor t;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t);
  if (!status->status.ok()) return;
  *value = TF_TensorFromTensor(t, &status->status);
}

void TF_OperationGetAttrTensorList(TF_Operation* oper, const char* attr_name,
                                   TF_Tensor** values, int max_values,
                                   TF_Status* status) {
  std::vector<Tensor> ts;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts);
  if (!status->status.ok()) return;
  const auto len = std::min(max_values, static_cast<int>(ts.size()));
  for (int i = 0; i < len; ++i) {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
}

void TF_OperationGetAttrValueProto(TF_Operation* oper, const char* attr_name,
                                   TF_Buffer* output_attr_value,
                                   TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  status->status = MessageToBuffer(*attr, output_attr_value);
}

int TF_OperationGetNumAttrs(TF_Operation* oper) {
  return oper->node.attrs().size();
}

int TF_OperationGetAttrNameLength(TF_Operation* oper, int i) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
  return -1;
}

void TF_OperationGetAttrName(TF_Operation* oper, int i, char* output,
                             TF_Status* status) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
  status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th");
}

void TF_OperationToNodeDef(TF_Operation* oper, TF_Buffer* output_node_def,
                           TF_Status* status) {
  status->status = MessageToBuffer(oper->node.def(), output_node_def);
}

// TF_Graph functions ---------------------------------------------------------

TF_Graph::TF_Graph()
    : graph(tensorflow::OpRegistry::Global()),
      refiner(graph.versions().producer(), graph.op_registry()),
      delete_requested(false),
      parent(nullptr),
      parent_inputs(nullptr) {
  // Tell the shape refiner to also run shape inference on functions.
  refiner.set_function_library_for_shape_inference(&graph.flib_def());
}

TF_Graph* TF_NewGraph() { return new TF_Graph; }

void TF_DeleteGraph(TF_Graph* g) {
  if (g == nullptr) return;
  g->mu.lock();
  g->delete_requested = true;
  const bool del = g->sessions.empty();
  g->mu.unlock();
  if (del) delete g;
}

TF_Operation* TF_GraphOperationByName(TF_Graph* graph, const char* oper_name) {
  mutex_lock l(graph->mu);
  auto iter = graph->name_map.find(oper_name);
  if (iter == graph->name_map.end()) {
    return nullptr;
  } else {
    return ToOperation(iter->second);
  }
}

TF_Operation* TF_GraphNextOperation(TF_Graph* graph, size_t* pos) {
  if (*pos == 0) {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  } else {
    // Advance to the next node.
    *pos += 1;
  }

  mutex_lock l(graph->mu);
  while (*pos < static_cast<size_t>(graph->graph.num_node_ids())) {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }

  // No more nodes.
  return nullptr;
}

void TF_GraphToGraphDef(TF_Graph* graph, TF_Buffer* output_graph_def,
                        TF_Status* status) {
  GraphDef def;
  {
    mutex_lock l(graph->mu);
    graph->graph.ToGraphDef(&def);
  }
  status->status = MessageToBuffer(def, output_graph_def);
}

void TF_GraphGetOpDef(TF_Graph* graph, const char* op_name,
                      TF_Buffer* output_op_def, TF_Status* status) {
  const OpDef* op_def;
  {
    mutex_lock l(graph->mu);
    status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
    if (!status->status.ok()) return;
  }
  status->status = MessageToBuffer(*op_def, output_op_def);
}

void TF_GraphVersions(TF_Graph* graph, TF_Buffer* output_version_def,
                      TF_Status* status) {
  VersionDef versions;
  {
    mutex_lock l(graph->mu);
    versions = graph->graph.versions();
  }
  status->status = MessageToBuffer(versions, output_version_def);
}

TF_ImportGraphDefOptions* TF_NewImportGraphDefOptions() {
  return new TF_ImportGraphDefOptions;
}
void TF_DeleteImportGraphDefOptions(TF_ImportGraphDefOptions* opts) {
  delete opts;
}
void TF_ImportGraphDefOptionsSetPrefix(TF_ImportGraphDefOptions* opts,
                                       const char* prefix) {
  opts->opts.prefix = prefix;
}
void TF_ImportGraphDefOptionsSetDefaultDevice(TF_ImportGraphDefOptions* opts,
                                              const char* device) {
  opts->opts.default_device = device;
}

void TF_ImportGraphDefOptionsSetUniquifyNames(TF_ImportGraphDefOptions* opts,
                                              unsigned char uniquify_names) {
  opts->opts.uniquify_names = uniquify_names;
}

void TF_ImportGraphDefOptionsSetUniquifyPrefix(TF_ImportGraphDefOptions* opts,
                                               unsigned char uniquify_prefix) {
  opts->opts.uniquify_prefix = uniquify_prefix;
}

void TF_ImportGraphDefOptionsAddInputMapping(TF_ImportGraphDefOptions* opts,
                                             const char* src_name,
                                             int src_index, TF_Output dst) {
  opts->tensor_id_data.push_back(src_name);
  const string& src_name_str = opts->tensor_id_data.back();
  // We don't need to store dst's name in tensor_id_data, since `dst` must
  // outlive the ImportGraphDef call.
  opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst);
}

void TF_ImportGraphDefOptionsRemapControlDependency(
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst) {
  opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot);
}

extern void TF_ImportGraphDefOptionsAddControlDependency(
    TF_ImportGraphDefOptions* opts, TF_Operation* oper) {
  opts->opts.control_dependencies.push_back(oper->node.name());
}

void TF_ImportGraphDefOptionsAddReturnOutput(TF_ImportGraphDefOptions* opts,
                                             const char* oper_name, int index) {
  opts->tensor_id_data.push_back(oper_name);
  const string& oper_name_str = opts->tensor_id_data.back();
  opts->opts.return_tensors.emplace_back(oper_name_str, index);
}

int TF_ImportGraphDefOptionsNumReturnOutputs(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_tensors.size();
}

void TF_ImportGraphDefOptionsAddReturnOperation(TF_ImportGraphDefOptions* opts,
                                                const char* oper_name) {
  opts->opts.return_nodes.push_back(oper_name);
}

int TF_ImportGraphDefOptionsNumReturnOperations(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_nodes.size();
}

void TF_ImportGraphDefResultsReturnOutputs(TF_ImportGraphDefResults* results,
                                           int* num_outputs,
                                           TF_Output** outputs) {
  *num_outputs = results->return_tensors.size();
  *outputs = results->return_tensors.data();
}

void TF_ImportGraphDefResultsReturnOperations(TF_ImportGraphDefResults* results,
                                              int* num_opers,
                                              TF_Operation*** opers) {
  *num_opers = results->return_nodes.size();
  *opers = results->return_nodes.data();
}

void TF_ImportGraphDefResultsMissingUnusedInputMappings(
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes) {
  *num_missing_unused_input_mappings = results->missing_unused_key_names.size();
  *src_names = results->missing_unused_key_names.data();
  *src_indexes = results->missing_unused_key_indexes.data();
}

void TF_DeleteImportGraphDefResults(TF_ImportGraphDefResults* results) {
  delete results;
}

static void GraphImportGraphDefLocked(TF_Graph* graph, const GraphDef& def,
                                      const TF_ImportGraphDefOptions* opts,
                                      TF_ImportGraphDefResults* tf_results,
                                      TF_Status* status)
    TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu) {
  const int last_node_id = graph->graph.num_node_ids();
  tensorflow::ImportGraphDefResults results;
  status->status = tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results);
  if (!status->status.ok()) return;

  // Add new nodes to name_map
  for (int i = last_node_id; i < graph->graph.num_node_ids(); ++i) {
    auto* node = graph->graph.FindNodeId(i);
    if (node != nullptr) graph->name_map[node->name()] = node;
  }

  // Populate return_tensors
  DCHECK(tf_results->return_tensors.empty());
  tf_results->return_tensors.resize(results.return_tensors.size());
  for (int i = 0; i < results.return_tensors.size(); ++i) {
    tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
    tf_results->return_tensors[i].index = results.return_tensors[i].second;
  }

  // Populate return_nodes
  DCHECK(tf_results->return_nodes.empty());
  tf_results->return_nodes.resize(results.return_nodes.size());
  for (int i = 0; i < results.return_nodes.size(); ++i) {
    tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
  }

  // Populate missing unused map keys
  DCHECK(tf_results->missing_unused_key_names.empty());
  DCHECK(tf_results->missing_unused_key_indexes.empty());
  DCHECK(tf_results->missing_unused_key_names_data.empty());

  size_t size = results.missing_unused_input_map_keys.size();
  tf_results->missing_unused_key_names.resize(size);
  tf_results->missing_unused_key_indexes.resize(size);

  for (int i = 0; i < size; ++i) {
    TensorId id = results.missing_unused_input_map_keys[i];
    tf_results->missing_unused_key_names_data.emplace_back(id.first);
    tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
    tf_results->missing_unused_key_indexes[i] = id.second;
  }
}

TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResults(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}

TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResultsNoSerialization(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  const GraphDef* graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data);
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}

void TF_GraphImportGraphDefWithReturnOutputs(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status) {
  if (num_return_outputs != options->opts.return_tensors.size()) {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
  if (num_return_outputs > 0 && return_outputs == nullptr) {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  TF_ImportGraphDefResults results;
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, &results, status);
  DCHECK_EQ(results.return_tensors.size(), num_return_outputs);
  memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output));
}

void TF_GraphImportGraphDef(TF_Graph* graph, const TF_Buffer* graph_def,
                            const TF_ImportGraphDefOptions* options,
                            TF_Status* status) {
  TF_ImportGraphDefResults* results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status);
  TF_DeleteImportGraphDefResults(results);
}

// While loop functions -------------------------------------------------------

namespace {

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

// Creates a placeholder representing an input to the cond or body graph.
// TODO(skyewm): remove these from final graph
bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}

// Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
// `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
// will be prepended to copied node names. `control_deps` are nodes in
// `dst_graph` that the copied `src_graph` nodes will have control dependencies
// on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
// in `dst_graph` will be returned. `return_nodes` must be non-null.
Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}

bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}

bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}

#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void FreeWhileResources(const TF_WhileParams* params) {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}

TF_WhileParams EmptyWhileParams() {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}

}  // namespace

TF_WhileParams TF_NewWhile(TF_Graph* g, TF_Output* inputs, int ninputs,
                           TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return EmptyWhileParams();
#else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
namespace {

// TODO(skyewm): make nodes in while loop unfetchable like in Python version
void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}

}  // namespace
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void TF_FinishWhile(const TF_WhileParams* params, TF_Status* status,
                    TF_Output* outputs) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_AbortWhile(const TF_WhileParams* params) { FreeWhileResources(params); }

void TF_AddGradients(TF_Graph* g, TF_Output* y, int ny, TF_Output* x, int nx,
                     TF_Output* dx, TF_Status* status, TF_Output* dy) {
  TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy);
}

void TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix, TF_Output* y,
                               int ny, TF_Output* x, int nx, TF_Output* dx,
                               TF_Status* status, TF_Output* dy) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

// TF_Session functions ----------------------------------------------

TF_Session::TF_Session(tensorflow::Session* s, TF_Graph* g)
    : session(s), graph(g), last_num_graph_nodes(0), extend_before_run(true) {}

TF_Session* TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  } else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}

TF_Session* TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status) {
// TODO(sjr): Remove the IS_MOBILE_PLATFORM guard. This will require ensuring
// that the tensorflow/cc/saved_model:loader build target is mobile friendly.
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return nullptr;
#else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_CloseSession(TF_Session* s, TF_Status* status) {
  status->status = s->session->Close();
}

void TF_DeleteSession(TF_Session* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  TF_Graph* const graph = s->graph;
  if (graph != nullptr) {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
  delete s->session;
  delete s;
}

void TF_SessionRun(TF_Session* session, const TF_Buffer* run_options,
                   const TF_Output* inputs, TF_Tensor* const* input_values,
                   int ninputs, const TF_Output* outputs,
                   TF_Tensor** output_values, int noutputs,
                   const TF_Operation* const* target_opers, int ntargets,
                   TF_Buffer* run_metadata, TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  // Actually run.
  TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status);
}

void TF_SessionPRunSetup(TF_Session* session, const TF_Output* inputs,
                         int ninputs, const TF_Output* outputs, int noutputs,
                         const TF_Operation* const* target_opers, int ntargets,
                         const char** handle, TF_Status* status) {
  *handle = nullptr;

  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  std::vector<string> input_names(ninputs);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = OutputName(inputs[i]);
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  string new_handle;
  status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}

void TF_DeletePRunHandle(const char* handle) {
  delete[] handle;
  // TODO(suharshs): Free up any resources held by the partial run state.
}

void TF_SessionPRun(TF_Session* session, const char* handle,
                    const TF_Output* inputs, TF_Tensor* const* input_values,
                    int ninputs, const TF_Output* outputs,
                    TF_Tensor** output_values, int noutputs,
                    const TF_Operation* const* target_opers, int ntargets,
                    TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status);
}

unsigned char TF_TryEvaluateConstant(TF_Graph* graph, TF_Output output,
                                     TF_Tensor** result, TF_Status* status) {
  mutex_lock l(graph->mu);
  auto status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      });
  if (!status_or.ok() || !status_or->has_value()) {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
  *result = TF_TensorFromTensor(**status_or, &status->status);
  return status->status.ok();
}

TF_ApiDefMap* TF_NewApiDefMap(TF_Buffer* op_list_buffer, TF_Status* status) {
  tensorflow::OpList op_list;
  if (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)) {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
  status->status = absl::OkStatus();
  return new TF_ApiDefMap(op_list);
}

void TF_DeleteApiDefMap(TF_ApiDefMap* apimap) { delete apimap; }

void TF_ApiDefMapPut(TF_ApiDefMap* api_def_map, const char* text,
                     size_t text_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
#else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

TF_Buffer* TF_ApiDefMapGet(TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
  return nullptr;
#else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

TF_Buffer* TF_GetAllRegisteredKernels(TF_Status* status) {
  tensorflow::KernelList kernel_list = tensorflow::GetAllRegisteredKernels();
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}

TF_Buffer* TF_GetRegisteredKernelsForOp(const char* name, TF_Status* status) {
  tensorflow::KernelList kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name);
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}

void TF_UpdateEdge(TF_Graph* graph, TF_Output new_src, TF_Input dst,
                   TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&new_src.oper->node);

  if (ic->num_outputs() <= new_src.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
  tensorflow::shape_inference::ShapeHandle shape = ic->output(new_src.index);

  tensorflow::shape_inference::InferenceContext* ic_dst =
      graph->refiner.GetContext(&dst.oper->node);
  if (ic_dst->num_inputs() <= dst.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
  if (!ic_dst->MergeInput(dst.index, shape)) {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
  status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index);

  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
}

// Apis that are corresponding to python c api. --------------------------

void TF_AddOperationControlInput(TF_Graph* graph, TF_Operation* op,
                                 TF_Operation* input) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  graph->graph.AddControlEdge(&input->node, &op->node);
  RecordMutation(graph, *op, "adding control input");
}

void TF_SetAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                TF_Buffer* attr_value_proto, TF_Status* status) {
  using tensorflow::RecordMutation;
  tensorflow::AttrValue attr_val;
  if (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)) {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }

  mutex_lock l(graph->mu);
  op->node.AddAttr(attr_name, attr_val);
  RecordMutation(graph, *op, "setting attribute");
}

void TF_ClearAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                  TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.ClearAttr(attr_name);
  RecordMutation(graph, *op, "clearing attribute");
}

void TF_SetFullType(TF_Graph* graph, TF_Operation* op,
                    const TF_Buffer* full_type_proto) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  FullTypeDef full_type;
  full_type.ParseFromArray(full_type_proto->data, full_type_proto->length);
  *op->node.mutable_def()->mutable_experimental_type() = full_type;
  RecordMutation(graph, *op, "setting fulltype");
}

void TF_SetRequestedDevice(TF_Graph* graph, TF_Operation* op,
                           const char* device) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.set_requested_device(device);
  RecordMutation(graph, *op, "setting device");
}

void TF_RemoveAllControlInputs(TF_Graph* graph, TF_Operation* op) {
  mutex_lock l(graph->mu);
  std::vector<const tensorflow::Edge*> control_edges;
  for (const tensorflow::Edge* edge : op->node.in_edges()) {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
  for (const tensorflow::Edge* edge : control_edges) {
    graph->graph.RemoveControlEdge(edge);
  }
}

void TF_SetRequireShapeInferenceFns(TF_Graph* graph, bool require) {
  mutex_lock l(graph->mu);
  graph->refiner.set_require_shape_inference_fns(require);
}

void TF_ExtendSession(TF_Session* session, TF_Status* status) {
  ExtendSessionGraphHelper(session, status);
  session->extend_before_run = false;
}

TF_Buffer* TF_GetHandleShapeAndType(TF_Graph* graph, TF_Output output) {
  Node* node = &output.oper->node;
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  handle_data.set_is_set(true);
  {
    mutex_lock l(graph->mu);
    tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
    CHECK(ic != nullptr);                       // Crash OK
    CHECK_LT(output.index, ic->num_outputs());  // Crash OK
    const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
    if (shapes_and_types == nullptr) return nullptr;

    for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
  }
  string str_data;
  handle_data.SerializeToString(&str_data);

  TF_Buffer* result = TF_NewBufferFromString(str_data.c_str(), str_data.size());
  return result;
}

void TF_SetHandleShapeAndType(TF_Graph* graph, TF_Output output,
                              const void* proto, size_t proto_len,
                              TF_Status* status) {
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  if (!handle_data.ParseFromArray(proto, proto_len)) {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
  DCHECK(handle_data.is_set());

  tensorflow::mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&output.oper->node);

  std::vector<tensorflow::shape_inference::ShapeAndType> shapes_and_types;
  for (const auto& shape_and_type_proto : handle_data.shape_and_type()) {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
  ic->set_output_handle_shapes_and_types(output.index, shapes_and_types);
}

void TF_AddWhileInputHack(TF_Graph* graph, TF_Output new_src, TF_Operation* dst,
                          TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node);
  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
}

// -------------------------------------------------------------------

// TF_Server functions ----------------------------------------------

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
TF_Server::TF_Server(std::unique_ptr<tensorflow::ServerInterface> server)
    : target(server->target()), server(std::move(server)) {}
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

TF_Server* TF_NewServer(const void* proto, size_t proto_len,
                        TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
  return nullptr;
#else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerStart(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Start();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerStop(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Stop();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerJoin(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Join();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

const char* TF_ServerTarget(TF_Server* server) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  return nullptr;
#else
  return server->target.c_str();
#endif
}

void TF_DeleteServer(TF_Server* server) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  delete server;
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}

void TF_RegisterLogListener(void (*listener)(const char*)) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  tensorflow::logging::RegisterListener(listener);
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}

void TF_RegisterFilesystemPlugin(const char* plugin_filename,
                                 TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
#else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

}  // end extern "C"

 comment: /* Copyright 2015 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
 preproc_include: #include "tensorflow/c/c_api.h"

  #include: #include
  string_literal: "tensorflow/c/c_api.h"
   ": "
   string_content: tensorflow/c/c_api.h
   ": "
 preproc_include: #include <algorithm>

  #include: #include
  system_lib_string: <algorithm>
 preproc_include: #include <cstring>

  #include: #include
  system_lib_string: <cstring>
 preproc_include: #include <limits>

  #include: #include
  system_lib_string: <limits>
 preproc_include: #include <memory>

  #include: #include
  system_lib_string: <memory>
 preproc_include: #include <optional>

  #include: #include
  system_lib_string: <optional>
 preproc_include: #include <unordered_set>

  #include: #include
  system_lib_string: <unordered_set>
 preproc_include: #include <utility>

  #include: #include
  system_lib_string: <utility>
 preproc_include: #include <vector>

  #include: #include
  system_lib_string: <vector>
 preproc_include: #include "absl/strings/match.h"

  #include: #include
  string_literal: "absl/strings/match.h"
   ": "
   string_content: absl/strings/match.h
   ": "
 comment: // Required for IS_MOBILE_PLATFORM
 preproc_include: #include "tensorflow/core/platform/platform.h"  // NOLINT

  #include: #include
  string_literal: "tensorflow/core/platform/platform.h"
   ": "
   string_content: tensorflow/core/platform/platform.h
   ": "
  comment: // NOLINT
 preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
#include "tensorflow/c/experimental/filesystem/modular_filesystem.h"
#include "tensorflow/cc/framework/gradients.h"
#include "tensorflow/cc/framework/ops.h"
#include "tensorflow/cc/framework/scope_internal.h"
#include "tensorflow/cc/ops/while_loop.h"
#include "tensorflow/cc/saved_model/loader.h"
#include "tensorflow/core/distributed_runtime/server_lib.h"
#include "tensorflow/core/framework/logging.h"
#include "tensorflow/core/framework/op_gen_lib.h"
#endif
  #if: #if
  binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
   unary_expression: !defined(IS_MOBILE_PLATFORM)
    !: !
    preproc_defined: defined(IS_MOBILE_PLATFORM)
     defined: defined
     (: (
     identifier: IS_MOBILE_PLATFORM
     ): )
   &&: &&
   unary_expression: !defined(IS_SLIM_BUILD)
    !: !
    preproc_defined: defined(IS_SLIM_BUILD)
     defined: defined
     (: (
     identifier: IS_SLIM_BUILD
     ): )
  
: 

  preproc_include: #include "tensorflow/c/experimental/filesystem/modular_filesystem.h"

   #include: #include
   string_literal: "tensorflow/c/experimental/filesystem/modular_filesystem.h"
    ": "
    string_content: tensorflow/c/experimental/filesystem/modular_filesystem.h
    ": "
  preproc_include: #include "tensorflow/cc/framework/gradients.h"

   #include: #include
   string_literal: "tensorflow/cc/framework/gradients.h"
    ": "
    string_content: tensorflow/cc/framework/gradients.h
    ": "
  preproc_include: #include "tensorflow/cc/framework/ops.h"

   #include: #include
   string_literal: "tensorflow/cc/framework/ops.h"
    ": "
    string_content: tensorflow/cc/framework/ops.h
    ": "
  preproc_include: #include "tensorflow/cc/framework/scope_internal.h"

   #include: #include
   string_literal: "tensorflow/cc/framework/scope_internal.h"
    ": "
    string_content: tensorflow/cc/framework/scope_internal.h
    ": "
  preproc_include: #include "tensorflow/cc/ops/while_loop.h"

   #include: #include
   string_literal: "tensorflow/cc/ops/while_loop.h"
    ": "
    string_content: tensorflow/cc/ops/while_loop.h
    ": "
  preproc_include: #include "tensorflow/cc/saved_model/loader.h"

   #include: #include
   string_literal: "tensorflow/cc/saved_model/loader.h"
    ": "
    string_content: tensorflow/cc/saved_model/loader.h
    ": "
  preproc_include: #include "tensorflow/core/distributed_runtime/server_lib.h"

   #include: #include
   string_literal: "tensorflow/core/distributed_runtime/server_lib.h"
    ": "
    string_content: tensorflow/core/distributed_runtime/server_lib.h
    ": "
  preproc_include: #include "tensorflow/core/framework/logging.h"

   #include: #include
   string_literal: "tensorflow/core/framework/logging.h"
    ": "
    string_content: tensorflow/core/framework/logging.h
    ": "
  preproc_include: #include "tensorflow/core/framework/op_gen_lib.h"

   #include: #include
   string_literal: "tensorflow/core/framework/op_gen_lib.h"
    ": "
    string_content: tensorflow/core/framework/op_gen_lib.h
    ": "
  #endif: #endif
 comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
 preproc_include: #include "tensorflow/c/c_api_internal.h"

  #include: #include
  string_literal: "tensorflow/c/c_api_internal.h"
   ": "
   string_content: tensorflow/c/c_api_internal.h
   ": "
 preproc_include: #include "tensorflow/c/tf_buffer_internal.h"

  #include: #include
  string_literal: "tensorflow/c/tf_buffer_internal.h"
   ": "
   string_content: tensorflow/c/tf_buffer_internal.h
   ": "
 preproc_include: #include "tensorflow/c/tf_status_internal.h"

  #include: #include
  string_literal: "tensorflow/c/tf_status_internal.h"
   ": "
   string_content: tensorflow/c/tf_status_internal.h
   ": "
 preproc_include: #include "tensorflow/c/tf_tensor.h"

  #include: #include
  string_literal: "tensorflow/c/tf_tensor.h"
   ": "
   string_content: tensorflow/c/tf_tensor.h
   ": "
 preproc_include: #include "tensorflow/core/common_runtime/device_mgr.h"

  #include: #include
  string_literal: "tensorflow/core/common_runtime/device_mgr.h"
   ": "
   string_content: tensorflow/core/common_runtime/device_mgr.h
   ": "
 preproc_include: #include "tensorflow/core/common_runtime/eval_const_tensor.h"

  #include: #include
  string_literal: "tensorflow/core/common_runtime/eval_const_tensor.h"
   ": "
   string_content: tensorflow/core/common_runtime/eval_const_tensor.h
   ": "
 preproc_include: #include "tensorflow/core/common_runtime/graph_constructor.h"

  #include: #include
  string_literal: "tensorflow/core/common_runtime/graph_constructor.h"
   ": "
   string_content: tensorflow/core/common_runtime/graph_constructor.h
   ": "
 preproc_include: #include "tensorflow/core/common_runtime/shape_refiner.h"

  #include: #include
  string_literal: "tensorflow/core/common_runtime/shape_refiner.h"
   ": "
   string_content: tensorflow/core/common_runtime/shape_refiner.h
   ": "
 preproc_include: #include "tensorflow/core/config/flag_defs.h"

  #include: #include
  string_literal: "tensorflow/core/config/flag_defs.h"
   ": "
   string_content: tensorflow/core/config/flag_defs.h
   ": "
 preproc_include: #include "tensorflow/core/config/flags.h"

  #include: #include
  string_literal: "tensorflow/core/config/flags.h"
   ": "
   string_content: tensorflow/core/config/flags.h
   ": "
 preproc_include: #include "tensorflow/core/framework/allocation_description.pb.h"

  #include: #include
  string_literal: "tensorflow/core/framework/allocation_description.pb.h"
   ": "
   string_content: tensorflow/core/framework/allocation_description.pb.h
   ": "
 preproc_include: #include "tensorflow/core/framework/attr_value_util.h"

  #include: #include
  string_literal: "tensorflow/core/framework/attr_value_util.h"
   ": "
   string_content: tensorflow/core/framework/attr_value_util.h
   ": "
 preproc_include: #include "tensorflow/core/framework/cpp_shape_inference.pb.h"

  #include: #include
  string_literal: "tensorflow/core/framework/cpp_shape_inference.pb.h"
   ": "
   string_content: tensorflow/core/framework/cpp_shape_inference.pb.h
   ": "
 preproc_include: #include "tensorflow/core/framework/full_type.pb.h"

  #include: #include
  string_literal: "tensorflow/core/framework/full_type.pb.h"
   ": "
   string_content: tensorflow/core/framework/full_type.pb.h
   ": "
 preproc_include: #include "tensorflow/core/framework/kernel_def.pb.h"

  #include: #include
  string_literal: "tensorflow/core/framework/kernel_def.pb.h"
   ": "
   string_content: tensorflow/core/framework/kernel_def.pb.h
   ": "
 preproc_include: #include "tensorflow/core/framework/log_memory.h"

  #include: #include
  string_literal: "tensorflow/core/framework/log_memory.h"
   ": "
   string_content: tensorflow/core/framework/log_memory.h
   ": "
 preproc_include: #include "tensorflow/core/framework/node_def_util.h"

  #include: #include
  string_literal: "tensorflow/core/framework/node_def_util.h"
   ": "
   string_content: tensorflow/core/framework/node_def_util.h
   ": "
 preproc_include: #include "tensorflow/core/framework/op_kernel.h"

  #include: #include
  string_literal: "tensorflow/core/framework/op_kernel.h"
   ": "
   string_content: tensorflow/core/framework/op_kernel.h
   ": "
 preproc_include: #include "tensorflow/core/framework/partial_tensor_shape.h"

  #include: #include
  string_literal: "tensorflow/core/framework/partial_tensor_shape.h"
   ": "
   string_content: tensorflow/core/framework/partial_tensor_shape.h
   ": "
 preproc_include: #include "tensorflow/core/framework/shape_inference.h"

  #include: #include
  string_literal: "tensorflow/core/framework/shape_inference.h"
   ": "
   string_content: tensorflow/core/framework/shape_inference.h
   ": "
 preproc_include: #include "tensorflow/core/framework/tensor.h"

  #include: #include
  string_literal: "tensorflow/core/framework/tensor.h"
   ": "
   string_content: tensorflow/core/framework/tensor.h
   ": "
 preproc_include: #include "tensorflow/core/framework/tensor.pb.h"  // NOLINT

  #include: #include
  string_literal: "tensorflow/core/framework/tensor.pb.h"
   ": "
   string_content: tensorflow/core/framework/tensor.pb.h
   ": "
  comment: // NOLINT
 preproc_include: #include "tensorflow/core/framework/tensor_shape.h"

  #include: #include
  string_literal: "tensorflow/core/framework/tensor_shape.h"
   ": "
   string_content: tensorflow/core/framework/tensor_shape.h
   ": "
 preproc_include: #include "tensorflow/core/framework/tensor_shape.pb.h"

  #include: #include
  string_literal: "tensorflow/core/framework/tensor_shape.pb.h"
   ": "
   string_content: tensorflow/core/framework/tensor_shape.pb.h
   ": "
 preproc_include: #include "tensorflow/core/framework/types.h"

  #include: #include
  string_literal: "tensorflow/core/framework/types.h"
   ": "
   string_content: tensorflow/core/framework/types.h
   ": "
 preproc_include: #include "tensorflow/core/framework/versions.pb.h"

  #include: #include
  string_literal: "tensorflow/core/framework/versions.pb.h"
   ": "
   string_content: tensorflow/core/framework/versions.pb.h
   ": "
 preproc_include: #include "tensorflow/core/graph/graph.h"

  #include: #include
  string_literal: "tensorflow/core/graph/graph.h"
   ": "
   string_content: tensorflow/core/graph/graph.h
   ": "
 preproc_include: #include "tensorflow/core/graph/node_builder.h"

  #include: #include
  string_literal: "tensorflow/core/graph/node_builder.h"
   ": "
   string_content: tensorflow/core/graph/node_builder.h
   ": "
 preproc_include: #include "tensorflow/core/graph/validate.h"

  #include: #include
  string_literal: "tensorflow/core/graph/validate.h"
   ": "
   string_content: tensorflow/core/graph/validate.h
   ": "
 preproc_include: #include "tensorflow/core/lib/gtl/array_slice.h"

  #include: #include
  string_literal: "tensorflow/core/lib/gtl/array_slice.h"
   ": "
   string_content: tensorflow/core/lib/gtl/array_slice.h
   ": "
 preproc_include: #include "tensorflow/core/platform/coding.h"

  #include: #include
  string_literal: "tensorflow/core/platform/coding.h"
   ": "
   string_content: tensorflow/core/platform/coding.h
   ": "
 preproc_include: #include "tensorflow/core/platform/errors.h"

  #include: #include
  string_literal: "tensorflow/core/platform/errors.h"
   ": "
   string_content: tensorflow/core/platform/errors.h
   ": "
 preproc_include: #include "tensorflow/core/platform/mem.h"

  #include: #include
  string_literal: "tensorflow/core/platform/mem.h"
   ": "
   string_content: tensorflow/core/platform/mem.h
   ": "
 preproc_include: #include "tensorflow/core/platform/mutex.h"

  #include: #include
  string_literal: "tensorflow/core/platform/mutex.h"
   ": "
   string_content: tensorflow/core/platform/mutex.h
   ": "
 preproc_include: #include "tensorflow/core/platform/protobuf.h"

  #include: #include
  string_literal: "tensorflow/core/platform/protobuf.h"
   ": "
   string_content: tensorflow/core/platform/protobuf.h
   ": "
 preproc_include: #include "tensorflow/core/platform/status.h"

  #include: #include
  string_literal: "tensorflow/core/platform/status.h"
   ": "
   string_content: tensorflow/core/platform/status.h
   ": "
 preproc_include: #include "tensorflow/core/platform/str_util.h"

  #include: #include
  string_literal: "tensorflow/core/platform/str_util.h"
   ": "
   string_content: tensorflow/core/platform/str_util.h
   ": "
 preproc_include: #include "tensorflow/core/platform/strcat.h"

  #include: #include
  string_literal: "tensorflow/core/platform/strcat.h"
   ": "
   string_content: tensorflow/core/platform/strcat.h
   ": "
 preproc_include: #include "tensorflow/core/platform/stringpiece.h"

  #include: #include
  string_literal: "tensorflow/core/platform/stringpiece.h"
   ": "
   string_content: tensorflow/core/platform/stringpiece.h
   ": "
 preproc_include: #include "tensorflow/core/platform/thread_annotations.h"

  #include: #include
  string_literal: "tensorflow/core/platform/thread_annotations.h"
   ": "
   string_content: tensorflow/core/platform/thread_annotations.h
   ": "
 preproc_include: #include "tensorflow/core/platform/types.h"

  #include: #include
  string_literal: "tensorflow/core/platform/types.h"
   ": "
   string_content: tensorflow/core/platform/types.h
   ": "
 preproc_include: #include "tensorflow/core/public/release_version.h"

  #include: #include
  string_literal: "tensorflow/core/public/release_version.h"
   ": "
   string_content: tensorflow/core/public/release_version.h
   ": "
 preproc_include: #include "tensorflow/core/public/session.h"

  #include: #include
  string_literal: "tensorflow/core/public/session.h"
   ": "
   string_content: tensorflow/core/public/session.h
   ": "
 comment: // The implementation below is at the top level instead of the
 comment: // brain namespace because we are defining 'extern "C"' functions.
 using_declaration: using tensorflow::AttrValueMap;
  using: using
  qualified_identifier: tensorflow::AttrValueMap
   namespace_identifier: tensorflow
   ::: ::
   identifier: AttrValueMap
  ;: ;
 using_declaration: using tensorflow::DataType;
  using: using
  qualified_identifier: tensorflow::DataType
   namespace_identifier: tensorflow
   ::: ::
   identifier: DataType
  ;: ;
 using_declaration: using tensorflow::ExtendSessionGraphHelper;
  using: using
  qualified_identifier: tensorflow::ExtendSessionGraphHelper
   namespace_identifier: tensorflow
   ::: ::
   identifier: ExtendSessionGraphHelper
  ;: ;
 using_declaration: using tensorflow::FullTypeDef;
  using: using
  qualified_identifier: tensorflow::FullTypeDef
   namespace_identifier: tensorflow
   ::: ::
   identifier: FullTypeDef
  ;: ;
 using_declaration: using tensorflow::Graph;
  using: using
  qualified_identifier: tensorflow::Graph
   namespace_identifier: tensorflow
   ::: ::
   identifier: Graph
  ;: ;
 using_declaration: using tensorflow::GraphDef;
  using: using
  qualified_identifier: tensorflow::GraphDef
   namespace_identifier: tensorflow
   ::: ::
   identifier: GraphDef
  ;: ;
 using_declaration: using tensorflow::mutex_lock;
  using: using
  qualified_identifier: tensorflow::mutex_lock
   namespace_identifier: tensorflow
   ::: ::
   identifier: mutex_lock
  ;: ;
 using_declaration: using tensorflow::NameRangeMap;
  using: using
  qualified_identifier: tensorflow::NameRangeMap
   namespace_identifier: tensorflow
   ::: ::
   identifier: NameRangeMap
  ;: ;
 using_declaration: using tensorflow::NameRangesForNode;
  using: using
  qualified_identifier: tensorflow::NameRangesForNode
   namespace_identifier: tensorflow
   ::: ::
   identifier: NameRangesForNode
  ;: ;
 using_declaration: using tensorflow::NewSession;
  using: using
  qualified_identifier: tensorflow::NewSession
   namespace_identifier: tensorflow
   ::: ::
   identifier: NewSession
  ;: ;
 using_declaration: using tensorflow::Node;
  using: using
  qualified_identifier: tensorflow::Node
   namespace_identifier: tensorflow
   ::: ::
   identifier: Node
  ;: ;
 using_declaration: using tensorflow::NodeBuilder;
  using: using
  qualified_identifier: tensorflow::NodeBuilder
   namespace_identifier: tensorflow
   ::: ::
   identifier: NodeBuilder
  ;: ;
 using_declaration: using tensorflow::OpDef;
  using: using
  qualified_identifier: tensorflow::OpDef
   namespace_identifier: tensorflow
   ::: ::
   identifier: OpDef
  ;: ;
 using_declaration: using tensorflow::PartialTensorShape;
  using: using
  qualified_identifier: tensorflow::PartialTensorShape
   namespace_identifier: tensorflow
   ::: ::
   identifier: PartialTensorShape
  ;: ;
 using_declaration: using tensorflow::RunMetadata;
  using: using
  qualified_identifier: tensorflow::RunMetadata
   namespace_identifier: tensorflow
   ::: ::
   identifier: RunMetadata
  ;: ;
 using_declaration: using tensorflow::RunOptions;
  using: using
  qualified_identifier: tensorflow::RunOptions
   namespace_identifier: tensorflow
   ::: ::
   identifier: RunOptions
  ;: ;
 using_declaration: using tensorflow::Session;
  using: using
  qualified_identifier: tensorflow::Session
   namespace_identifier: tensorflow
   ::: ::
   identifier: Session
  ;: ;
 using_declaration: using tensorflow::Status;
  using: using
  qualified_identifier: tensorflow::Status
   namespace_identifier: tensorflow
   ::: ::
   identifier: Status
  ;: ;
 using_declaration: using tensorflow::string;
  using: using
  qualified_identifier: tensorflow::string
   namespace_identifier: tensorflow
   ::: ::
   identifier: string
  ;: ;
 using_declaration: using tensorflow::Tensor;
  using: using
  qualified_identifier: tensorflow::Tensor
   namespace_identifier: tensorflow
   ::: ::
   identifier: Tensor
  ;: ;
 using_declaration: using tensorflow::TensorId;
  using: using
  qualified_identifier: tensorflow::TensorId
   namespace_identifier: tensorflow
   ::: ::
   identifier: TensorId
  ;: ;
 using_declaration: using tensorflow::TensorShapeProto;
  using: using
  qualified_identifier: tensorflow::TensorShapeProto
   namespace_identifier: tensorflow
   ::: ::
   identifier: TensorShapeProto
  ;: ;
 using_declaration: using tensorflow::VersionDef;
  using: using
  qualified_identifier: tensorflow::VersionDef
   namespace_identifier: tensorflow
   ::: ::
   identifier: VersionDef
  ;: ;
 using_declaration: using tensorflow::errors::FailedPrecondition;
  using: using
  qualified_identifier: tensorflow::errors::FailedPrecondition
   namespace_identifier: tensorflow
   ::: ::
   qualified_identifier: errors::FailedPrecondition
    namespace_identifier: errors
    ::: ::
    identifier: FailedPrecondition
  ;: ;
 using_declaration: using tensorflow::errors::InvalidArgument;
  using: using
  qualified_identifier: tensorflow::errors::InvalidArgument
   namespace_identifier: tensorflow
   ::: ::
   qualified_identifier: errors::InvalidArgument
    namespace_identifier: errors
    ::: ::
    identifier: InvalidArgument
  ;: ;
 using_declaration: using tensorflow::errors::OutOfRange;
  using: using
  qualified_identifier: tensorflow::errors::OutOfRange
   namespace_identifier: tensorflow
   ::: ::
   qualified_identifier: errors::OutOfRange
    namespace_identifier: errors
    ::: ::
    identifier: OutOfRange
  ;: ;
 using_declaration: using tensorflow::gtl::ArraySlice;
  using: using
  qualified_identifier: tensorflow::gtl::ArraySlice
   namespace_identifier: tensorflow
   ::: ::
   qualified_identifier: gtl::ArraySlice
    namespace_identifier: gtl
    ::: ::
    identifier: ArraySlice
  ;: ;
 using_declaration: using tensorflow::strings::StrCat;
  using: using
  qualified_identifier: tensorflow::strings::StrCat
   namespace_identifier: tensorflow
   ::: ::
   qualified_identifier: strings::StrCat
    namespace_identifier: strings
    ::: ::
    identifier: StrCat
  ;: ;
 linkage_specification: extern "C" {

// --------------------------------------------------------------------------
const char* TF_Version() { return TF_VERSION_STRING; }

// --------------------------------------------------------------------------

// --------------------------------------------------------------------------
TF_SessionOptions* TF_NewSessionOptions() {
  TF_SessionOptions* out = new TF_SessionOptions;
  // Disable optimizations for static graph to allow calls to Session::Extend.
  out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
  return out;
}
void TF_DeleteSessionOptions(TF_SessionOptions* opt) { delete opt; }

void TF_SetTarget(TF_SessionOptions* options, const char* target) {
  options->options.target = target;
}

void TF_SetConfig(TF_SessionOptions* options, const void* proto,
                  size_t proto_len, TF_Status* status) {
  if (!options->options.config.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
  // Disable optimizations for static graph to allow calls to Session::Extend.
  options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
}

void TF_TensorFromProto(const TF_Buffer* from, TF_Tensor* to,
                        TF_Status* status) {
  TF_SetStatus(status, TF_OK, "");
  tensorflow::TensorProto from_tensor_proto;
  status->status = BufferToMessage(from, &from_tensor_proto);
  if (!status->status.ok()) {
    return;
  }
  status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto);
}
// --------------------------------------------------------------------------

TF_DeprecatedSession* TF_NewDeprecatedSession(const TF_SessionOptions* opt,
                                              TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}

void TF_CloseDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = s->session->Close();
}

void TF_DeleteDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  delete s->session;
  delete s;
}

void TF_ExtendGraph(TF_DeprecatedSession* s, const void* proto,
                    size_t proto_len, TF_Status* status) {
  GraphDef g;
  if (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  status->status = s->session->Extend(g);
}

}
  extern: extern
  string_literal: "C"
   ": "
   string_content: C
   ": "
  declaration_list: {

// --------------------------------------------------------------------------
const char* TF_Version() { return TF_VERSION_STRING; }

// --------------------------------------------------------------------------

// --------------------------------------------------------------------------
TF_SessionOptions* TF_NewSessionOptions() {
  TF_SessionOptions* out = new TF_SessionOptions;
  // Disable optimizations for static graph to allow calls to Session::Extend.
  out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
  return out;
}
void TF_DeleteSessionOptions(TF_SessionOptions* opt) { delete opt; }

void TF_SetTarget(TF_SessionOptions* options, const char* target) {
  options->options.target = target;
}

void TF_SetConfig(TF_SessionOptions* options, const void* proto,
                  size_t proto_len, TF_Status* status) {
  if (!options->options.config.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
  // Disable optimizations for static graph to allow calls to Session::Extend.
  options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
}

void TF_TensorFromProto(const TF_Buffer* from, TF_Tensor* to,
                        TF_Status* status) {
  TF_SetStatus(status, TF_OK, "");
  tensorflow::TensorProto from_tensor_proto;
  status->status = BufferToMessage(from, &from_tensor_proto);
  if (!status->status.ok()) {
    return;
  }
  status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto);
}
// --------------------------------------------------------------------------

TF_DeprecatedSession* TF_NewDeprecatedSession(const TF_SessionOptions* opt,
                                              TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}

void TF_CloseDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = s->session->Close();
}

void TF_DeleteDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  delete s->session;
  delete s;
}

void TF_ExtendGraph(TF_DeprecatedSession* s, const void* proto,
                    size_t proto_len, TF_Status* status) {
  GraphDef g;
  if (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  status->status = s->session->Extend(g);
}

}
   {: {
   comment: // --------------------------------------------------------------------------
   function_definition: const char* TF_Version() { return TF_VERSION_STRING; }
    type_qualifier: const
     const: const
    primitive_type: char
    pointer_declarator: * TF_Version()
     *: *
     function_declarator: TF_Version()
      identifier: TF_Version
      parameter_list: ()
       (: (
       ): )
    compound_statement: { return TF_VERSION_STRING; }
     {: {
     return_statement: return TF_VERSION_STRING;
      return: return
      identifier: TF_VERSION_STRING
      ;: ;
     }: }
   comment: // --------------------------------------------------------------------------
   comment: // --------------------------------------------------------------------------
   function_definition: TF_SessionOptions* TF_NewSessionOptions() {
  TF_SessionOptions* out = new TF_SessionOptions;
  // Disable optimizations for static graph to allow calls to Session::Extend.
  out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
  return out;
}
    type_identifier: TF_SessionOptions
    pointer_declarator: * TF_NewSessionOptions()
     *: *
     function_declarator: TF_NewSessionOptions()
      identifier: TF_NewSessionOptions
      parameter_list: ()
       (: (
       ): )
    compound_statement: {
  TF_SessionOptions* out = new TF_SessionOptions;
  // Disable optimizations for static graph to allow calls to Session::Extend.
  out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
  return out;
}
     {: {
     declaration: TF_SessionOptions* out = new TF_SessionOptions;
      type_identifier: TF_SessionOptions
      init_declarator: * out = new TF_SessionOptions
       pointer_declarator: * out
        *: *
        identifier: out
       =: =
       new_expression: new TF_SessionOptions
        new: new
        type_identifier: TF_SessionOptions
      ;: ;
     comment: // Disable optimizations for static graph to allow calls to Session::Extend.
     expression_statement: out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
      call_expression: out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true)
       field_expression: out->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph
        call_expression: out->options.config.mutable_experimental()
         field_expression: out->options.config.mutable_experimental
          field_expression: out->options.config
           field_expression: out->options
            identifier: out
            ->: ->
            field_identifier: options
           .: .
           field_identifier: config
          .: .
          field_identifier: mutable_experimental
         argument_list: ()
          (: (
          ): )
        ->: ->
        field_identifier: set_disable_optimize_for_static_graph
       argument_list: (true)
        (: (
        true: true
        ): )
      ;: ;
     return_statement: return out;
      return: return
      identifier: out
      ;: ;
     }: }
   function_definition: void TF_DeleteSessionOptions(TF_SessionOptions* opt) { delete opt; }
    primitive_type: void
    function_declarator: TF_DeleteSessionOptions(TF_SessionOptions* opt)
     identifier: TF_DeleteSessionOptions
     parameter_list: (TF_SessionOptions* opt)
      (: (
      parameter_declaration: TF_SessionOptions* opt
       type_identifier: TF_SessionOptions
       pointer_declarator: * opt
        *: *
        identifier: opt
      ): )
    compound_statement: { delete opt; }
     {: {
     expression_statement: delete opt;
      delete_expression: delete opt
       delete: delete
       identifier: opt
      ;: ;
     }: }
   function_definition: void TF_SetTarget(TF_SessionOptions* options, const char* target) {
  options->options.target = target;
}
    primitive_type: void
    function_declarator: TF_SetTarget(TF_SessionOptions* options, const char* target)
     identifier: TF_SetTarget
     parameter_list: (TF_SessionOptions* options, const char* target)
      (: (
      parameter_declaration: TF_SessionOptions* options
       type_identifier: TF_SessionOptions
       pointer_declarator: * options
        *: *
        identifier: options
      ,: ,
      parameter_declaration: const char* target
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * target
        *: *
        identifier: target
      ): )
    compound_statement: {
  options->options.target = target;
}
     {: {
     expression_statement: options->options.target = target;
      assignment_expression: options->options.target = target
       field_expression: options->options.target
        field_expression: options->options
         identifier: options
         ->: ->
         field_identifier: options
        .: .
        field_identifier: target
       =: =
       identifier: target
      ;: ;
     }: }
   function_definition: void TF_SetConfig(TF_SessionOptions* options, const void* proto,
                  size_t proto_len, TF_Status* status) {
  if (!options->options.config.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
  // Disable optimizations for static graph to allow calls to Session::Extend.
  options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
}
    primitive_type: void
    function_declarator: TF_SetConfig(TF_SessionOptions* options, const void* proto,
                  size_t proto_len, TF_Status* status)
     identifier: TF_SetConfig
     parameter_list: (TF_SessionOptions* options, const void* proto,
                  size_t proto_len, TF_Status* status)
      (: (
      parameter_declaration: TF_SessionOptions* options
       type_identifier: TF_SessionOptions
       pointer_declarator: * options
        *: *
        identifier: options
      ,: ,
      parameter_declaration: const void* proto
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * proto
        *: *
        identifier: proto
      ,: ,
      parameter_declaration: size_t proto_len
       primitive_type: size_t
       identifier: proto_len
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  if (!options->options.config.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
  // Disable optimizations for static graph to allow calls to Session::Extend.
  options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
}
     {: {
     if_statement: if (!options->options.config.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
      if: if
      condition_clause: (!options->options.config.ParseFromArray(proto, proto_len))
       (: (
       unary_expression: !options->options.config.ParseFromArray(proto, proto_len)
        !: !
        call_expression: options->options.config.ParseFromArray(proto, proto_len)
         field_expression: options->options.config.ParseFromArray
          field_expression: options->options.config
           field_expression: options->options
            identifier: options
            ->: ->
            field_identifier: options
           .: .
           field_identifier: config
          .: .
          field_identifier: ParseFromArray
         argument_list: (proto, proto_len)
          (: (
          identifier: proto
          ,: ,
          identifier: proto_len
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Unparseable ConfigProto");
  }
       {: {
       expression_statement: status->status = InvalidArgument("Unparseable ConfigProto");
        assignment_expression: status->status = InvalidArgument("Unparseable ConfigProto")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Unparseable ConfigProto")
          identifier: InvalidArgument
          argument_list: ("Unparseable ConfigProto")
           (: (
           string_literal: "Unparseable ConfigProto"
            ": "
            string_content: Unparseable ConfigProto
            ": "
           ): )
        ;: ;
       }: }
     comment: // Disable optimizations for static graph to allow calls to Session::Extend.
     expression_statement: options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true);
      call_expression: options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph(true)
       field_expression: options->options.config.mutable_experimental()
      ->set_disable_optimize_for_static_graph
        call_expression: options->options.config.mutable_experimental()
         field_expression: options->options.config.mutable_experimental
          field_expression: options->options.config
           field_expression: options->options
            identifier: options
            ->: ->
            field_identifier: options
           .: .
           field_identifier: config
          .: .
          field_identifier: mutable_experimental
         argument_list: ()
          (: (
          ): )
        ->: ->
        field_identifier: set_disable_optimize_for_static_graph
       argument_list: (true)
        (: (
        true: true
        ): )
      ;: ;
     }: }
   function_definition: void TF_TensorFromProto(const TF_Buffer* from, TF_Tensor* to,
                        TF_Status* status) {
  TF_SetStatus(status, TF_OK, "");
  tensorflow::TensorProto from_tensor_proto;
  status->status = BufferToMessage(from, &from_tensor_proto);
  if (!status->status.ok()) {
    return;
  }
  status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto);
}
    primitive_type: void
    function_declarator: TF_TensorFromProto(const TF_Buffer* from, TF_Tensor* to,
                        TF_Status* status)
     identifier: TF_TensorFromProto
     parameter_list: (const TF_Buffer* from, TF_Tensor* to,
                        TF_Status* status)
      (: (
      parameter_declaration: const TF_Buffer* from
       type_qualifier: const
        const: const
       type_identifier: TF_Buffer
       pointer_declarator: * from
        *: *
        identifier: from
      ,: ,
      parameter_declaration: TF_Tensor* to
       type_identifier: TF_Tensor
       pointer_declarator: * to
        *: *
        identifier: to
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  TF_SetStatus(status, TF_OK, "");
  tensorflow::TensorProto from_tensor_proto;
  status->status = BufferToMessage(from, &from_tensor_proto);
  if (!status->status.ok()) {
    return;
  }
  status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto);
}
     {: {
     expression_statement: TF_SetStatus(status, TF_OK, "");
      call_expression: TF_SetStatus(status, TF_OK, "")
       identifier: TF_SetStatus
       argument_list: (status, TF_OK, "")
        (: (
        identifier: status
        ,: ,
        identifier: TF_OK
        ,: ,
        string_literal: ""
         ": "
         ": "
        ): )
      ;: ;
     declaration: tensorflow::TensorProto from_tensor_proto;
      qualified_identifier: tensorflow::TensorProto
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: TensorProto
      identifier: from_tensor_proto
      ;: ;
     expression_statement: status->status = BufferToMessage(from, &from_tensor_proto);
      assignment_expression: status->status = BufferToMessage(from, &from_tensor_proto)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: BufferToMessage(from, &from_tensor_proto)
        identifier: BufferToMessage
        argument_list: (from, &from_tensor_proto)
         (: (
         identifier: from
         ,: ,
         pointer_expression: &from_tensor_proto
          &: &
          identifier: from_tensor_proto
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) {
    return;
  }
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    return;
  }
       {: {
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto);
      assignment_expression: status->status =
      tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto(from_tensor_proto)
        field_expression: tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          ->FromProto
         call_expression: tensorflow::down_cast<tensorflow::TensorInterface*>(to->tensor)
          qualified_identifier: tensorflow::down_cast<tensorflow::TensorInterface*>
           namespace_identifier: tensorflow
           ::: ::
           template_function: down_cast<tensorflow::TensorInterface*>
            identifier: down_cast
            template_argument_list: <tensorflow::TensorInterface*>
             <: <
             type_descriptor: tensorflow::TensorInterface*
              qualified_identifier: tensorflow::TensorInterface
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: TensorInterface
              abstract_pointer_declarator: *
               *: *
             >: >
          argument_list: (to->tensor)
           (: (
           field_expression: to->tensor
            identifier: to
            ->: ->
            field_identifier: tensor
           ): )
         ->: ->
         field_identifier: FromProto
        argument_list: (from_tensor_proto)
         (: (
         identifier: from_tensor_proto
         ): )
      ;: ;
     }: }
   comment: // --------------------------------------------------------------------------
   function_definition: TF_DeprecatedSession* TF_NewDeprecatedSession(const TF_SessionOptions* opt,
                                              TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}
    type_identifier: TF_DeprecatedSession
    pointer_declarator: * TF_NewDeprecatedSession(const TF_SessionOptions* opt,
                                              TF_Status* status)
     *: *
     function_declarator: TF_NewDeprecatedSession(const TF_SessionOptions* opt,
                                              TF_Status* status)
      identifier: TF_NewDeprecatedSession
      parameter_list: (const TF_SessionOptions* opt,
                                              TF_Status* status)
       (: (
       parameter_declaration: const TF_SessionOptions* opt
        type_qualifier: const
         const: const
        type_identifier: TF_SessionOptions
        pointer_declarator: * opt
         *: *
         identifier: opt
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}
     {: {
     declaration: Session* session;
      type_identifier: Session
      pointer_declarator: * session
       *: *
       identifier: session
      ;: ;
     expression_statement: status->status = NewSession(opt->options, &session);
      assignment_expression: status->status = NewSession(opt->options, &session)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: NewSession(opt->options, &session)
        identifier: NewSession
        argument_list: (opt->options, &session)
         (: (
         field_expression: opt->options
          identifier: opt
          ->: ->
          field_identifier: options
         ,: ,
         pointer_expression: &session
          &: &
          identifier: session
         ): )
      ;: ;
     if_statement: if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
      if: if
      condition_clause: (status->status.ok())
       (: (
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
       ): )
      compound_statement: {
    return new TF_DeprecatedSession({session});
  }
       {: {
       return_statement: return new TF_DeprecatedSession({session});
        return: return
        new_expression: new TF_DeprecatedSession({session})
         new: new
         type_identifier: TF_DeprecatedSession
         argument_list: ({session})
          (: (
          initializer_list: {session}
           {: {
           identifier: session
           }: }
          ): )
        ;: ;
       }: }
      else_clause: else {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
       else: else
       compound_statement: {
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
        {: {
        expression_statement: DCHECK_EQ(nullptr, session);
         call_expression: DCHECK_EQ(nullptr, session)
          identifier: DCHECK_EQ
          argument_list: (nullptr, session)
           (: (
           null: nullptr
            nullptr: nullptr
           ,: ,
           identifier: session
           ): )
         ;: ;
        return_statement: return nullptr;
         return: return
         null: nullptr
          nullptr: nullptr
         ;: ;
        }: }
     }: }
   function_definition: void TF_CloseDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = s->session->Close();
}
    primitive_type: void
    function_declarator: TF_CloseDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status)
     identifier: TF_CloseDeprecatedSession
     parameter_list: (TF_DeprecatedSession* s, TF_Status* status)
      (: (
      parameter_declaration: TF_DeprecatedSession* s
       type_identifier: TF_DeprecatedSession
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  status->status = s->session->Close();
}
     {: {
     expression_statement: status->status = s->session->Close();
      assignment_expression: status->status = s->session->Close()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: s->session->Close()
        field_expression: s->session->Close
         field_expression: s->session
          identifier: s
          ->: ->
          field_identifier: session
         ->: ->
         field_identifier: Close
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   function_definition: void TF_DeleteDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  delete s->session;
  delete s;
}
    primitive_type: void
    function_declarator: TF_DeleteDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status)
     identifier: TF_DeleteDeprecatedSession
     parameter_list: (TF_DeprecatedSession* s, TF_Status* status)
      (: (
      parameter_declaration: TF_DeprecatedSession* s
       type_identifier: TF_DeprecatedSession
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  delete s->session;
  delete s;
}
     {: {
     expression_statement: status->status = absl::OkStatus();
      assignment_expression: status->status = absl::OkStatus()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: absl::OkStatus()
        qualified_identifier: absl::OkStatus
         namespace_identifier: absl
         ::: ::
         identifier: OkStatus
        argument_list: ()
         (: (
         ): )
      ;: ;
     if_statement: if (s == nullptr) return;
      if: if
      condition_clause: (s == nullptr)
       (: (
       binary_expression: s == nullptr
        identifier: s
        ==: ==
        null: nullptr
         nullptr: nullptr
       ): )
      return_statement: return;
       return: return
       ;: ;
     expression_statement: delete s->session;
      delete_expression: delete s->session
       delete: delete
       field_expression: s->session
        identifier: s
        ->: ->
        field_identifier: session
      ;: ;
     expression_statement: delete s;
      delete_expression: delete s
       delete: delete
       identifier: s
      ;: ;
     }: }
   function_definition: void TF_ExtendGraph(TF_DeprecatedSession* s, const void* proto,
                    size_t proto_len, TF_Status* status) {
  GraphDef g;
  if (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  status->status = s->session->Extend(g);
}
    primitive_type: void
    function_declarator: TF_ExtendGraph(TF_DeprecatedSession* s, const void* proto,
                    size_t proto_len, TF_Status* status)
     identifier: TF_ExtendGraph
     parameter_list: (TF_DeprecatedSession* s, const void* proto,
                    size_t proto_len, TF_Status* status)
      (: (
      parameter_declaration: TF_DeprecatedSession* s
       type_identifier: TF_DeprecatedSession
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: const void* proto
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * proto
        *: *
        identifier: proto
      ,: ,
      parameter_declaration: size_t proto_len
       primitive_type: size_t
       identifier: proto_len
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  GraphDef g;
  if (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  status->status = s->session->Extend(g);
}
     {: {
     declaration: GraphDef g;
      type_identifier: GraphDef
      identifier: g
      ;: ;
     if_statement: if (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
      if: if
      condition_clause: (!tensorflow::ParseProtoUnlimited(&g, proto, proto_len))
       (: (
       unary_expression: !tensorflow::ParseProtoUnlimited(&g, proto, proto_len)
        !: !
        call_expression: tensorflow::ParseProtoUnlimited(&g, proto, proto_len)
         qualified_identifier: tensorflow::ParseProtoUnlimited
          namespace_identifier: tensorflow
          ::: ::
          identifier: ParseProtoUnlimited
         argument_list: (&g, proto, proto_len)
          (: (
          pointer_expression: &g
           &: &
           identifier: g
          ,: ,
          identifier: proto
          ,: ,
          identifier: proto_len
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Invalid GraphDef");
        assignment_expression: status->status = InvalidArgument("Invalid GraphDef")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Invalid GraphDef")
          identifier: InvalidArgument
          argument_list: ("Invalid GraphDef")
           (: (
           string_literal: "Invalid GraphDef"
            ": "
            string_content: Invalid GraphDef
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: status->status = s->session->Extend(g);
      assignment_expression: status->status = s->session->Extend(g)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: s->session->Extend(g)
        field_expression: s->session->Extend
         field_expression: s->session
          identifier: s
          ->: ->
          field_identifier: session
         ->: ->
         field_identifier: Extend
        argument_list: (g)
         (: (
         identifier: g
         ): )
      ;: ;
     }: }
   }: }
 comment: // end extern "C"
 comment: // Reset helper for converting character arrays to string vectors.
 function_definition: static void TF_Reset_Helper(const TF_SessionOptions* opt,
                            const char** containers, int ncontainers,
                            TF_Status* status) {
  std::vector<string> container_names(ncontainers);
  for (int i = 0; i < ncontainers; ++i) {
    container_names[i] = containers[i];
  }

  status->status = Reset(opt->options, container_names);
}
  storage_class_specifier: static
   static: static
  primitive_type: void
  function_declarator: TF_Reset_Helper(const TF_SessionOptions* opt,
                            const char** containers, int ncontainers,
                            TF_Status* status)
   identifier: TF_Reset_Helper
   parameter_list: (const TF_SessionOptions* opt,
                            const char** containers, int ncontainers,
                            TF_Status* status)
    (: (
    parameter_declaration: const TF_SessionOptions* opt
     type_qualifier: const
      const: const
     type_identifier: TF_SessionOptions
     pointer_declarator: * opt
      *: *
      identifier: opt
    ,: ,
    parameter_declaration: const char** containers
     type_qualifier: const
      const: const
     primitive_type: char
     pointer_declarator: ** containers
      *: *
      pointer_declarator: * containers
       *: *
       identifier: containers
    ,: ,
    parameter_declaration: int ncontainers
     primitive_type: int
     identifier: ncontainers
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  std::vector<string> container_names(ncontainers);
  for (int i = 0; i < ncontainers; ++i) {
    container_names[i] = containers[i];
  }

  status->status = Reset(opt->options, container_names);
}
   {: {
   declaration: std::vector<string> container_names(ncontainers);
    qualified_identifier: std::vector<string>
     namespace_identifier: std
     ::: ::
     template_type: vector<string>
      type_identifier: vector
      template_argument_list: <string>
       <: <
       type_descriptor: string
        type_identifier: string
       >: >
    function_declarator: container_names(ncontainers)
     identifier: container_names
     parameter_list: (ncontainers)
      (: (
      parameter_declaration: ncontainers
       type_identifier: ncontainers
      ): )
    ;: ;
   for_statement: for (int i = 0; i < ncontainers; ++i) {
    container_names[i] = containers[i];
  }
    for: for
    (: (
    declaration: int i = 0;
     primitive_type: int
     init_declarator: i = 0
      identifier: i
      =: =
      number_literal: 0
     ;: ;
    binary_expression: i < ncontainers
     identifier: i
     <: <
     identifier: ncontainers
    ;: ;
    update_expression: ++i
     ++: ++
     identifier: i
    ): )
    compound_statement: {
    container_names[i] = containers[i];
  }
     {: {
     expression_statement: container_names[i] = containers[i];
      assignment_expression: container_names[i] = containers[i]
       subscript_expression: container_names[i]
        identifier: container_names
        subscript_argument_list: [i]
         [: [
         identifier: i
         ]: ]
       =: =
       subscript_expression: containers[i]
        identifier: containers
        subscript_argument_list: [i]
         [: [
         identifier: i
         ]: ]
      ;: ;
     }: }
   expression_statement: status->status = Reset(opt->options, container_names);
    assignment_expression: status->status = Reset(opt->options, container_names)
     field_expression: status->status
      identifier: status
      ->: ->
      field_identifier: status
     =: =
     call_expression: Reset(opt->options, container_names)
      identifier: Reset
      argument_list: (opt->options, container_names)
       (: (
       field_expression: opt->options
        identifier: opt
        ->: ->
        field_identifier: options
       ,: ,
       identifier: container_names
       ): )
    ;: ;
   }: }
 linkage_specification: extern "C" {

void TF_Reset(const TF_SessionOptions* opt, const char** containers,
              int ncontainers, TF_Status* status) {
  TF_Reset_Helper(opt, containers, ncontainers, status);
}

}
  extern: extern
  string_literal: "C"
   ": "
   string_content: C
   ": "
  declaration_list: {

void TF_Reset(const TF_SessionOptions* opt, const char** containers,
              int ncontainers, TF_Status* status) {
  TF_Reset_Helper(opt, containers, ncontainers, status);
}

}
   {: {
   function_definition: void TF_Reset(const TF_SessionOptions* opt, const char** containers,
              int ncontainers, TF_Status* status) {
  TF_Reset_Helper(opt, containers, ncontainers, status);
}
    primitive_type: void
    function_declarator: TF_Reset(const TF_SessionOptions* opt, const char** containers,
              int ncontainers, TF_Status* status)
     identifier: TF_Reset
     parameter_list: (const TF_SessionOptions* opt, const char** containers,
              int ncontainers, TF_Status* status)
      (: (
      parameter_declaration: const TF_SessionOptions* opt
       type_qualifier: const
        const: const
       type_identifier: TF_SessionOptions
       pointer_declarator: * opt
        *: *
        identifier: opt
      ,: ,
      parameter_declaration: const char** containers
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** containers
        *: *
        pointer_declarator: * containers
         *: *
         identifier: containers
      ,: ,
      parameter_declaration: int ncontainers
       primitive_type: int
       identifier: ncontainers
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  TF_Reset_Helper(opt, containers, ncontainers, status);
}
     {: {
     expression_statement: TF_Reset_Helper(opt, containers, ncontainers, status);
      call_expression: TF_Reset_Helper(opt, containers, ncontainers, status)
       identifier: TF_Reset_Helper
       argument_list: (opt, containers, ncontainers, status)
        (: (
        identifier: opt
        ,: ,
        identifier: containers
        ,: ,
        identifier: ncontainers
        ,: ,
        identifier: status
        ): )
      ;: ;
     }: }
   }: }
 comment: // end extern "C"
 namespace_definition: namespace tensorflow {

void RecordMutation(TF_Graph* graph, const TF_Operation& op,
                    const char* mutation_type) {
  // If any session has already run this node_id, mark this session as
  // unrunnable.
  for (auto it : graph->sessions) {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
}

namespace {

// Helper method that creates a shape handle for a shape described by dims.
tensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims) {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}

}  // namespace

void TF_GraphSetOutputHandleShapesAndTypes(TF_Graph* graph, TF_Output output,
                                           int num_shapes_and_types,
                                           const int64_t** shapes,
                                           const int* ranks,
                                           const TF_DataType* types,
                                           TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  auto shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types);
  for (int i = 0; i < num_shapes_and_types; ++i) {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }

  ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec);
}

// Helpers for loading a TensorFlow plugin (a .so file).
absl::Status LoadDynamicLibrary(const char* library_filename, void** result,
                                const void** buf, size_t* len);

// TODO(josh11b,mrry): Change Session to be able to use a Graph*
// directly, instead of requiring us to serialize to a GraphDef and
// call Session::Extend().
bool ExtendSessionGraphHelper(TF_Session* session, TF_Status* status) {
  if (session->graph != nullptr) {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
  return true;
}

}
  namespace: namespace
  namespace_identifier: tensorflow
  declaration_list: {

void RecordMutation(TF_Graph* graph, const TF_Operation& op,
                    const char* mutation_type) {
  // If any session has already run this node_id, mark this session as
  // unrunnable.
  for (auto it : graph->sessions) {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
}

namespace {

// Helper method that creates a shape handle for a shape described by dims.
tensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims) {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}

}  // namespace

void TF_GraphSetOutputHandleShapesAndTypes(TF_Graph* graph, TF_Output output,
                                           int num_shapes_and_types,
                                           const int64_t** shapes,
                                           const int* ranks,
                                           const TF_DataType* types,
                                           TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  auto shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types);
  for (int i = 0; i < num_shapes_and_types; ++i) {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }

  ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec);
}

// Helpers for loading a TensorFlow plugin (a .so file).
absl::Status LoadDynamicLibrary(const char* library_filename, void** result,
                                const void** buf, size_t* len);

// TODO(josh11b,mrry): Change Session to be able to use a Graph*
// directly, instead of requiring us to serialize to a GraphDef and
// call Session::Extend().
bool ExtendSessionGraphHelper(TF_Session* session, TF_Status* status) {
  if (session->graph != nullptr) {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
  return true;
}

}
   {: {
   function_definition: void RecordMutation(TF_Graph* graph, const TF_Operation& op,
                    const char* mutation_type) {
  // If any session has already run this node_id, mark this session as
  // unrunnable.
  for (auto it : graph->sessions) {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
}
    primitive_type: void
    function_declarator: RecordMutation(TF_Graph* graph, const TF_Operation& op,
                    const char* mutation_type)
     identifier: RecordMutation
     parameter_list: (TF_Graph* graph, const TF_Operation& op,
                    const char* mutation_type)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: const TF_Operation& op
       type_qualifier: const
        const: const
       type_identifier: TF_Operation
       reference_declarator: & op
        &: &
        identifier: op
      ,: ,
      parameter_declaration: const char* mutation_type
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * mutation_type
        *: *
        identifier: mutation_type
      ): )
    compound_statement: {
  // If any session has already run this node_id, mark this session as
  // unrunnable.
  for (auto it : graph->sessions) {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
}
     {: {
     comment: // If any session has already run this node_id, mark this session as
     comment: // unrunnable.
     for_range_loop: for (auto it : graph->sessions) {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
      for: for
      (: (
      placeholder_type_specifier: auto
       auto: auto
      identifier: it
      :: :
      field_expression: graph->sessions
       identifier: graph
       ->: ->
       field_identifier: sessions
      ): )
      compound_statement: {
    mutex_lock session_lock(it.first->mu);
    if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
  }
       {: {
       declaration: mutex_lock session_lock(it.first->mu);
        type_identifier: mutex_lock
        init_declarator: session_lock(it.first->mu)
         identifier: session_lock
         argument_list: (it.first->mu)
          (: (
          field_expression: it.first->mu
           field_expression: it.first
            identifier: it
            .: .
            field_identifier: first
           ->: ->
           field_identifier: mu
          ): )
        ;: ;
       if_statement: if (it.first->last_num_graph_nodes > op.node.id()) {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
        if: if
        condition_clause: (it.first->last_num_graph_nodes > op.node.id())
         (: (
         binary_expression: it.first->last_num_graph_nodes > op.node.id()
          field_expression: it.first->last_num_graph_nodes
           field_expression: it.first
            identifier: it
            .: .
            field_identifier: first
           ->: ->
           field_identifier: last_num_graph_nodes
          >: >
          call_expression: op.node.id()
           field_expression: op.node.id
            field_expression: op.node
             identifier: op
             .: .
             field_identifier: node
            .: .
            field_identifier: id
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
      it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
    }
         {: {
         expression_statement: it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.");
          assignment_expression: it.second = strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.")
           field_expression: it.second
            identifier: it
            .: .
            field_identifier: second
           =: =
           call_expression: strings::StrCat(
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.")
            qualified_identifier: strings::StrCat
             namespace_identifier: strings
             ::: ::
             identifier: StrCat
            argument_list: (
          "Operation '", op.node.DebugString(), "' was changed by ",
          mutation_type,
          " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session.")
             (: (
             string_literal: "Operation '"
              ": "
              string_content: Operation '
              ": "
             ,: ,
             call_expression: op.node.DebugString()
              field_expression: op.node.DebugString
               field_expression: op.node
                identifier: op
                .: .
                field_identifier: node
               .: .
               field_identifier: DebugString
              argument_list: ()
               (: (
               ): )
             ,: ,
             string_literal: "' was changed by "
              ": "
              string_content: ' was changed by 
              ": "
             ,: ,
             identifier: mutation_type
             ,: ,
             concatenated_string: " after it was run by a session. This mutation will have no effect, "
          "and will trigger an error in the future. Either don't modify "
          "nodes after running them or create a new session."
              string_literal: " after it was run by a session. This mutation will have no effect, "
               ": "
               string_content:  after it was run by a session. This mutation will have no effect, 
               ": "
              string_literal: "and will trigger an error in the future. Either don't modify "
               ": "
               string_content: and will trigger an error in the future. Either don't modify 
               ": "
              string_literal: "nodes after running them or create a new session."
               ": "
               string_content: nodes after running them or create a new session.
               ": "
             ): )
          ;: ;
         }: }
       }: }
     }: }
   namespace_definition: namespace {

// Helper method that creates a shape handle for a shape described by dims.
tensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims) {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}

}
    namespace: namespace
    declaration_list: {

// Helper method that creates a shape handle for a shape described by dims.
tensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims) {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}

}
     {: {
     comment: // Helper method that creates a shape handle for a shape described by dims.
     function_definition: tensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims) {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}
      qualified_identifier: tensorflow::shape_inference::ShapeHandle
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: shape_inference::ShapeHandle
        namespace_identifier: shape_inference
        ::: ::
        type_identifier: ShapeHandle
      function_declarator: ShapeHandleFromDims(
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims)
       identifier: ShapeHandleFromDims
       parameter_list: (
    tensorflow::shape_inference::InferenceContext* ic, int num_dims,
    const int64_t* dims)
        (: (
        parameter_declaration: tensorflow::shape_inference::InferenceContext* ic
         qualified_identifier: tensorflow::shape_inference::InferenceContext
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: shape_inference::InferenceContext
           namespace_identifier: shape_inference
           ::: ::
           type_identifier: InferenceContext
         pointer_declarator: * ic
          *: *
          identifier: ic
        ,: ,
        parameter_declaration: int num_dims
         primitive_type: int
         identifier: num_dims
        ,: ,
        parameter_declaration: const int64_t* dims
         type_qualifier: const
          const: const
         primitive_type: int64_t
         pointer_declarator: * dims
          *: *
          identifier: dims
        ): )
      compound_statement: {
  if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
}
       {: {
       if_statement: if (num_dims != -1) {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  } else {
    return ic->UnknownShape();
  }
        if: if
        condition_clause: (num_dims != -1)
         (: (
         binary_expression: num_dims != -1
          identifier: num_dims
          !=: !=
          number_literal: -1
         ): )
        compound_statement: {
    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
    dim_vec.reserve(num_dims);
    for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
    return ic->MakeShape(dim_vec);
  }
         {: {
         declaration: std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;
          qualified_identifier: std::vector<tensorflow::shape_inference::DimensionHandle>
           namespace_identifier: std
           ::: ::
           template_type: vector<tensorflow::shape_inference::DimensionHandle>
            type_identifier: vector
            template_argument_list: <tensorflow::shape_inference::DimensionHandle>
             <: <
             type_descriptor: tensorflow::shape_inference::DimensionHandle
              qualified_identifier: tensorflow::shape_inference::DimensionHandle
               namespace_identifier: tensorflow
               ::: ::
               qualified_identifier: shape_inference::DimensionHandle
                namespace_identifier: shape_inference
                ::: ::
                type_identifier: DimensionHandle
             >: >
          identifier: dim_vec
          ;: ;
         expression_statement: dim_vec.reserve(num_dims);
          call_expression: dim_vec.reserve(num_dims)
           field_expression: dim_vec.reserve
            identifier: dim_vec
            .: .
            field_identifier: reserve
           argument_list: (num_dims)
            (: (
            identifier: num_dims
            ): )
          ;: ;
         for_statement: for (int i = 0; i < num_dims; ++i) {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
          for: for
          (: (
          declaration: int i = 0;
           primitive_type: int
           init_declarator: i = 0
            identifier: i
            =: =
            number_literal: 0
           ;: ;
          binary_expression: i < num_dims
           identifier: i
           <: <
           identifier: num_dims
          ;: ;
          update_expression: ++i
           ++: ++
           identifier: i
          ): )
          compound_statement: {
      dim_vec.push_back(ic->MakeDim(dims[i]));
    }
           {: {
           expression_statement: dim_vec.push_back(ic->MakeDim(dims[i]));
            call_expression: dim_vec.push_back(ic->MakeDim(dims[i]))
             field_expression: dim_vec.push_back
              identifier: dim_vec
              .: .
              field_identifier: push_back
             argument_list: (ic->MakeDim(dims[i]))
              (: (
              call_expression: ic->MakeDim(dims[i])
               field_expression: ic->MakeDim
                identifier: ic
                ->: ->
                field_identifier: MakeDim
               argument_list: (dims[i])
                (: (
                subscript_expression: dims[i]
                 identifier: dims
                 subscript_argument_list: [i]
                  [: [
                  identifier: i
                  ]: ]
                ): )
              ): )
            ;: ;
           }: }
         return_statement: return ic->MakeShape(dim_vec);
          return: return
          call_expression: ic->MakeShape(dim_vec)
           field_expression: ic->MakeShape
            identifier: ic
            ->: ->
            field_identifier: MakeShape
           argument_list: (dim_vec)
            (: (
            identifier: dim_vec
            ): )
          ;: ;
         }: }
        else_clause: else {
    return ic->UnknownShape();
  }
         else: else
         compound_statement: {
    return ic->UnknownShape();
  }
          {: {
          return_statement: return ic->UnknownShape();
           return: return
           call_expression: ic->UnknownShape()
            field_expression: ic->UnknownShape
             identifier: ic
             ->: ->
             field_identifier: UnknownShape
            argument_list: ()
             (: (
             ): )
           ;: ;
          }: }
       }: }
     }: }
   comment: // namespace
   function_definition: void TF_GraphSetOutputHandleShapesAndTypes(TF_Graph* graph, TF_Output output,
                                           int num_shapes_and_types,
                                           const int64_t** shapes,
                                           const int* ranks,
                                           const TF_DataType* types,
                                           TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  auto shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types);
  for (int i = 0; i < num_shapes_and_types; ++i) {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }

  ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec);
}
    primitive_type: void
    function_declarator: TF_GraphSetOutputHandleShapesAndTypes(TF_Graph* graph, TF_Output output,
                                           int num_shapes_and_types,
                                           const int64_t** shapes,
                                           const int* ranks,
                                           const TF_DataType* types,
                                           TF_Status* status)
     identifier: TF_GraphSetOutputHandleShapesAndTypes
     parameter_list: (TF_Graph* graph, TF_Output output,
                                           int num_shapes_and_types,
                                           const int64_t** shapes,
                                           const int* ranks,
                                           const TF_DataType* types,
                                           TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Output output
       type_identifier: TF_Output
       identifier: output
      ,: ,
      parameter_declaration: int num_shapes_and_types
       primitive_type: int
       identifier: num_shapes_and_types
      ,: ,
      parameter_declaration: const int64_t** shapes
       type_qualifier: const
        const: const
       primitive_type: int64_t
       pointer_declarator: ** shapes
        *: *
        pointer_declarator: * shapes
         *: *
         identifier: shapes
      ,: ,
      parameter_declaration: const int* ranks
       type_qualifier: const
        const: const
       primitive_type: int
       pointer_declarator: * ranks
        *: *
        identifier: ranks
      ,: ,
      parameter_declaration: const TF_DataType* types
       type_qualifier: const
        const: const
       type_identifier: TF_DataType
       pointer_declarator: * types
        *: *
        identifier: types
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  auto shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types);
  for (int i = 0; i < num_shapes_and_types; ++i) {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }

  ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec);
}
     {: {
     declaration: Node* node = &output.oper->node;
      type_identifier: Node
      init_declarator: * node = &output.oper->node
       pointer_declarator: * node
        *: *
        identifier: node
       =: =
       pointer_expression: &output.oper->node
        &: &
        field_expression: output.oper->node
         field_expression: output.oper
          identifier: output
          .: .
          field_identifier: oper
         ->: ->
         field_identifier: node
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
      qualified_identifier: tensorflow::shape_inference::InferenceContext
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: shape_inference::InferenceContext
        namespace_identifier: shape_inference
        ::: ::
        type_identifier: InferenceContext
      init_declarator: * ic =
      graph->refiner.GetContext(node)
       pointer_declarator: * ic
        *: *
        identifier: ic
       =: =
       call_expression: graph->refiner.GetContext(node)
        field_expression: graph->refiner.GetContext
         field_expression: graph->refiner
          identifier: graph
          ->: ->
          field_identifier: refiner
         .: .
         field_identifier: GetContext
        argument_list: (node)
         (: (
         identifier: node
         ): )
      ;: ;
     if_statement: if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
      if: if
      condition_clause: (ic == nullptr)
       (: (
       binary_expression: ic == nullptr
        identifier: ic
        ==: ==
        null: nullptr
         nullptr: nullptr
       ): )
      compound_statement: {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
       {: {
       expression_statement: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
        assignment_expression: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Node ", node->name(), " was not found in the graph")
          identifier: InvalidArgument
          argument_list: ("Node ", node->name(), " was not found in the graph")
           (: (
           string_literal: "Node "
            ": "
            string_content: Node 
            ": "
           ,: ,
           call_expression: node->name()
            field_expression: node->name
             identifier: node
             ->: ->
             field_identifier: name
            argument_list: ()
             (: (
             ): )
           ,: ,
           string_literal: " was not found in the graph"
            ": "
            string_content:  was not found in the graph
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: auto shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: shape_and_type_vec =
      std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types)
       identifier: shape_and_type_vec
       =: =
       call_expression: std::vector<tensorflow::shape_inference::ShapeAndType>(
          num_shapes_and_types)
        qualified_identifier: std::vector<tensorflow::shape_inference::ShapeAndType>
         namespace_identifier: std
         ::: ::
         template_function: vector<tensorflow::shape_inference::ShapeAndType>
          identifier: vector
          template_argument_list: <tensorflow::shape_inference::ShapeAndType>
           <: <
           type_descriptor: tensorflow::shape_inference::ShapeAndType
            qualified_identifier: tensorflow::shape_inference::ShapeAndType
             namespace_identifier: tensorflow
             ::: ::
             qualified_identifier: shape_inference::ShapeAndType
              namespace_identifier: shape_inference
              ::: ::
              type_identifier: ShapeAndType
           >: >
        argument_list: (
          num_shapes_and_types)
         (: (
         identifier: num_shapes_and_types
         ): )
      ;: ;
     for_statement: for (int i = 0; i < num_shapes_and_types; ++i) {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < num_shapes_and_types
       identifier: i
       <: <
       identifier: num_shapes_and_types
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
    shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
  }
       {: {
       declaration: tensorflow::shape_inference::ShapeHandle shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i]);
        qualified_identifier: tensorflow::shape_inference::ShapeHandle
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: shape_inference::ShapeHandle
          namespace_identifier: shape_inference
          ::: ::
          type_identifier: ShapeHandle
        init_declarator: shape_handle =
        ShapeHandleFromDims(ic, ranks[i], shapes[i])
         identifier: shape_handle
         =: =
         call_expression: ShapeHandleFromDims(ic, ranks[i], shapes[i])
          identifier: ShapeHandleFromDims
          argument_list: (ic, ranks[i], shapes[i])
           (: (
           identifier: ic
           ,: ,
           subscript_expression: ranks[i]
            identifier: ranks
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ,: ,
           subscript_expression: shapes[i]
            identifier: shapes
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       expression_statement: shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]));
        assignment_expression: shape_and_type_vec[i] = tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]))
         subscript_expression: shape_and_type_vec[i]
          identifier: shape_and_type_vec
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: tensorflow::shape_inference::ShapeAndType(
        shape_handle, static_cast<DataType>(types[i]))
          qualified_identifier: tensorflow::shape_inference::ShapeAndType
           namespace_identifier: tensorflow
           ::: ::
           qualified_identifier: shape_inference::ShapeAndType
            namespace_identifier: shape_inference
            ::: ::
            identifier: ShapeAndType
          argument_list: (
        shape_handle, static_cast<DataType>(types[i]))
           (: (
           identifier: shape_handle
           ,: ,
           call_expression: static_cast<DataType>(types[i])
            template_function: static_cast<DataType>
             identifier: static_cast
             template_argument_list: <DataType>
              <: <
              type_descriptor: DataType
               type_identifier: DataType
              >: >
            argument_list: (types[i])
             (: (
             subscript_expression: types[i]
              identifier: types
              subscript_argument_list: [i]
               [: [
               identifier: i
               ]: ]
             ): )
           ): )
        ;: ;
       }: }
     expression_statement: ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec);
      call_expression: ic->set_output_handle_shapes_and_types(output.index, shape_and_type_vec)
       field_expression: ic->set_output_handle_shapes_and_types
        identifier: ic
        ->: ->
        field_identifier: set_output_handle_shapes_and_types
       argument_list: (output.index, shape_and_type_vec)
        (: (
        field_expression: output.index
         identifier: output
         .: .
         field_identifier: index
        ,: ,
        identifier: shape_and_type_vec
        ): )
      ;: ;
     }: }
   comment: // Helpers for loading a TensorFlow plugin (a .so file).
   declaration: absl::Status LoadDynamicLibrary(const char* library_filename, void** result,
                                const void** buf, size_t* len);
    qualified_identifier: absl::Status
     namespace_identifier: absl
     ::: ::
     type_identifier: Status
    function_declarator: LoadDynamicLibrary(const char* library_filename, void** result,
                                const void** buf, size_t* len)
     identifier: LoadDynamicLibrary
     parameter_list: (const char* library_filename, void** result,
                                const void** buf, size_t* len)
      (: (
      parameter_declaration: const char* library_filename
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * library_filename
        *: *
        identifier: library_filename
      ,: ,
      parameter_declaration: void** result
       primitive_type: void
       pointer_declarator: ** result
        *: *
        pointer_declarator: * result
         *: *
         identifier: result
      ,: ,
      parameter_declaration: const void** buf
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: ** buf
        *: *
        pointer_declarator: * buf
         *: *
         identifier: buf
      ,: ,
      parameter_declaration: size_t* len
       primitive_type: size_t
       pointer_declarator: * len
        *: *
        identifier: len
      ): )
    ;: ;
   comment: // TODO(josh11b,mrry): Change Session to be able to use a Graph*
   comment: // directly, instead of requiring us to serialize to a GraphDef and
   comment: // call Session::Extend().
   function_definition: bool ExtendSessionGraphHelper(TF_Session* session, TF_Status* status) {
  if (session->graph != nullptr) {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
  return true;
}
    primitive_type: bool
    function_declarator: ExtendSessionGraphHelper(TF_Session* session, TF_Status* status)
     identifier: ExtendSessionGraphHelper
     parameter_list: (TF_Session* session, TF_Status* status)
      (: (
      parameter_declaration: TF_Session* session
       type_identifier: TF_Session
       pointer_declarator: * session
        *: *
        identifier: session
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  if (session->graph != nullptr) {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
  return true;
}
     {: {
     if_statement: if (session->graph != nullptr) {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
      if: if
      condition_clause: (session->graph != nullptr)
       (: (
       binary_expression: session->graph != nullptr
        field_expression: session->graph
         identifier: session
         ->: ->
         field_identifier: graph
        !=: !=
        null: nullptr
         nullptr: nullptr
       ): )
      compound_statement: {
    // Take the graph lock before the session lock to avoid deadlock. This is
    // safe since session->graph does not change.
    session->graph->mu.lock();
    mutex_lock session_lock(session->mu);
    const Graph& graph = session->graph->graph;

    const string& mutation_warning = session->graph->sessions[session];
    if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }

    const auto num_nodes = graph.num_node_ids();
    if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
  }
       {: {
       comment: // Take the graph lock before the session lock to avoid deadlock. This is
       comment: // safe since session->graph does not change.
       expression_statement: session->graph->mu.lock();
        call_expression: session->graph->mu.lock()
         field_expression: session->graph->mu.lock
          field_expression: session->graph->mu
           field_expression: session->graph
            identifier: session
            ->: ->
            field_identifier: graph
           ->: ->
           field_identifier: mu
          .: .
          field_identifier: lock
         argument_list: ()
          (: (
          ): )
        ;: ;
       declaration: mutex_lock session_lock(session->mu);
        type_identifier: mutex_lock
        init_declarator: session_lock(session->mu)
         identifier: session_lock
         argument_list: (session->mu)
          (: (
          field_expression: session->mu
           identifier: session
           ->: ->
           field_identifier: mu
          ): )
        ;: ;
       declaration: const Graph& graph = session->graph->graph;
        type_qualifier: const
         const: const
        type_identifier: Graph
        init_declarator: & graph = session->graph->graph
         reference_declarator: & graph
          &: &
          identifier: graph
         =: =
         field_expression: session->graph->graph
          field_expression: session->graph
           identifier: session
           ->: ->
           field_identifier: graph
          ->: ->
          field_identifier: graph
        ;: ;
       declaration: const string& mutation_warning = session->graph->sessions[session];
        type_qualifier: const
         const: const
        type_identifier: string
        init_declarator: & mutation_warning = session->graph->sessions[session]
         reference_declarator: & mutation_warning
          &: &
          identifier: mutation_warning
         =: =
         subscript_expression: session->graph->sessions[session]
          field_expression: session->graph->sessions
           field_expression: session->graph
            identifier: session
            ->: ->
            field_identifier: graph
           ->: ->
           field_identifier: sessions
          subscript_argument_list: [session]
           [: [
           identifier: session
           ]: ]
        ;: ;
       if_statement: if (!mutation_warning.empty()) {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }
        if: if
        condition_clause: (!mutation_warning.empty())
         (: (
         unary_expression: !mutation_warning.empty()
          !: !
          call_expression: mutation_warning.empty()
           field_expression: mutation_warning.empty
            identifier: mutation_warning
            .: .
            field_identifier: empty
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
      // TODO(b/74949947): turn this back into an error status
      LOG(WARNING) << mutation_warning;
      session->graph->sessions[session].clear();
    }
         {: {
         comment: // TODO(b/74949947): turn this back into an error status
         expression_statement: LOG(WARNING) << mutation_warning;
          binary_expression: LOG(WARNING) << mutation_warning
           call_expression: LOG(WARNING)
            identifier: LOG
            argument_list: (WARNING)
             (: (
             identifier: WARNING
             ): )
           <<: <<
           identifier: mutation_warning
          ;: ;
         expression_statement: session->graph->sessions[session].clear();
          call_expression: session->graph->sessions[session].clear()
           field_expression: session->graph->sessions[session].clear
            subscript_expression: session->graph->sessions[session]
             field_expression: session->graph->sessions
              field_expression: session->graph
               identifier: session
               ->: ->
               field_identifier: graph
              ->: ->
              field_identifier: sessions
             subscript_argument_list: [session]
              [: [
              identifier: session
              ]: ]
            .: .
            field_identifier: clear
           argument_list: ()
            (: (
            ): )
          ;: ;
         }: }
       declaration: const auto num_nodes = graph.num_node_ids();
        type_qualifier: const
         const: const
        placeholder_type_specifier: auto
         auto: auto
        init_declarator: num_nodes = graph.num_node_ids()
         identifier: num_nodes
         =: =
         call_expression: graph.num_node_ids()
          field_expression: graph.num_node_ids
           identifier: graph
           .: .
           field_identifier: num_node_ids
          argument_list: ()
           (: (
           ): )
        ;: ;
       if_statement: if (session->last_num_graph_nodes < num_nodes) {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    } else {
      session->graph->mu.unlock();
    }
        if: if
        condition_clause: (session->last_num_graph_nodes < num_nodes)
         (: (
         binary_expression: session->last_num_graph_nodes < num_nodes
          field_expression: session->last_num_graph_nodes
           identifier: session
           ->: ->
           field_identifier: last_num_graph_nodes
          <: <
          identifier: num_nodes
         ): )
        compound_statement: {
      // TODO(nolivia): check this on a subset of the graph instead of all of
      // it.
      status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
      if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }

      GraphDef graph_def;
      *graph_def.mutable_versions() = graph.versions();
      // Fill graph_def with nodes with ids in the range
      // [session->last_num_graph_nodes, num_nodes), that is the nodes
      // added since the last TF_SessionRun() call.
      for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
      *graph_def.mutable_library() = graph.flib_def().ToProto();
      if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
      session->graph->mu.unlock();
      status->status = session->session->Extend(std::move(graph_def));
      if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
      // Note: session->session is not modified if Extend() fails, so
      // we only set last_num_graph_nodes if it succeeds.
      session->last_num_graph_nodes = num_nodes;
    }
         {: {
         comment: // TODO(nolivia): check this on a subset of the graph instead of all of
         comment: // it.
         expression_statement: status->status = graph::ValidateGraphHasNoCycle(session->graph->graph);
          assignment_expression: status->status = graph::ValidateGraphHasNoCycle(session->graph->graph)
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: graph::ValidateGraphHasNoCycle(session->graph->graph)
            qualified_identifier: graph::ValidateGraphHasNoCycle
             namespace_identifier: graph
             ::: ::
             identifier: ValidateGraphHasNoCycle
            argument_list: (session->graph->graph)
             (: (
             field_expression: session->graph->graph
              field_expression: session->graph
               identifier: session
               ->: ->
               field_identifier: graph
              ->: ->
              field_identifier: graph
             ): )
          ;: ;
         if_statement: if (!status->status.ok()) {
        session->graph->mu.unlock();
        return false;
      }
          if: if
          condition_clause: (!status->status.ok())
           (: (
           unary_expression: !status->status.ok()
            !: !
            call_expression: status->status.ok()
             field_expression: status->status.ok
              field_expression: status->status
               identifier: status
               ->: ->
               field_identifier: status
              .: .
              field_identifier: ok
             argument_list: ()
              (: (
              ): )
           ): )
          compound_statement: {
        session->graph->mu.unlock();
        return false;
      }
           {: {
           expression_statement: session->graph->mu.unlock();
            call_expression: session->graph->mu.unlock()
             field_expression: session->graph->mu.unlock
              field_expression: session->graph->mu
               field_expression: session->graph
                identifier: session
                ->: ->
                field_identifier: graph
               ->: ->
               field_identifier: mu
              .: .
              field_identifier: unlock
             argument_list: ()
              (: (
              ): )
            ;: ;
           return_statement: return false;
            return: return
            false: false
            ;: ;
           }: }
         declaration: GraphDef graph_def;
          type_identifier: GraphDef
          identifier: graph_def
          ;: ;
         expression_statement: *graph_def.mutable_versions() = graph.versions();
          assignment_expression: *graph_def.mutable_versions() = graph.versions()
           pointer_expression: *graph_def.mutable_versions()
            *: *
            call_expression: graph_def.mutable_versions()
             field_expression: graph_def.mutable_versions
              identifier: graph_def
              .: .
              field_identifier: mutable_versions
             argument_list: ()
              (: (
              ): )
           =: =
           call_expression: graph.versions()
            field_expression: graph.versions
             identifier: graph
             .: .
             field_identifier: versions
            argument_list: ()
             (: (
             ): )
          ;: ;
         comment: // Fill graph_def with nodes with ids in the range
         comment: // [session->last_num_graph_nodes, num_nodes), that is the nodes
         comment: // added since the last TF_SessionRun() call.
         for_statement: for (auto id = session->last_num_graph_nodes; id < num_nodes; ++id) {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
          for: for
          (: (
          declaration: auto id = session->last_num_graph_nodes;
           placeholder_type_specifier: auto
            auto: auto
           init_declarator: id = session->last_num_graph_nodes
            identifier: id
            =: =
            field_expression: session->last_num_graph_nodes
             identifier: session
             ->: ->
             field_identifier: last_num_graph_nodes
           ;: ;
          binary_expression: id < num_nodes
           identifier: id
           <: <
           identifier: num_nodes
          ;: ;
          update_expression: ++id
           ++: ++
           identifier: id
          ): )
          compound_statement: {
        Node* const node = graph.FindNodeId(id);
        if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
      }
           {: {
           declaration: Node* const node = graph.FindNodeId(id);
            type_identifier: Node
            init_declarator: * const node = graph.FindNodeId(id)
             pointer_declarator: * const node
              *: *
              type_qualifier: const
               const: const
              identifier: node
             =: =
             call_expression: graph.FindNodeId(id)
              field_expression: graph.FindNodeId
               identifier: graph
               .: .
               field_identifier: FindNodeId
              argument_list: (id)
               (: (
               identifier: id
               ): )
            ;: ;
           if_statement: if (node != nullptr && node->IsOp()) {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
            if: if
            condition_clause: (node != nullptr && node->IsOp())
             (: (
             binary_expression: node != nullptr && node->IsOp()
              binary_expression: node != nullptr
               identifier: node
               !=: !=
               null: nullptr
                nullptr: nullptr
              &&: &&
              call_expression: node->IsOp()
               field_expression: node->IsOp
                identifier: node
                ->: ->
                field_identifier: IsOp
               argument_list: ()
                (: (
                ): )
             ): )
            compound_statement: {
          NodeDef* const node_def = graph_def.add_node();
          *node_def = node->def();
        }
             {: {
             declaration: NodeDef* const node_def = graph_def.add_node();
              type_identifier: NodeDef
              init_declarator: * const node_def = graph_def.add_node()
               pointer_declarator: * const node_def
                *: *
                type_qualifier: const
                 const: const
                identifier: node_def
               =: =
               call_expression: graph_def.add_node()
                field_expression: graph_def.add_node
                 identifier: graph_def
                 .: .
                 field_identifier: add_node
                argument_list: ()
                 (: (
                 ): )
              ;: ;
             expression_statement: *node_def = node->def();
              assignment_expression: *node_def = node->def()
               pointer_expression: *node_def
                *: *
                identifier: node_def
               =: =
               call_expression: node->def()
                field_expression: node->def
                 identifier: node
                 ->: ->
                 field_identifier: def
                argument_list: ()
                 (: (
                 ): )
              ;: ;
             }: }
           }: }
         expression_statement: *graph_def.mutable_library() = graph.flib_def().ToProto();
          assignment_expression: *graph_def.mutable_library() = graph.flib_def().ToProto()
           pointer_expression: *graph_def.mutable_library()
            *: *
            call_expression: graph_def.mutable_library()
             field_expression: graph_def.mutable_library
              identifier: graph_def
              .: .
              field_identifier: mutable_library
             argument_list: ()
              (: (
              ): )
           =: =
           call_expression: graph.flib_def().ToProto()
            field_expression: graph.flib_def().ToProto
             call_expression: graph.flib_def()
              field_expression: graph.flib_def
               identifier: graph
               .: .
               field_identifier: flib_def
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: ToProto
            argument_list: ()
             (: (
             ): )
          ;: ;
         if_statement: if (flags::Global().more_stack_traces.value()) {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
          if: if
          condition_clause: (flags::Global().more_stack_traces.value())
           (: (
           call_expression: flags::Global().more_stack_traces.value()
            field_expression: flags::Global().more_stack_traces.value
             field_expression: flags::Global().more_stack_traces
              call_expression: flags::Global()
               qualified_identifier: flags::Global
                namespace_identifier: flags
                ::: ::
                identifier: Global
               argument_list: ()
                (: (
                ): )
              .: .
              field_identifier: more_stack_traces
             .: .
             field_identifier: value
            argument_list: ()
             (: (
             ): )
           ): )
          compound_statement: {
        *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
      }
           {: {
           expression_statement: *graph_def.mutable_debug_info() = graph.BuildDebugInfo();
            assignment_expression: *graph_def.mutable_debug_info() = graph.BuildDebugInfo()
             pointer_expression: *graph_def.mutable_debug_info()
              *: *
              call_expression: graph_def.mutable_debug_info()
               field_expression: graph_def.mutable_debug_info
                identifier: graph_def
                .: .
                field_identifier: mutable_debug_info
               argument_list: ()
                (: (
                ): )
             =: =
             call_expression: graph.BuildDebugInfo()
              field_expression: graph.BuildDebugInfo
               identifier: graph
               .: .
               field_identifier: BuildDebugInfo
              argument_list: ()
               (: (
               ): )
            ;: ;
           }: }
         expression_statement: session->graph->mu.unlock();
          call_expression: session->graph->mu.unlock()
           field_expression: session->graph->mu.unlock
            field_expression: session->graph->mu
             field_expression: session->graph
              identifier: session
              ->: ->
              field_identifier: graph
             ->: ->
             field_identifier: mu
            .: .
            field_identifier: unlock
           argument_list: ()
            (: (
            ): )
          ;: ;
         expression_statement: status->status = session->session->Extend(std::move(graph_def));
          assignment_expression: status->status = session->session->Extend(std::move(graph_def))
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: session->session->Extend(std::move(graph_def))
            field_expression: session->session->Extend
             field_expression: session->session
              identifier: session
              ->: ->
              field_identifier: session
             ->: ->
             field_identifier: Extend
            argument_list: (std::move(graph_def))
             (: (
             call_expression: std::move(graph_def)
              qualified_identifier: std::move
               namespace_identifier: std
               ::: ::
               identifier: move
              argument_list: (graph_def)
               (: (
               identifier: graph_def
               ): )
             ): )
          ;: ;
         if_statement: if (!status->status.ok()) {
        // Contract is we always delete input_values[i].
        return false;
      }
          if: if
          condition_clause: (!status->status.ok())
           (: (
           unary_expression: !status->status.ok()
            !: !
            call_expression: status->status.ok()
             field_expression: status->status.ok
              field_expression: status->status
               identifier: status
               ->: ->
               field_identifier: status
              .: .
              field_identifier: ok
             argument_list: ()
              (: (
              ): )
           ): )
          compound_statement: {
        // Contract is we always delete input_values[i].
        return false;
      }
           {: {
           comment: // Contract is we always delete input_values[i].
           return_statement: return false;
            return: return
            false: false
            ;: ;
           }: }
         comment: // Note: session->session is not modified if Extend() fails, so
         comment: // we only set last_num_graph_nodes if it succeeds.
         expression_statement: session->last_num_graph_nodes = num_nodes;
          assignment_expression: session->last_num_graph_nodes = num_nodes
           field_expression: session->last_num_graph_nodes
            identifier: session
            ->: ->
            field_identifier: last_num_graph_nodes
           =: =
           identifier: num_nodes
          ;: ;
         }: }
        else_clause: else {
      session->graph->mu.unlock();
    }
         else: else
         compound_statement: {
      session->graph->mu.unlock();
    }
          {: {
          expression_statement: session->graph->mu.unlock();
           call_expression: session->graph->mu.unlock()
            field_expression: session->graph->mu.unlock
             field_expression: session->graph->mu
              field_expression: session->graph
               identifier: session
               ->: ->
               field_identifier: graph
              ->: ->
              field_identifier: mu
             .: .
             field_identifier: unlock
            argument_list: ()
             (: (
             ): )
           ;: ;
          }: }
       }: }
     return_statement: return true;
      return: return
      true: true
      ;: ;
     }: }
   }: }
 comment: // namespace tensorflow
 function_definition: static void TF_Run_Setup(int noutputs, TF_Tensor** c_outputs,
                         TF_Status* status) {
  status->status = absl::OkStatus();
  for (int i = 0; i < noutputs; ++i) {
    c_outputs[i] = nullptr;
  }
}
  storage_class_specifier: static
   static: static
  primitive_type: void
  function_declarator: TF_Run_Setup(int noutputs, TF_Tensor** c_outputs,
                         TF_Status* status)
   identifier: TF_Run_Setup
   parameter_list: (int noutputs, TF_Tensor** c_outputs,
                         TF_Status* status)
    (: (
    parameter_declaration: int noutputs
     primitive_type: int
     identifier: noutputs
    ,: ,
    parameter_declaration: TF_Tensor** c_outputs
     type_identifier: TF_Tensor
     pointer_declarator: ** c_outputs
      *: *
      pointer_declarator: * c_outputs
       *: *
       identifier: c_outputs
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  status->status = absl::OkStatus();
  for (int i = 0; i < noutputs; ++i) {
    c_outputs[i] = nullptr;
  }
}
   {: {
   expression_statement: status->status = absl::OkStatus();
    assignment_expression: status->status = absl::OkStatus()
     field_expression: status->status
      identifier: status
      ->: ->
      field_identifier: status
     =: =
     call_expression: absl::OkStatus()
      qualified_identifier: absl::OkStatus
       namespace_identifier: absl
       ::: ::
       identifier: OkStatus
      argument_list: ()
       (: (
       ): )
    ;: ;
   for_statement: for (int i = 0; i < noutputs; ++i) {
    c_outputs[i] = nullptr;
  }
    for: for
    (: (
    declaration: int i = 0;
     primitive_type: int
     init_declarator: i = 0
      identifier: i
      =: =
      number_literal: 0
     ;: ;
    binary_expression: i < noutputs
     identifier: i
     <: <
     identifier: noutputs
    ;: ;
    update_expression: ++i
     ++: ++
     identifier: i
    ): )
    compound_statement: {
    c_outputs[i] = nullptr;
  }
     {: {
     expression_statement: c_outputs[i] = nullptr;
      assignment_expression: c_outputs[i] = nullptr
       subscript_expression: c_outputs[i]
        identifier: c_outputs
        subscript_argument_list: [i]
         [: [
         identifier: i
         ]: ]
       =: =
       null: nullptr
        nullptr: nullptr
      ;: ;
     }: }
   }: }
 comment: // TF_TensorToTensorV1 decodes a string serialization to DT_RESOURCE.
 comment: // In the TFv1 convention, TF_Tensor can hold a string serialization of
 comment: // DT_RESOURCE. The string serialization is converted back to a
 comment: // ResourceHandle during Session run where the TF_Tensor is converted to a
 comment: // Tensor.
 comment: // TFv2 does not depend on this conversion. There is no matching
 comment: // TF_TensorFromTensorV1 because the conversion to string is performed by the
 comment: // python side of Session.
 function_definition: static Status TF_TensorToTensorV1(const TF_Tensor* src, Tensor* dst) {
  Status status = TF_TensorToTensor(src, dst);
  if (!status.ok()) {
    return status;
  }
  if (dst->dtype() == tensorflow::DT_RESOURCE) {
    const auto tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor);

    if (dst->dims() != 0) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
    *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape());
    if (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
    return absl::OkStatus();
  }
  return absl::OkStatus();
}
  storage_class_specifier: static
   static: static
  type_identifier: Status
  function_declarator: TF_TensorToTensorV1(const TF_Tensor* src, Tensor* dst)
   identifier: TF_TensorToTensorV1
   parameter_list: (const TF_Tensor* src, Tensor* dst)
    (: (
    parameter_declaration: const TF_Tensor* src
     type_qualifier: const
      const: const
     type_identifier: TF_Tensor
     pointer_declarator: * src
      *: *
      identifier: src
    ,: ,
    parameter_declaration: Tensor* dst
     type_identifier: Tensor
     pointer_declarator: * dst
      *: *
      identifier: dst
    ): )
  compound_statement: {
  Status status = TF_TensorToTensor(src, dst);
  if (!status.ok()) {
    return status;
  }
  if (dst->dtype() == tensorflow::DT_RESOURCE) {
    const auto tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor);

    if (dst->dims() != 0) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
    *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape());
    if (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
    return absl::OkStatus();
  }
  return absl::OkStatus();
}
   {: {
   declaration: Status status = TF_TensorToTensor(src, dst);
    type_identifier: Status
    init_declarator: status = TF_TensorToTensor(src, dst)
     identifier: status
     =: =
     call_expression: TF_TensorToTensor(src, dst)
      identifier: TF_TensorToTensor
      argument_list: (src, dst)
       (: (
       identifier: src
       ,: ,
       identifier: dst
       ): )
    ;: ;
   if_statement: if (!status.ok()) {
    return status;
  }
    if: if
    condition_clause: (!status.ok())
     (: (
     unary_expression: !status.ok()
      !: !
      call_expression: status.ok()
       field_expression: status.ok
        identifier: status
        .: .
        field_identifier: ok
       argument_list: ()
        (: (
        ): )
     ): )
    compound_statement: {
    return status;
  }
     {: {
     return_statement: return status;
      return: return
      identifier: status
      ;: ;
     }: }
   if_statement: if (dst->dtype() == tensorflow::DT_RESOURCE) {
    const auto tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor);

    if (dst->dims() != 0) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
    *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape());
    if (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
    return absl::OkStatus();
  }
    if: if
    condition_clause: (dst->dtype() == tensorflow::DT_RESOURCE)
     (: (
     binary_expression: dst->dtype() == tensorflow::DT_RESOURCE
      call_expression: dst->dtype()
       field_expression: dst->dtype
        identifier: dst
        ->: ->
        field_identifier: dtype
       argument_list: ()
        (: (
        ): )
      ==: ==
      qualified_identifier: tensorflow::DT_RESOURCE
       namespace_identifier: tensorflow
       ::: ::
       identifier: DT_RESOURCE
     ): )
    compound_statement: {
    const auto tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor);

    if (dst->dims() != 0) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
    *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape());
    if (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
    return absl::OkStatus();
  }
     {: {
     declaration: const auto tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: tensor_interface =
        tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor)
       identifier: tensor_interface
       =: =
       call_expression: tensorflow::down_cast<const tensorflow::TensorInterface*>(src->tensor)
        qualified_identifier: tensorflow::down_cast<const tensorflow::TensorInterface*>
         namespace_identifier: tensorflow
         ::: ::
         template_function: down_cast<const tensorflow::TensorInterface*>
          identifier: down_cast
          template_argument_list: <const tensorflow::TensorInterface*>
           <: <
           type_descriptor: const tensorflow::TensorInterface*
            type_qualifier: const
             const: const
            qualified_identifier: tensorflow::TensorInterface
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: TensorInterface
            abstract_pointer_declarator: *
             *: *
           >: >
        argument_list: (src->tensor)
         (: (
         field_expression: src->tensor
          identifier: src
          ->: ->
          field_identifier: tensor
         ): )
      ;: ;
     if_statement: if (dst->dims() != 0) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
      if: if
      condition_clause: (dst->dims() != 0)
       (: (
       binary_expression: dst->dims() != 0
        call_expression: dst->dims()
         field_expression: dst->dims
          identifier: dst
          ->: ->
          field_identifier: dims
         argument_list: ()
          (: (
          ): )
        !=: !=
        number_literal: 0
       ): )
      compound_statement: {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
    }
       {: {
       return_statement: return InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString());
        return: return
        call_expression: InvalidArgument(
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString())
         identifier: InvalidArgument
         argument_list: (
          "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape ",
          dst->shape().DebugString())
          (: (
          concatenated_string: "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
          "shape "
           string_literal: "Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with "
            ": "
            string_content: Malformed TF_RESOURCE tensor: expected a scalar, got a tensor with 
            ": "
           string_literal: "shape "
            ": "
            string_content: shape 
            ": "
          ,: ,
          call_expression: dst->shape().DebugString()
           field_expression: dst->shape().DebugString
            call_expression: dst->shape()
             field_expression: dst->shape
              identifier: dst
              ->: ->
              field_identifier: shape
             argument_list: ()
              (: (
              ): )
            .: .
            field_identifier: DebugString
           argument_list: ()
            (: (
            ): )
          ): )
        ;: ;
       }: }
     expression_statement: *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape());
      assignment_expression: *dst = tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape())
       pointer_expression: *dst
        *: *
        identifier: dst
       =: =
       call_expression: tensorflow::Tensor(tensorflow::DT_RESOURCE, dst->shape())
        qualified_identifier: tensorflow::Tensor
         namespace_identifier: tensorflow
         ::: ::
         identifier: Tensor
        argument_list: (tensorflow::DT_RESOURCE, dst->shape())
         (: (
         qualified_identifier: tensorflow::DT_RESOURCE
          namespace_identifier: tensorflow
          ::: ::
          identifier: DT_RESOURCE
         ,: ,
         call_expression: dst->shape()
          field_expression: dst->shape
           identifier: dst
           ->: ->
           field_identifier: shape
          argument_list: ()
           (: (
           ): )
         ): )
      ;: ;
     if_statement: if (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))) {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
      if: if
      condition_clause: (!dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize())))
       (: (
       unary_expression: !dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))
        !: !
        call_expression: dst->scalar<tensorflow::ResourceHandle>()().ParseFromString(
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))
         field_expression: dst->scalar<tensorflow::ResourceHandle>()().ParseFromString
          call_expression: dst->scalar<tensorflow::ResourceHandle>()()
           call_expression: dst->scalar<tensorflow::ResourceHandle>()
            field_expression: dst->scalar<tensorflow::ResourceHandle>
             identifier: dst
             ->: ->
             template_method: scalar<tensorflow::ResourceHandle>
              field_identifier: scalar
              template_argument_list: <tensorflow::ResourceHandle>
               <: <
               type_descriptor: tensorflow::ResourceHandle
                qualified_identifier: tensorflow::ResourceHandle
                 namespace_identifier: tensorflow
                 ::: ::
                 type_identifier: ResourceHandle
               >: >
            argument_list: ()
             (: (
             ): )
           argument_list: ()
            (: (
            ): )
          .: .
          field_identifier: ParseFromString
         argument_list: (
            string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize()))
          (: (
          call_expression: string(static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize())
           identifier: string
           argument_list: (static_cast<const char*>(tensor_interface->Data()),
                   tensor_interface->ByteSize())
            (: (
            call_expression: static_cast<const char*>(tensor_interface->Data())
             template_function: static_cast<const char*>
              identifier: static_cast
              template_argument_list: <const char*>
               <: <
               type_descriptor: const char*
                type_qualifier: const
                 const: const
                primitive_type: char
                abstract_pointer_declarator: *
                 *: *
               >: >
             argument_list: (tensor_interface->Data())
              (: (
              call_expression: tensor_interface->Data()
               field_expression: tensor_interface->Data
                identifier: tensor_interface
                ->: ->
                field_identifier: Data
               argument_list: ()
                (: (
                ): )
              ): )
            ,: ,
            call_expression: tensor_interface->ByteSize()
             field_expression: tensor_interface->ByteSize
              identifier: tensor_interface
              ->: ->
              field_identifier: ByteSize
             argument_list: ()
              (: (
              ): )
            ): )
          ): )
       ): )
      compound_statement: {
      return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
    }
       {: {
       return_statement: return InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle");
        return: return
        call_expression: InvalidArgument(
          "Malformed TF_RESOURCE tensor: unable to parse resource handle")
         identifier: InvalidArgument
         argument_list: (
          "Malformed TF_RESOURCE tensor: unable to parse resource handle")
          (: (
          string_literal: "Malformed TF_RESOURCE tensor: unable to parse resource handle"
           ": "
           string_content: Malformed TF_RESOURCE tensor: unable to parse resource handle
           ": "
          ): )
        ;: ;
       }: }
     return_statement: return absl::OkStatus();
      return: return
      call_expression: absl::OkStatus()
       qualified_identifier: absl::OkStatus
        namespace_identifier: absl
        ::: ::
        identifier: OkStatus
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   return_statement: return absl::OkStatus();
    return: return
    call_expression: absl::OkStatus()
     qualified_identifier: absl::OkStatus
      namespace_identifier: absl
      ::: ::
      identifier: OkStatus
     argument_list: ()
      (: (
      ): )
    ;: ;
   }: }
 function_definition: static bool TF_Run_Inputs(TF_Tensor* const* c_inputs,
                          std::vector<std::pair<string, Tensor>>* input_pairs,
                          TF_Status* status) {
  const int ninputs = input_pairs->size();
  for (int i = 0; i < ninputs; ++i) {
    status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second);
    if (!status->status.ok()) return false;
  }
  return true;
}
  storage_class_specifier: static
   static: static
  primitive_type: bool
  function_declarator: TF_Run_Inputs(TF_Tensor* const* c_inputs,
                          std::vector<std::pair<string, Tensor>>* input_pairs,
                          TF_Status* status)
   identifier: TF_Run_Inputs
   parameter_list: (TF_Tensor* const* c_inputs,
                          std::vector<std::pair<string, Tensor>>* input_pairs,
                          TF_Status* status)
    (: (
    parameter_declaration: TF_Tensor* const* c_inputs
     type_identifier: TF_Tensor
     pointer_declarator: * const* c_inputs
      *: *
      type_qualifier: const
       const: const
      pointer_declarator: * c_inputs
       *: *
       identifier: c_inputs
    ,: ,
    parameter_declaration: std::vector<std::pair<string, Tensor>>* input_pairs
     qualified_identifier: std::vector<std::pair<string, Tensor>>
      namespace_identifier: std
      ::: ::
      template_type: vector<std::pair<string, Tensor>>
       type_identifier: vector
       template_argument_list: <std::pair<string, Tensor>>
        <: <
        type_descriptor: std::pair<string, Tensor>
         qualified_identifier: std::pair<string, Tensor>
          namespace_identifier: std
          ::: ::
          template_type: pair<string, Tensor>
           type_identifier: pair
           template_argument_list: <string, Tensor>
            <: <
            type_descriptor: string
             type_identifier: string
            ,: ,
            type_descriptor: Tensor
             type_identifier: Tensor
            >: >
        >: >
     pointer_declarator: * input_pairs
      *: *
      identifier: input_pairs
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  const int ninputs = input_pairs->size();
  for (int i = 0; i < ninputs; ++i) {
    status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second);
    if (!status->status.ok()) return false;
  }
  return true;
}
   {: {
   declaration: const int ninputs = input_pairs->size();
    type_qualifier: const
     const: const
    primitive_type: int
    init_declarator: ninputs = input_pairs->size()
     identifier: ninputs
     =: =
     call_expression: input_pairs->size()
      field_expression: input_pairs->size
       identifier: input_pairs
       ->: ->
       field_identifier: size
      argument_list: ()
       (: (
       ): )
    ;: ;
   for_statement: for (int i = 0; i < ninputs; ++i) {
    status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second);
    if (!status->status.ok()) return false;
  }
    for: for
    (: (
    declaration: int i = 0;
     primitive_type: int
     init_declarator: i = 0
      identifier: i
      =: =
      number_literal: 0
     ;: ;
    binary_expression: i < ninputs
     identifier: i
     <: <
     identifier: ninputs
    ;: ;
    update_expression: ++i
     ++: ++
     identifier: i
    ): )
    compound_statement: {
    status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second);
    if (!status->status.ok()) return false;
  }
     {: {
     expression_statement: status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second);
      assignment_expression: status->status =
        TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: TF_TensorToTensorV1(c_inputs[i], &(*input_pairs)[i].second)
        identifier: TF_TensorToTensorV1
        argument_list: (c_inputs[i], &(*input_pairs)[i].second)
         (: (
         subscript_expression: c_inputs[i]
          identifier: c_inputs
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         ,: ,
         pointer_expression: &(*input_pairs)[i].second
          &: &
          field_expression: (*input_pairs)[i].second
           subscript_expression: (*input_pairs)[i]
            parenthesized_expression: (*input_pairs)
             (: (
             pointer_expression: *input_pairs
              *: *
              identifier: input_pairs
             ): )
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           .: .
           field_identifier: second
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return false;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return false;
       return: return
       false: false
       ;: ;
     }: }
   return_statement: return true;
    return: return
    true: true
    ;: ;
   }: }
 comment: // Create an empty tensor of type 'dtype'. 'shape' can be arbitrary, but has to
 comment: // result in a zero-sized tensor.
 function_definition: static TF_Tensor* EmptyTensor(TF_DataType dtype,
                              const tensorflow::TensorShape& shape) {
  static char empty;
  int64_t nelems = 1;
  std::vector<int64_t> dims;
  dims.reserve(shape.dims());
  for (int i = 0; i < shape.dims(); ++i) {
    dims.push_back(shape.dim_size(i));
    nelems *= shape.dim_size(i);
  }
  CHECK_EQ(nelems, 0);
  return TF_NewTensor(
      dtype, reinterpret_cast<const int64_t*>(dims.data()), shape.dims(),
      reinterpret_cast<void*>(&empty), 0, [](void*, size_t, void*) {}, nullptr);
}
  storage_class_specifier: static
   static: static
  type_identifier: TF_Tensor
  pointer_declarator: * EmptyTensor(TF_DataType dtype,
                              const tensorflow::TensorShape& shape)
   *: *
   function_declarator: EmptyTensor(TF_DataType dtype,
                              const tensorflow::TensorShape& shape)
    identifier: EmptyTensor
    parameter_list: (TF_DataType dtype,
                              const tensorflow::TensorShape& shape)
     (: (
     parameter_declaration: TF_DataType dtype
      type_identifier: TF_DataType
      identifier: dtype
     ,: ,
     parameter_declaration: const tensorflow::TensorShape& shape
      type_qualifier: const
       const: const
      qualified_identifier: tensorflow::TensorShape
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: TensorShape
      reference_declarator: & shape
       &: &
       identifier: shape
     ): )
  compound_statement: {
  static char empty;
  int64_t nelems = 1;
  std::vector<int64_t> dims;
  dims.reserve(shape.dims());
  for (int i = 0; i < shape.dims(); ++i) {
    dims.push_back(shape.dim_size(i));
    nelems *= shape.dim_size(i);
  }
  CHECK_EQ(nelems, 0);
  return TF_NewTensor(
      dtype, reinterpret_cast<const int64_t*>(dims.data()), shape.dims(),
      reinterpret_cast<void*>(&empty), 0, [](void*, size_t, void*) {}, nullptr);
}
   {: {
   declaration: static char empty;
    storage_class_specifier: static
     static: static
    primitive_type: char
    identifier: empty
    ;: ;
   declaration: int64_t nelems = 1;
    primitive_type: int64_t
    init_declarator: nelems = 1
     identifier: nelems
     =: =
     number_literal: 1
    ;: ;
   declaration: std::vector<int64_t> dims;
    qualified_identifier: std::vector<int64_t>
     namespace_identifier: std
     ::: ::
     template_type: vector<int64_t>
      type_identifier: vector
      template_argument_list: <int64_t>
       <: <
       type_descriptor: int64_t
        primitive_type: int64_t
       >: >
    identifier: dims
    ;: ;
   expression_statement: dims.reserve(shape.dims());
    call_expression: dims.reserve(shape.dims())
     field_expression: dims.reserve
      identifier: dims
      .: .
      field_identifier: reserve
     argument_list: (shape.dims())
      (: (
      call_expression: shape.dims()
       field_expression: shape.dims
        identifier: shape
        .: .
        field_identifier: dims
       argument_list: ()
        (: (
        ): )
      ): )
    ;: ;
   for_statement: for (int i = 0; i < shape.dims(); ++i) {
    dims.push_back(shape.dim_size(i));
    nelems *= shape.dim_size(i);
  }
    for: for
    (: (
    declaration: int i = 0;
     primitive_type: int
     init_declarator: i = 0
      identifier: i
      =: =
      number_literal: 0
     ;: ;
    binary_expression: i < shape.dims()
     identifier: i
     <: <
     call_expression: shape.dims()
      field_expression: shape.dims
       identifier: shape
       .: .
       field_identifier: dims
      argument_list: ()
       (: (
       ): )
    ;: ;
    update_expression: ++i
     ++: ++
     identifier: i
    ): )
    compound_statement: {
    dims.push_back(shape.dim_size(i));
    nelems *= shape.dim_size(i);
  }
     {: {
     expression_statement: dims.push_back(shape.dim_size(i));
      call_expression: dims.push_back(shape.dim_size(i))
       field_expression: dims.push_back
        identifier: dims
        .: .
        field_identifier: push_back
       argument_list: (shape.dim_size(i))
        (: (
        call_expression: shape.dim_size(i)
         field_expression: shape.dim_size
          identifier: shape
          .: .
          field_identifier: dim_size
         argument_list: (i)
          (: (
          identifier: i
          ): )
        ): )
      ;: ;
     expression_statement: nelems *= shape.dim_size(i);
      assignment_expression: nelems *= shape.dim_size(i)
       identifier: nelems
       *=: *=
       call_expression: shape.dim_size(i)
        field_expression: shape.dim_size
         identifier: shape
         .: .
         field_identifier: dim_size
        argument_list: (i)
         (: (
         identifier: i
         ): )
      ;: ;
     }: }
   expression_statement: CHECK_EQ(nelems, 0);
    call_expression: CHECK_EQ(nelems, 0)
     identifier: CHECK_EQ
     argument_list: (nelems, 0)
      (: (
      identifier: nelems
      ,: ,
      number_literal: 0
      ): )
    ;: ;
   return_statement: return TF_NewTensor(
      dtype, reinterpret_cast<const int64_t*>(dims.data()), shape.dims(),
      reinterpret_cast<void*>(&empty), 0, [](void*, size_t, void*) {}, nullptr);
    return: return
    call_expression: TF_NewTensor(
      dtype, reinterpret_cast<const int64_t*>(dims.data()), shape.dims(),
      reinterpret_cast<void*>(&empty), 0, [](void*, size_t, void*) {}, nullptr)
     identifier: TF_NewTensor
     argument_list: (
      dtype, reinterpret_cast<const int64_t*>(dims.data()), shape.dims(),
      reinterpret_cast<void*>(&empty), 0, [](void*, size_t, void*) {}, nullptr)
      (: (
      identifier: dtype
      ,: ,
      call_expression: reinterpret_cast<const int64_t*>(dims.data())
       template_function: reinterpret_cast<const int64_t*>
        identifier: reinterpret_cast
        template_argument_list: <const int64_t*>
         <: <
         type_descriptor: const int64_t*
          type_qualifier: const
           const: const
          primitive_type: int64_t
          abstract_pointer_declarator: *
           *: *
         >: >
       argument_list: (dims.data())
        (: (
        call_expression: dims.data()
         field_expression: dims.data
          identifier: dims
          .: .
          field_identifier: data
         argument_list: ()
          (: (
          ): )
        ): )
      ,: ,
      call_expression: shape.dims()
       field_expression: shape.dims
        identifier: shape
        .: .
        field_identifier: dims
       argument_list: ()
        (: (
        ): )
      ,: ,
      call_expression: reinterpret_cast<void*>(&empty)
       template_function: reinterpret_cast<void*>
        identifier: reinterpret_cast
        template_argument_list: <void*>
         <: <
         type_descriptor: void*
          primitive_type: void
          abstract_pointer_declarator: *
           *: *
         >: >
       argument_list: (&empty)
        (: (
        pointer_expression: &empty
         &: &
         identifier: empty
        ): )
      ,: ,
      number_literal: 0
      ,: ,
      lambda_expression: [](void*, size_t, void*) {}
       lambda_capture_specifier: []
        [: [
        ]: ]
       lambda_declarator: (void*, size_t, void*)
        parameter_list: (void*, size_t, void*)
         (: (
         parameter_declaration: void*
          primitive_type: void
          abstract_pointer_declarator: *
           *: *
         ,: ,
         parameter_declaration: size_t
          primitive_type: size_t
         ,: ,
         parameter_declaration: void*
          primitive_type: void
          abstract_pointer_declarator: *
           *: *
         ): )
       compound_statement: {}
        {: {
        }: }
      ,: ,
      null: nullptr
       nullptr: nullptr
      ): )
    ;: ;
   }: }
 function_definition: static void TF_Run_Helper(
    Session* session, const char* handle, const TF_Buffer* run_options,
    // Input tensors
    const std::vector<std::pair<string, Tensor>>& input_pairs,
    // Output tensors
    const std::vector<string>& output_tensor_names, TF_Tensor** c_outputs,
    // Target nodes
    const std::vector<string>& target_oper_names, TF_Buffer* run_metadata,
    TF_Status* status) {
  const int noutputs = output_tensor_names.size();
  std::vector<Tensor> outputs(noutputs);
  Status result;

  if (handle == nullptr) {
    RunOptions run_options_proto;
    if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)) {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
    if (run_metadata != nullptr && run_metadata->data != nullptr) {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }

    RunMetadata run_metadata_proto;
    result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto);

    // Serialize back to upstream client, who now owns the new buffer
    if (run_metadata != nullptr) {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
  } else {
    // NOTE(zongheng): PRun does not support RunOptions yet.
    result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
  }
  if (!result.ok()) {
    status->status = result;
    return;
  }

  // Store results in c_outputs[]
  for (int i = 0; i < noutputs; ++i) {
    const Tensor& src = outputs[i];
    if (!src.IsInitialized() || src.NumElements() == 0) {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
    c_outputs[i] = TF_TensorFromTensor(src, &status->status);
    if (!status->status.ok()) return;
  }
}
  storage_class_specifier: static
   static: static
  primitive_type: void
  function_declarator: TF_Run_Helper(
    Session* session, const char* handle, const TF_Buffer* run_options,
    // Input tensors
    const std::vector<std::pair<string, Tensor>>& input_pairs,
    // Output tensors
    const std::vector<string>& output_tensor_names, TF_Tensor** c_outputs,
    // Target nodes
    const std::vector<string>& target_oper_names, TF_Buffer* run_metadata,
    TF_Status* status)
   identifier: TF_Run_Helper
   parameter_list: (
    Session* session, const char* handle, const TF_Buffer* run_options,
    // Input tensors
    const std::vector<std::pair<string, Tensor>>& input_pairs,
    // Output tensors
    const std::vector<string>& output_tensor_names, TF_Tensor** c_outputs,
    // Target nodes
    const std::vector<string>& target_oper_names, TF_Buffer* run_metadata,
    TF_Status* status)
    (: (
    parameter_declaration: Session* session
     type_identifier: Session
     pointer_declarator: * session
      *: *
      identifier: session
    ,: ,
    parameter_declaration: const char* handle
     type_qualifier: const
      const: const
     primitive_type: char
     pointer_declarator: * handle
      *: *
      identifier: handle
    ,: ,
    parameter_declaration: const TF_Buffer* run_options
     type_qualifier: const
      const: const
     type_identifier: TF_Buffer
     pointer_declarator: * run_options
      *: *
      identifier: run_options
    ,: ,
    comment: // Input tensors
    parameter_declaration: const std::vector<std::pair<string, Tensor>>& input_pairs
     type_qualifier: const
      const: const
     qualified_identifier: std::vector<std::pair<string, Tensor>>
      namespace_identifier: std
      ::: ::
      template_type: vector<std::pair<string, Tensor>>
       type_identifier: vector
       template_argument_list: <std::pair<string, Tensor>>
        <: <
        type_descriptor: std::pair<string, Tensor>
         qualified_identifier: std::pair<string, Tensor>
          namespace_identifier: std
          ::: ::
          template_type: pair<string, Tensor>
           type_identifier: pair
           template_argument_list: <string, Tensor>
            <: <
            type_descriptor: string
             type_identifier: string
            ,: ,
            type_descriptor: Tensor
             type_identifier: Tensor
            >: >
        >: >
     reference_declarator: & input_pairs
      &: &
      identifier: input_pairs
    ,: ,
    comment: // Output tensors
    parameter_declaration: const std::vector<string>& output_tensor_names
     type_qualifier: const
      const: const
     qualified_identifier: std::vector<string>
      namespace_identifier: std
      ::: ::
      template_type: vector<string>
       type_identifier: vector
       template_argument_list: <string>
        <: <
        type_descriptor: string
         type_identifier: string
        >: >
     reference_declarator: & output_tensor_names
      &: &
      identifier: output_tensor_names
    ,: ,
    parameter_declaration: TF_Tensor** c_outputs
     type_identifier: TF_Tensor
     pointer_declarator: ** c_outputs
      *: *
      pointer_declarator: * c_outputs
       *: *
       identifier: c_outputs
    ,: ,
    comment: // Target nodes
    parameter_declaration: const std::vector<string>& target_oper_names
     type_qualifier: const
      const: const
     qualified_identifier: std::vector<string>
      namespace_identifier: std
      ::: ::
      template_type: vector<string>
       type_identifier: vector
       template_argument_list: <string>
        <: <
        type_descriptor: string
         type_identifier: string
        >: >
     reference_declarator: & target_oper_names
      &: &
      identifier: target_oper_names
    ,: ,
    parameter_declaration: TF_Buffer* run_metadata
     type_identifier: TF_Buffer
     pointer_declarator: * run_metadata
      *: *
      identifier: run_metadata
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  const int noutputs = output_tensor_names.size();
  std::vector<Tensor> outputs(noutputs);
  Status result;

  if (handle == nullptr) {
    RunOptions run_options_proto;
    if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)) {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
    if (run_metadata != nullptr && run_metadata->data != nullptr) {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }

    RunMetadata run_metadata_proto;
    result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto);

    // Serialize back to upstream client, who now owns the new buffer
    if (run_metadata != nullptr) {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
  } else {
    // NOTE(zongheng): PRun does not support RunOptions yet.
    result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
  }
  if (!result.ok()) {
    status->status = result;
    return;
  }

  // Store results in c_outputs[]
  for (int i = 0; i < noutputs; ++i) {
    const Tensor& src = outputs[i];
    if (!src.IsInitialized() || src.NumElements() == 0) {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
    c_outputs[i] = TF_TensorFromTensor(src, &status->status);
    if (!status->status.ok()) return;
  }
}
   {: {
   declaration: const int noutputs = output_tensor_names.size();
    type_qualifier: const
     const: const
    primitive_type: int
    init_declarator: noutputs = output_tensor_names.size()
     identifier: noutputs
     =: =
     call_expression: output_tensor_names.size()
      field_expression: output_tensor_names.size
       identifier: output_tensor_names
       .: .
       field_identifier: size
      argument_list: ()
       (: (
       ): )
    ;: ;
   declaration: std::vector<Tensor> outputs(noutputs);
    qualified_identifier: std::vector<Tensor>
     namespace_identifier: std
     ::: ::
     template_type: vector<Tensor>
      type_identifier: vector
      template_argument_list: <Tensor>
       <: <
       type_descriptor: Tensor
        type_identifier: Tensor
       >: >
    function_declarator: outputs(noutputs)
     identifier: outputs
     parameter_list: (noutputs)
      (: (
      parameter_declaration: noutputs
       type_identifier: noutputs
      ): )
    ;: ;
   declaration: Status result;
    type_identifier: Status
    identifier: result
    ;: ;
   if_statement: if (handle == nullptr) {
    RunOptions run_options_proto;
    if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)) {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
    if (run_metadata != nullptr && run_metadata->data != nullptr) {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }

    RunMetadata run_metadata_proto;
    result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto);

    // Serialize back to upstream client, who now owns the new buffer
    if (run_metadata != nullptr) {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
  } else {
    // NOTE(zongheng): PRun does not support RunOptions yet.
    result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
  }
    if: if
    condition_clause: (handle == nullptr)
     (: (
     binary_expression: handle == nullptr
      identifier: handle
      ==: ==
      null: nullptr
       nullptr: nullptr
     ): )
    compound_statement: {
    RunOptions run_options_proto;
    if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)) {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
    if (run_metadata != nullptr && run_metadata->data != nullptr) {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }

    RunMetadata run_metadata_proto;
    result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto);

    // Serialize back to upstream client, who now owns the new buffer
    if (run_metadata != nullptr) {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
  }
     {: {
     declaration: RunOptions run_options_proto;
      type_identifier: RunOptions
      identifier: run_options_proto
      ;: ;
     if_statement: if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)) {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
      if: if
      condition_clause: (run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length))
       (: (
       binary_expression: run_options != nullptr && !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)
        binary_expression: run_options != nullptr
         identifier: run_options
         !=: !=
         null: nullptr
          nullptr: nullptr
        &&: &&
        unary_expression: !run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)
         !: !
         call_expression: run_options_proto.ParseFromArray(
                                      run_options->data, run_options->length)
          field_expression: run_options_proto.ParseFromArray
           identifier: run_options_proto
           .: .
           field_identifier: ParseFromArray
          argument_list: (
                                      run_options->data, run_options->length)
           (: (
           field_expression: run_options->data
            identifier: run_options
            ->: ->
            field_identifier: data
           ,: ,
           field_expression: run_options->length
            identifier: run_options
            ->: ->
            field_identifier: length
           ): )
       ): )
      compound_statement: {
      status->status = InvalidArgument("Unparseable RunOptions proto");
      return;
    }
       {: {
       expression_statement: status->status = InvalidArgument("Unparseable RunOptions proto");
        assignment_expression: status->status = InvalidArgument("Unparseable RunOptions proto")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Unparseable RunOptions proto")
          identifier: InvalidArgument
          argument_list: ("Unparseable RunOptions proto")
           (: (
           string_literal: "Unparseable RunOptions proto"
            ": "
            string_content: Unparseable RunOptions proto
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     if_statement: if (run_metadata != nullptr && run_metadata->data != nullptr) {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }
      if: if
      condition_clause: (run_metadata != nullptr && run_metadata->data != nullptr)
       (: (
       binary_expression: run_metadata != nullptr && run_metadata->data != nullptr
        binary_expression: run_metadata != nullptr
         identifier: run_metadata
         !=: !=
         null: nullptr
          nullptr: nullptr
        &&: &&
        binary_expression: run_metadata->data != nullptr
         field_expression: run_metadata->data
          identifier: run_metadata
          ->: ->
          field_identifier: data
         !=: !=
         null: nullptr
          nullptr: nullptr
       ): )
      compound_statement: {
      status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
      return;
    }
       {: {
       expression_statement: status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.");
        assignment_expression: status->status =
          InvalidArgument("Passing non-empty run_metadata is invalid.")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Passing non-empty run_metadata is invalid.")
          identifier: InvalidArgument
          argument_list: ("Passing non-empty run_metadata is invalid.")
           (: (
           string_literal: "Passing non-empty run_metadata is invalid."
            ": "
            string_content: Passing non-empty run_metadata is invalid.
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: RunMetadata run_metadata_proto;
      type_identifier: RunMetadata
      identifier: run_metadata_proto
      ;: ;
     expression_statement: result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto);
      assignment_expression: result = session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto)
       identifier: result
       =: =
       call_expression: session->Run(run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto)
        field_expression: session->Run
         identifier: session
         ->: ->
         field_identifier: Run
        argument_list: (run_options_proto, input_pairs, output_tensor_names,
                          target_oper_names, &outputs, &run_metadata_proto)
         (: (
         identifier: run_options_proto
         ,: ,
         identifier: input_pairs
         ,: ,
         identifier: output_tensor_names
         ,: ,
         identifier: target_oper_names
         ,: ,
         pointer_expression: &outputs
          &: &
          identifier: outputs
         ,: ,
         pointer_expression: &run_metadata_proto
          &: &
          identifier: run_metadata_proto
         ): )
      ;: ;
     comment: // Serialize back to upstream client, who now owns the new buffer
     if_statement: if (run_metadata != nullptr) {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
      if: if
      condition_clause: (run_metadata != nullptr)
       (: (
       binary_expression: run_metadata != nullptr
        identifier: run_metadata
        !=: !=
        null: nullptr
         nullptr: nullptr
       ): )
      compound_statement: {
      status->status = MessageToBuffer(run_metadata_proto, run_metadata);
      if (!status->status.ok()) return;
    }
       {: {
       expression_statement: status->status = MessageToBuffer(run_metadata_proto, run_metadata);
        assignment_expression: status->status = MessageToBuffer(run_metadata_proto, run_metadata)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: MessageToBuffer(run_metadata_proto, run_metadata)
          identifier: MessageToBuffer
          argument_list: (run_metadata_proto, run_metadata)
           (: (
           identifier: run_metadata_proto
           ,: ,
           identifier: run_metadata
           ): )
        ;: ;
       if_statement: if (!status->status.ok()) return;
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        return_statement: return;
         return: return
         ;: ;
       }: }
     }: }
    else_clause: else {
    // NOTE(zongheng): PRun does not support RunOptions yet.
    result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
  }
     else: else
     compound_statement: {
    // NOTE(zongheng): PRun does not support RunOptions yet.
    result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
  }
      {: {
      comment: // NOTE(zongheng): PRun does not support RunOptions yet.
      expression_statement: result = session->PRun(handle, input_pairs, output_tensor_names, &outputs);
       assignment_expression: result = session->PRun(handle, input_pairs, output_tensor_names, &outputs)
        identifier: result
        =: =
        call_expression: session->PRun(handle, input_pairs, output_tensor_names, &outputs)
         field_expression: session->PRun
          identifier: session
          ->: ->
          field_identifier: PRun
         argument_list: (handle, input_pairs, output_tensor_names, &outputs)
          (: (
          identifier: handle
          ,: ,
          identifier: input_pairs
          ,: ,
          identifier: output_tensor_names
          ,: ,
          pointer_expression: &outputs
           &: &
           identifier: outputs
          ): )
       ;: ;
      }: }
   if_statement: if (!result.ok()) {
    status->status = result;
    return;
  }
    if: if
    condition_clause: (!result.ok())
     (: (
     unary_expression: !result.ok()
      !: !
      call_expression: result.ok()
       field_expression: result.ok
        identifier: result
        .: .
        field_identifier: ok
       argument_list: ()
        (: (
        ): )
     ): )
    compound_statement: {
    status->status = result;
    return;
  }
     {: {
     expression_statement: status->status = result;
      assignment_expression: status->status = result
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       identifier: result
      ;: ;
     return_statement: return;
      return: return
      ;: ;
     }: }
   comment: // Store results in c_outputs[]
   for_statement: for (int i = 0; i < noutputs; ++i) {
    const Tensor& src = outputs[i];
    if (!src.IsInitialized() || src.NumElements() == 0) {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
    c_outputs[i] = TF_TensorFromTensor(src, &status->status);
    if (!status->status.ok()) return;
  }
    for: for
    (: (
    declaration: int i = 0;
     primitive_type: int
     init_declarator: i = 0
      identifier: i
      =: =
      number_literal: 0
     ;: ;
    binary_expression: i < noutputs
     identifier: i
     <: <
     identifier: noutputs
    ;: ;
    update_expression: ++i
     ++: ++
     identifier: i
    ): )
    compound_statement: {
    const Tensor& src = outputs[i];
    if (!src.IsInitialized() || src.NumElements() == 0) {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
    c_outputs[i] = TF_TensorFromTensor(src, &status->status);
    if (!status->status.ok()) return;
  }
     {: {
     declaration: const Tensor& src = outputs[i];
      type_qualifier: const
       const: const
      type_identifier: Tensor
      init_declarator: & src = outputs[i]
       reference_declarator: & src
        &: &
        identifier: src
       =: =
       subscript_expression: outputs[i]
        identifier: outputs
        subscript_argument_list: [i]
         [: [
         identifier: i
         ]: ]
      ;: ;
     if_statement: if (!src.IsInitialized() || src.NumElements() == 0) {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
      if: if
      condition_clause: (!src.IsInitialized() || src.NumElements() == 0)
       (: (
       binary_expression: !src.IsInitialized() || src.NumElements() == 0
        unary_expression: !src.IsInitialized()
         !: !
         call_expression: src.IsInitialized()
          field_expression: src.IsInitialized
           identifier: src
           .: .
           field_identifier: IsInitialized
          argument_list: ()
           (: (
           ): )
        ||: ||
        binary_expression: src.NumElements() == 0
         call_expression: src.NumElements()
          field_expression: src.NumElements
           identifier: src
           .: .
           field_identifier: NumElements
          argument_list: ()
           (: (
           ): )
         ==: ==
         number_literal: 0
       ): )
      compound_statement: {
      c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
      continue;
    }
       {: {
       expression_statement: c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape());
        assignment_expression: c_outputs[i] =
          EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape())
         subscript_expression: c_outputs[i]
          identifier: c_outputs
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: EmptyTensor(static_cast<TF_DataType>(src.dtype()), src.shape())
          identifier: EmptyTensor
          argument_list: (static_cast<TF_DataType>(src.dtype()), src.shape())
           (: (
           call_expression: static_cast<TF_DataType>(src.dtype())
            template_function: static_cast<TF_DataType>
             identifier: static_cast
             template_argument_list: <TF_DataType>
              <: <
              type_descriptor: TF_DataType
               type_identifier: TF_DataType
              >: >
            argument_list: (src.dtype())
             (: (
             call_expression: src.dtype()
              field_expression: src.dtype
               identifier: src
               .: .
               field_identifier: dtype
              argument_list: ()
               (: (
               ): )
             ): )
           ,: ,
           call_expression: src.shape()
            field_expression: src.shape
             identifier: src
             .: .
             field_identifier: shape
            argument_list: ()
             (: (
             ): )
           ): )
        ;: ;
       continue_statement: continue;
        continue: continue
        ;: ;
       }: }
     expression_statement: c_outputs[i] = TF_TensorFromTensor(src, &status->status);
      assignment_expression: c_outputs[i] = TF_TensorFromTensor(src, &status->status)
       subscript_expression: c_outputs[i]
        identifier: c_outputs
        subscript_argument_list: [i]
         [: [
         identifier: i
         ]: ]
       =: =
       call_expression: TF_TensorFromTensor(src, &status->status)
        identifier: TF_TensorFromTensor
        argument_list: (src, &status->status)
         (: (
         identifier: src
         ,: ,
         pointer_expression: &status->status
          &: &
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     }: }
   }: }
 linkage_specification: extern "C" {

void TF_Run(TF_DeprecatedSession* s, const TF_Buffer* run_options,
            // Input tensors
            const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
            // Output tensors
            const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
            // Target nodes
            const char** c_target_oper_names, int ntargets,
            TF_Buffer* run_metadata, TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status);
}

void TF_PRunSetup(TF_DeprecatedSession* s,
                  // Input names
                  const char** c_input_names, int ninputs,
                  // Output names
                  const char** c_output_names, int noutputs,
                  // Target nodes
                  const char** c_target_oper_names, int ntargets,
                  const char** handle, TF_Status* status) {
  *handle = nullptr;

  std::vector<string> input_names(ninputs);
  std::vector<string> output_names(noutputs);
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = c_input_names[i];
  }
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  string new_handle;
  status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}

void TF_PRun(TF_DeprecatedSession* s, const char* handle,
             // Input tensors
             const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
             // Output tensors
             const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
             // Target nodes
             const char** c_target_oper_names, int ntargets,
             TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status);
}

TF_Library* TF_LoadLibrary(const char* library_filename, TF_Status* status) {
  TF_Library* lib_handle = new TF_Library;
  status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length);
  if (!status->status.ok()) {
    delete lib_handle;
    return nullptr;
  }
  return lib_handle;
}

TF_Buffer TF_GetOpList(TF_Library* lib_handle) { return lib_handle->op_list; }

void TF_DeleteLibraryHandle(TF_Library* lib_handle) {
  if (lib_handle == nullptr) return;
  tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data));
  delete lib_handle;
}

TF_Buffer* TF_GetAllOpList() {
  std::vector<tensorflow::OpDef> op_defs;
  tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs);
  tensorflow::OpList op_list;
  for (const auto& op : op_defs) {
    *(op_list.add_op()) = op;
  }
  TF_Buffer* ret = TF_NewBuffer();
  TF_CHECK_OK(MessageToBuffer(op_list, ret));
  return ret;
}

// --------------------------------------------------------------------------
// ListDevices & SessionListDevices API

void TF_DeleteDeviceList(TF_DeviceList* list) { delete list; }

TF_DeviceList* TF_SessionListDevices(TF_Session* session, TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}

TF_DeviceList* TF_DeprecatedSessionListDevices(TF_DeprecatedSession* session,
                                               TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}

int TF_DeviceListCount(const TF_DeviceList* list) {
  return list->response.size();
}

#define TF_DEVICELIST_METHOD(return_type, method_name, accessor, err_val) \
  return_type method_name(const TF_DeviceList* list, const int index,     \
                          TF_Status* status) {                            \
    if (list == nullptr) {                                                \
      status->status = InvalidArgument("list is null!");                  \
      return err_val;                                                     \
    }                                                                     \
    if (index < 0 || index >= list->response.size()) {                    \
      status->status = InvalidArgument("index out of bounds");            \
      return err_val;                                                     \
    }                                                                     \
    status->status = ::tensorflow::OkStatus();                            \
    return list->response[index].accessor;                                \
  }

TF_DEVICELIST_METHOD(const char*, TF_DeviceListName, name().c_str(), nullptr);
TF_DEVICELIST_METHOD(const char*, TF_DeviceListType, device_type().c_str(),
                     nullptr);
TF_DEVICELIST_METHOD(int64_t, TF_DeviceListMemoryBytes, memory_limit(), -1);
TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output) {
  return TensorId(output.oper->node.name(), output.index);
}

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
std::vector<tensorflow::Output> OutputsFromTFOutputs(TF_Output* tf_outputs,
                                                     int n) {
  std::vector<tensorflow::Output> outputs(n);
  for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
  return outputs;
}

void TFOutputsFromOutputs(const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs) {
  for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
}
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

}
  extern: extern
  string_literal: "C"
   ": "
   string_content: C
   ": "
  declaration_list: {

void TF_Run(TF_DeprecatedSession* s, const TF_Buffer* run_options,
            // Input tensors
            const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
            // Output tensors
            const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
            // Target nodes
            const char** c_target_oper_names, int ntargets,
            TF_Buffer* run_metadata, TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status);
}

void TF_PRunSetup(TF_DeprecatedSession* s,
                  // Input names
                  const char** c_input_names, int ninputs,
                  // Output names
                  const char** c_output_names, int noutputs,
                  // Target nodes
                  const char** c_target_oper_names, int ntargets,
                  const char** handle, TF_Status* status) {
  *handle = nullptr;

  std::vector<string> input_names(ninputs);
  std::vector<string> output_names(noutputs);
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = c_input_names[i];
  }
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  string new_handle;
  status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}

void TF_PRun(TF_DeprecatedSession* s, const char* handle,
             // Input tensors
             const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
             // Output tensors
             const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
             // Target nodes
             const char** c_target_oper_names, int ntargets,
             TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status);
}

TF_Library* TF_LoadLibrary(const char* library_filename, TF_Status* status) {
  TF_Library* lib_handle = new TF_Library;
  status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length);
  if (!status->status.ok()) {
    delete lib_handle;
    return nullptr;
  }
  return lib_handle;
}

TF_Buffer TF_GetOpList(TF_Library* lib_handle) { return lib_handle->op_list; }

void TF_DeleteLibraryHandle(TF_Library* lib_handle) {
  if (lib_handle == nullptr) return;
  tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data));
  delete lib_handle;
}

TF_Buffer* TF_GetAllOpList() {
  std::vector<tensorflow::OpDef> op_defs;
  tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs);
  tensorflow::OpList op_list;
  for (const auto& op : op_defs) {
    *(op_list.add_op()) = op;
  }
  TF_Buffer* ret = TF_NewBuffer();
  TF_CHECK_OK(MessageToBuffer(op_list, ret));
  return ret;
}

// --------------------------------------------------------------------------
// ListDevices & SessionListDevices API

void TF_DeleteDeviceList(TF_DeviceList* list) { delete list; }

TF_DeviceList* TF_SessionListDevices(TF_Session* session, TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}

TF_DeviceList* TF_DeprecatedSessionListDevices(TF_DeprecatedSession* session,
                                               TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}

int TF_DeviceListCount(const TF_DeviceList* list) {
  return list->response.size();
}

#define TF_DEVICELIST_METHOD(return_type, method_name, accessor, err_val) \
  return_type method_name(const TF_DeviceList* list, const int index,     \
                          TF_Status* status) {                            \
    if (list == nullptr) {                                                \
      status->status = InvalidArgument("list is null!");                  \
      return err_val;                                                     \
    }                                                                     \
    if (index < 0 || index >= list->response.size()) {                    \
      status->status = InvalidArgument("index out of bounds");            \
      return err_val;                                                     \
    }                                                                     \
    status->status = ::tensorflow::OkStatus();                            \
    return list->response[index].accessor;                                \
  }

TF_DEVICELIST_METHOD(const char*, TF_DeviceListName, name().c_str(), nullptr);
TF_DEVICELIST_METHOD(const char*, TF_DeviceListType, device_type().c_str(),
                     nullptr);
TF_DEVICELIST_METHOD(int64_t, TF_DeviceListMemoryBytes, memory_limit(), -1);
TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output) {
  return TensorId(output.oper->node.name(), output.index);
}

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
std::vector<tensorflow::Output> OutputsFromTFOutputs(TF_Output* tf_outputs,
                                                     int n) {
  std::vector<tensorflow::Output> outputs(n);
  for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
  return outputs;
}

void TFOutputsFromOutputs(const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs) {
  for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
}
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

}
   {: {
   function_definition: void TF_Run(TF_DeprecatedSession* s, const TF_Buffer* run_options,
            // Input tensors
            const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
            // Output tensors
            const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
            // Target nodes
            const char** c_target_oper_names, int ntargets,
            TF_Buffer* run_metadata, TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status);
}
    primitive_type: void
    function_declarator: TF_Run(TF_DeprecatedSession* s, const TF_Buffer* run_options,
            // Input tensors
            const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
            // Output tensors
            const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
            // Target nodes
            const char** c_target_oper_names, int ntargets,
            TF_Buffer* run_metadata, TF_Status* status)
     identifier: TF_Run
     parameter_list: (TF_DeprecatedSession* s, const TF_Buffer* run_options,
            // Input tensors
            const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
            // Output tensors
            const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
            // Target nodes
            const char** c_target_oper_names, int ntargets,
            TF_Buffer* run_metadata, TF_Status* status)
      (: (
      parameter_declaration: TF_DeprecatedSession* s
       type_identifier: TF_DeprecatedSession
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: const TF_Buffer* run_options
       type_qualifier: const
        const: const
       type_identifier: TF_Buffer
       pointer_declarator: * run_options
        *: *
        identifier: run_options
      ,: ,
      comment: // Input tensors
      parameter_declaration: const char** c_input_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_input_names
        *: *
        pointer_declarator: * c_input_names
         *: *
         identifier: c_input_names
      ,: ,
      parameter_declaration: TF_Tensor** c_inputs
       type_identifier: TF_Tensor
       pointer_declarator: ** c_inputs
        *: *
        pointer_declarator: * c_inputs
         *: *
         identifier: c_inputs
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      comment: // Output tensors
      parameter_declaration: const char** c_output_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_output_names
        *: *
        pointer_declarator: * c_output_names
         *: *
         identifier: c_output_names
      ,: ,
      parameter_declaration: TF_Tensor** c_outputs
       type_identifier: TF_Tensor
       pointer_declarator: ** c_outputs
        *: *
        pointer_declarator: * c_outputs
         *: *
         identifier: c_outputs
      ,: ,
      parameter_declaration: int noutputs
       primitive_type: int
       identifier: noutputs
      ,: ,
      comment: // Target nodes
      parameter_declaration: const char** c_target_oper_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_target_oper_names
        *: *
        pointer_declarator: * c_target_oper_names
         *: *
         identifier: c_target_oper_names
      ,: ,
      parameter_declaration: int ntargets
       primitive_type: int
       identifier: ntargets
      ,: ,
      parameter_declaration: TF_Buffer* run_metadata
       type_identifier: TF_Buffer
       pointer_declarator: * run_metadata
        *: *
        identifier: run_metadata
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status);
}
     {: {
     expression_statement: TF_Run_Setup(noutputs, c_outputs, status);
      call_expression: TF_Run_Setup(noutputs, c_outputs, status)
       identifier: TF_Run_Setup
       argument_list: (noutputs, c_outputs, status)
        (: (
        identifier: noutputs
        ,: ,
        identifier: c_outputs
        ,: ,
        identifier: status
        ): )
      ;: ;
     declaration: std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
      qualified_identifier: std::vector<std::pair<string, Tensor>>
       namespace_identifier: std
       ::: ::
       template_type: vector<std::pair<string, Tensor>>
        type_identifier: vector
        template_argument_list: <std::pair<string, Tensor>>
         <: <
         type_descriptor: std::pair<string, Tensor>
          qualified_identifier: std::pair<string, Tensor>
           namespace_identifier: std
           ::: ::
           template_type: pair<string, Tensor>
            type_identifier: pair
            template_argument_list: <string, Tensor>
             <: <
             type_descriptor: string
              type_identifier: string
             ,: ,
             type_descriptor: Tensor
              type_identifier: Tensor
             >: >
         >: >
      function_declarator: input_pairs(ninputs)
       identifier: input_pairs
       parameter_list: (ninputs)
        (: (
        parameter_declaration: ninputs
         type_identifier: ninputs
        ): )
      ;: ;
     if_statement: if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
      if: if
      condition_clause: (!TF_Run_Inputs(c_inputs, &input_pairs, status))
       (: (
       unary_expression: !TF_Run_Inputs(c_inputs, &input_pairs, status)
        !: !
        call_expression: TF_Run_Inputs(c_inputs, &input_pairs, status)
         identifier: TF_Run_Inputs
         argument_list: (c_inputs, &input_pairs, status)
          (: (
          identifier: c_inputs
          ,: ,
          pointer_expression: &input_pairs
           &: &
           identifier: input_pairs
          ,: ,
          identifier: status
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     for_statement: for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ninputs
       identifier: i
       <: <
       identifier: ninputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_pairs[i].first = c_input_names[i];
  }
       {: {
       expression_statement: input_pairs[i].first = c_input_names[i];
        assignment_expression: input_pairs[i].first = c_input_names[i]
         field_expression: input_pairs[i].first
          subscript_expression: input_pairs[i]
           identifier: input_pairs
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          .: .
          field_identifier: first
         =: =
         subscript_expression: c_input_names[i]
          identifier: c_input_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     declaration: std::vector<string> output_names(noutputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: output_names(noutputs)
       identifier: output_names
       parameter_list: (noutputs)
        (: (
        parameter_declaration: noutputs
         type_identifier: noutputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < noutputs
       identifier: i
       <: <
       identifier: noutputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    output_names[i] = c_output_names[i];
  }
       {: {
       expression_statement: output_names[i] = c_output_names[i];
        assignment_expression: output_names[i] = c_output_names[i]
         subscript_expression: output_names[i]
          identifier: output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_output_names[i]
          identifier: c_output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     declaration: std::vector<string> target_oper_names(ntargets);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: target_oper_names(ntargets)
       identifier: target_oper_names
       parameter_list: (ntargets)
        (: (
        parameter_declaration: ntargets
         type_identifier: ntargets
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ntargets
       identifier: i
       <: <
       identifier: ntargets
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    target_oper_names[i] = c_target_oper_names[i];
  }
       {: {
       expression_statement: target_oper_names[i] = c_target_oper_names[i];
        assignment_expression: target_oper_names[i] = c_target_oper_names[i]
         subscript_expression: target_oper_names[i]
          identifier: target_oper_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_target_oper_names[i]
          identifier: c_target_oper_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     expression_statement: TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status);
      call_expression: TF_Run_Helper(s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status)
       identifier: TF_Run_Helper
       argument_list: (s->session, nullptr, run_options, input_pairs, output_names,
                c_outputs, target_oper_names, run_metadata, status)
        (: (
        field_expression: s->session
         identifier: s
         ->: ->
         field_identifier: session
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: run_options
        ,: ,
        identifier: input_pairs
        ,: ,
        identifier: output_names
        ,: ,
        identifier: c_outputs
        ,: ,
        identifier: target_oper_names
        ,: ,
        identifier: run_metadata
        ,: ,
        identifier: status
        ): )
      ;: ;
     }: }
   function_definition: void TF_PRunSetup(TF_DeprecatedSession* s,
                  // Input names
                  const char** c_input_names, int ninputs,
                  // Output names
                  const char** c_output_names, int noutputs,
                  // Target nodes
                  const char** c_target_oper_names, int ntargets,
                  const char** handle, TF_Status* status) {
  *handle = nullptr;

  std::vector<string> input_names(ninputs);
  std::vector<string> output_names(noutputs);
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = c_input_names[i];
  }
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  string new_handle;
  status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}
    primitive_type: void
    function_declarator: TF_PRunSetup(TF_DeprecatedSession* s,
                  // Input names
                  const char** c_input_names, int ninputs,
                  // Output names
                  const char** c_output_names, int noutputs,
                  // Target nodes
                  const char** c_target_oper_names, int ntargets,
                  const char** handle, TF_Status* status)
     identifier: TF_PRunSetup
     parameter_list: (TF_DeprecatedSession* s,
                  // Input names
                  const char** c_input_names, int ninputs,
                  // Output names
                  const char** c_output_names, int noutputs,
                  // Target nodes
                  const char** c_target_oper_names, int ntargets,
                  const char** handle, TF_Status* status)
      (: (
      parameter_declaration: TF_DeprecatedSession* s
       type_identifier: TF_DeprecatedSession
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      comment: // Input names
      parameter_declaration: const char** c_input_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_input_names
        *: *
        pointer_declarator: * c_input_names
         *: *
         identifier: c_input_names
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      comment: // Output names
      parameter_declaration: const char** c_output_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_output_names
        *: *
        pointer_declarator: * c_output_names
         *: *
         identifier: c_output_names
      ,: ,
      parameter_declaration: int noutputs
       primitive_type: int
       identifier: noutputs
      ,: ,
      comment: // Target nodes
      parameter_declaration: const char** c_target_oper_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_target_oper_names
        *: *
        pointer_declarator: * c_target_oper_names
         *: *
         identifier: c_target_oper_names
      ,: ,
      parameter_declaration: int ntargets
       primitive_type: int
       identifier: ntargets
      ,: ,
      parameter_declaration: const char** handle
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** handle
        *: *
        pointer_declarator: * handle
         *: *
         identifier: handle
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  *handle = nullptr;

  std::vector<string> input_names(ninputs);
  std::vector<string> output_names(noutputs);
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = c_input_names[i];
  }
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  string new_handle;
  status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}
     {: {
     expression_statement: *handle = nullptr;
      assignment_expression: *handle = nullptr
       pointer_expression: *handle
        *: *
        identifier: handle
       =: =
       null: nullptr
        nullptr: nullptr
      ;: ;
     declaration: std::vector<string> input_names(ninputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: input_names(ninputs)
       identifier: input_names
       parameter_list: (ninputs)
        (: (
        parameter_declaration: ninputs
         type_identifier: ninputs
        ): )
      ;: ;
     declaration: std::vector<string> output_names(noutputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: output_names(noutputs)
       identifier: output_names
       parameter_list: (noutputs)
        (: (
        parameter_declaration: noutputs
         type_identifier: noutputs
        ): )
      ;: ;
     declaration: std::vector<string> target_oper_names(ntargets);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: target_oper_names(ntargets)
       identifier: target_oper_names
       parameter_list: (ntargets)
        (: (
        parameter_declaration: ntargets
         type_identifier: ntargets
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ninputs; ++i) {
    input_names[i] = c_input_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ninputs
       identifier: i
       <: <
       identifier: ninputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_names[i] = c_input_names[i];
  }
       {: {
       expression_statement: input_names[i] = c_input_names[i];
        assignment_expression: input_names[i] = c_input_names[i]
         subscript_expression: input_names[i]
          identifier: input_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_input_names[i]
          identifier: c_input_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     for_statement: for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < noutputs
       identifier: i
       <: <
       identifier: noutputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    output_names[i] = c_output_names[i];
  }
       {: {
       expression_statement: output_names[i] = c_output_names[i];
        assignment_expression: output_names[i] = c_output_names[i]
         subscript_expression: output_names[i]
          identifier: output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_output_names[i]
          identifier: c_output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     for_statement: for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ntargets
       identifier: i
       <: <
       identifier: ntargets
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    target_oper_names[i] = c_target_oper_names[i];
  }
       {: {
       expression_statement: target_oper_names[i] = c_target_oper_names[i];
        assignment_expression: target_oper_names[i] = c_target_oper_names[i]
         subscript_expression: target_oper_names[i]
          identifier: target_oper_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_target_oper_names[i]
          identifier: c_target_oper_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     declaration: string new_handle;
      type_identifier: string
      identifier: new_handle
      ;: ;
     expression_statement: status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle);
      assignment_expression: status->status = s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: s->session->PRunSetup(input_names, output_names,
                                         target_oper_names, &new_handle)
        field_expression: s->session->PRunSetup
         field_expression: s->session
          identifier: s
          ->: ->
          field_identifier: session
         ->: ->
         field_identifier: PRunSetup
        argument_list: (input_names, output_names,
                                         target_oper_names, &new_handle)
         (: (
         identifier: input_names
         ,: ,
         identifier: output_names
         ,: ,
         identifier: target_oper_names
         ,: ,
         pointer_expression: &new_handle
          &: &
          identifier: new_handle
         ): )
      ;: ;
     if_statement: if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
      if: if
      condition_clause: (status->status.ok())
       (: (
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
       ): )
      compound_statement: {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
       {: {
       declaration: char* buf = new char[new_handle.size() + 1];
        primitive_type: char
        init_declarator: * buf = new char[new_handle.size() + 1]
         pointer_declarator: * buf
          *: *
          identifier: buf
         =: =
         new_expression: new char[new_handle.size() + 1]
          new: new
          primitive_type: char
          new_declarator: [new_handle.size() + 1]
           [: [
           binary_expression: new_handle.size() + 1
            call_expression: new_handle.size()
             field_expression: new_handle.size
              identifier: new_handle
              .: .
              field_identifier: size
             argument_list: ()
              (: (
              ): )
            +: +
            number_literal: 1
           ]: ]
        ;: ;
       expression_statement: memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
        call_expression: memcpy(buf, new_handle.c_str(), new_handle.size() + 1)
         identifier: memcpy
         argument_list: (buf, new_handle.c_str(), new_handle.size() + 1)
          (: (
          identifier: buf
          ,: ,
          call_expression: new_handle.c_str()
           field_expression: new_handle.c_str
            identifier: new_handle
            .: .
            field_identifier: c_str
           argument_list: ()
            (: (
            ): )
          ,: ,
          binary_expression: new_handle.size() + 1
           call_expression: new_handle.size()
            field_expression: new_handle.size
             identifier: new_handle
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
           +: +
           number_literal: 1
          ): )
        ;: ;
       expression_statement: *handle = buf;
        assignment_expression: *handle = buf
         pointer_expression: *handle
          *: *
          identifier: handle
         =: =
         identifier: buf
        ;: ;
       }: }
     }: }
   function_definition: void TF_PRun(TF_DeprecatedSession* s, const char* handle,
             // Input tensors
             const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
             // Output tensors
             const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
             // Target nodes
             const char** c_target_oper_names, int ntargets,
             TF_Status* status) {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status);
}
    primitive_type: void
    function_declarator: TF_PRun(TF_DeprecatedSession* s, const char* handle,
             // Input tensors
             const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
             // Output tensors
             const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
             // Target nodes
             const char** c_target_oper_names, int ntargets,
             TF_Status* status)
     identifier: TF_PRun
     parameter_list: (TF_DeprecatedSession* s, const char* handle,
             // Input tensors
             const char** c_input_names, TF_Tensor** c_inputs, int ninputs,
             // Output tensors
             const char** c_output_names, TF_Tensor** c_outputs, int noutputs,
             // Target nodes
             const char** c_target_oper_names, int ntargets,
             TF_Status* status)
      (: (
      parameter_declaration: TF_DeprecatedSession* s
       type_identifier: TF_DeprecatedSession
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: const char* handle
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * handle
        *: *
        identifier: handle
      ,: ,
      comment: // Input tensors
      parameter_declaration: const char** c_input_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_input_names
        *: *
        pointer_declarator: * c_input_names
         *: *
         identifier: c_input_names
      ,: ,
      parameter_declaration: TF_Tensor** c_inputs
       type_identifier: TF_Tensor
       pointer_declarator: ** c_inputs
        *: *
        pointer_declarator: * c_inputs
         *: *
         identifier: c_inputs
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      comment: // Output tensors
      parameter_declaration: const char** c_output_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_output_names
        *: *
        pointer_declarator: * c_output_names
         *: *
         identifier: c_output_names
      ,: ,
      parameter_declaration: TF_Tensor** c_outputs
       type_identifier: TF_Tensor
       pointer_declarator: ** c_outputs
        *: *
        pointer_declarator: * c_outputs
         *: *
         identifier: c_outputs
      ,: ,
      parameter_declaration: int noutputs
       primitive_type: int
       identifier: noutputs
      ,: ,
      comment: // Target nodes
      parameter_declaration: const char** c_target_oper_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** c_target_oper_names
        *: *
        pointer_declarator: * c_target_oper_names
         *: *
         identifier: c_target_oper_names
      ,: ,
      parameter_declaration: int ntargets
       primitive_type: int
       identifier: ntargets
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  TF_Run_Setup(noutputs, c_outputs, status);
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
  std::vector<string> target_oper_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
  TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status);
}
     {: {
     expression_statement: TF_Run_Setup(noutputs, c_outputs, status);
      call_expression: TF_Run_Setup(noutputs, c_outputs, status)
       identifier: TF_Run_Setup
       argument_list: (noutputs, c_outputs, status)
        (: (
        identifier: noutputs
        ,: ,
        identifier: c_outputs
        ,: ,
        identifier: status
        ): )
      ;: ;
     declaration: std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
      qualified_identifier: std::vector<std::pair<string, Tensor>>
       namespace_identifier: std
       ::: ::
       template_type: vector<std::pair<string, Tensor>>
        type_identifier: vector
        template_argument_list: <std::pair<string, Tensor>>
         <: <
         type_descriptor: std::pair<string, Tensor>
          qualified_identifier: std::pair<string, Tensor>
           namespace_identifier: std
           ::: ::
           template_type: pair<string, Tensor>
            type_identifier: pair
            template_argument_list: <string, Tensor>
             <: <
             type_descriptor: string
              type_identifier: string
             ,: ,
             type_descriptor: Tensor
              type_identifier: Tensor
             >: >
         >: >
      function_declarator: input_pairs(ninputs)
       identifier: input_pairs
       parameter_list: (ninputs)
        (: (
        parameter_declaration: ninputs
         type_identifier: ninputs
        ): )
      ;: ;
     if_statement: if (!TF_Run_Inputs(c_inputs, &input_pairs, status)) return;
      if: if
      condition_clause: (!TF_Run_Inputs(c_inputs, &input_pairs, status))
       (: (
       unary_expression: !TF_Run_Inputs(c_inputs, &input_pairs, status)
        !: !
        call_expression: TF_Run_Inputs(c_inputs, &input_pairs, status)
         identifier: TF_Run_Inputs
         argument_list: (c_inputs, &input_pairs, status)
          (: (
          identifier: c_inputs
          ,: ,
          pointer_expression: &input_pairs
           &: &
           identifier: input_pairs
          ,: ,
          identifier: status
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     for_statement: for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = c_input_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ninputs
       identifier: i
       <: <
       identifier: ninputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_pairs[i].first = c_input_names[i];
  }
       {: {
       expression_statement: input_pairs[i].first = c_input_names[i];
        assignment_expression: input_pairs[i].first = c_input_names[i]
         field_expression: input_pairs[i].first
          subscript_expression: input_pairs[i]
           identifier: input_pairs
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          .: .
          field_identifier: first
         =: =
         subscript_expression: c_input_names[i]
          identifier: c_input_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     declaration: std::vector<string> output_names(noutputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: output_names(noutputs)
       identifier: output_names
       parameter_list: (noutputs)
        (: (
        parameter_declaration: noutputs
         type_identifier: noutputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < noutputs; ++i) {
    output_names[i] = c_output_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < noutputs
       identifier: i
       <: <
       identifier: noutputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    output_names[i] = c_output_names[i];
  }
       {: {
       expression_statement: output_names[i] = c_output_names[i];
        assignment_expression: output_names[i] = c_output_names[i]
         subscript_expression: output_names[i]
          identifier: output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_output_names[i]
          identifier: c_output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     declaration: std::vector<string> target_oper_names(ntargets);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: target_oper_names(ntargets)
       identifier: target_oper_names
       parameter_list: (ntargets)
        (: (
        parameter_declaration: ntargets
         type_identifier: ntargets
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ntargets; ++i) {
    target_oper_names[i] = c_target_oper_names[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ntargets
       identifier: i
       <: <
       identifier: ntargets
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    target_oper_names[i] = c_target_oper_names[i];
  }
       {: {
       expression_statement: target_oper_names[i] = c_target_oper_names[i];
        assignment_expression: target_oper_names[i] = c_target_oper_names[i]
         subscript_expression: target_oper_names[i]
          identifier: target_oper_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: c_target_oper_names[i]
          identifier: c_target_oper_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     expression_statement: TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status);
      call_expression: TF_Run_Helper(s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status)
       identifier: TF_Run_Helper
       argument_list: (s->session, handle, nullptr, input_pairs, output_names,
                c_outputs, target_oper_names, nullptr, status)
        (: (
        field_expression: s->session
         identifier: s
         ->: ->
         field_identifier: session
        ,: ,
        identifier: handle
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: input_pairs
        ,: ,
        identifier: output_names
        ,: ,
        identifier: c_outputs
        ,: ,
        identifier: target_oper_names
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: status
        ): )
      ;: ;
     }: }
   function_definition: TF_Library* TF_LoadLibrary(const char* library_filename, TF_Status* status) {
  TF_Library* lib_handle = new TF_Library;
  status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length);
  if (!status->status.ok()) {
    delete lib_handle;
    return nullptr;
  }
  return lib_handle;
}
    type_identifier: TF_Library
    pointer_declarator: * TF_LoadLibrary(const char* library_filename, TF_Status* status)
     *: *
     function_declarator: TF_LoadLibrary(const char* library_filename, TF_Status* status)
      identifier: TF_LoadLibrary
      parameter_list: (const char* library_filename, TF_Status* status)
       (: (
       parameter_declaration: const char* library_filename
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * library_filename
         *: *
         identifier: library_filename
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  TF_Library* lib_handle = new TF_Library;
  status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length);
  if (!status->status.ok()) {
    delete lib_handle;
    return nullptr;
  }
  return lib_handle;
}
     {: {
     declaration: TF_Library* lib_handle = new TF_Library;
      type_identifier: TF_Library
      init_declarator: * lib_handle = new TF_Library
       pointer_declarator: * lib_handle
        *: *
        identifier: lib_handle
       =: =
       new_expression: new TF_Library
        new: new
        type_identifier: TF_Library
      ;: ;
     expression_statement: status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length);
      assignment_expression: status->status = tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: tensorflow::LoadDynamicLibrary(
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length)
        qualified_identifier: tensorflow::LoadDynamicLibrary
         namespace_identifier: tensorflow
         ::: ::
         identifier: LoadDynamicLibrary
        argument_list: (
      library_filename, &lib_handle->lib_handle, &lib_handle->op_list.data,
      &lib_handle->op_list.length)
         (: (
         identifier: library_filename
         ,: ,
         pointer_expression: &lib_handle->lib_handle
          &: &
          field_expression: lib_handle->lib_handle
           identifier: lib_handle
           ->: ->
           field_identifier: lib_handle
         ,: ,
         pointer_expression: &lib_handle->op_list.data
          &: &
          field_expression: lib_handle->op_list.data
           field_expression: lib_handle->op_list
            identifier: lib_handle
            ->: ->
            field_identifier: op_list
           .: .
           field_identifier: data
         ,: ,
         pointer_expression: &lib_handle->op_list.length
          &: &
          field_expression: lib_handle->op_list.length
           field_expression: lib_handle->op_list
            identifier: lib_handle
            ->: ->
            field_identifier: op_list
           .: .
           field_identifier: length
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) {
    delete lib_handle;
    return nullptr;
  }
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    delete lib_handle;
    return nullptr;
  }
       {: {
       expression_statement: delete lib_handle;
        delete_expression: delete lib_handle
         delete: delete
         identifier: lib_handle
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     return_statement: return lib_handle;
      return: return
      identifier: lib_handle
      ;: ;
     }: }
   function_definition: TF_Buffer TF_GetOpList(TF_Library* lib_handle) { return lib_handle->op_list; }
    type_identifier: TF_Buffer
    function_declarator: TF_GetOpList(TF_Library* lib_handle)
     identifier: TF_GetOpList
     parameter_list: (TF_Library* lib_handle)
      (: (
      parameter_declaration: TF_Library* lib_handle
       type_identifier: TF_Library
       pointer_declarator: * lib_handle
        *: *
        identifier: lib_handle
      ): )
    compound_statement: { return lib_handle->op_list; }
     {: {
     return_statement: return lib_handle->op_list;
      return: return
      field_expression: lib_handle->op_list
       identifier: lib_handle
       ->: ->
       field_identifier: op_list
      ;: ;
     }: }
   function_definition: void TF_DeleteLibraryHandle(TF_Library* lib_handle) {
  if (lib_handle == nullptr) return;
  tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data));
  delete lib_handle;
}
    primitive_type: void
    function_declarator: TF_DeleteLibraryHandle(TF_Library* lib_handle)
     identifier: TF_DeleteLibraryHandle
     parameter_list: (TF_Library* lib_handle)
      (: (
      parameter_declaration: TF_Library* lib_handle
       type_identifier: TF_Library
       pointer_declarator: * lib_handle
        *: *
        identifier: lib_handle
      ): )
    compound_statement: {
  if (lib_handle == nullptr) return;
  tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data));
  delete lib_handle;
}
     {: {
     if_statement: if (lib_handle == nullptr) return;
      if: if
      condition_clause: (lib_handle == nullptr)
       (: (
       binary_expression: lib_handle == nullptr
        identifier: lib_handle
        ==: ==
        null: nullptr
         nullptr: nullptr
       ): )
      return_statement: return;
       return: return
       ;: ;
     expression_statement: tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data));
      call_expression: tensorflow::port::Free(const_cast<void*>(lib_handle->op_list.data))
       qualified_identifier: tensorflow::port::Free
        namespace_identifier: tensorflow
        ::: ::
        qualified_identifier: port::Free
         namespace_identifier: port
         ::: ::
         identifier: Free
       argument_list: (const_cast<void*>(lib_handle->op_list.data))
        (: (
        call_expression: const_cast<void*>(lib_handle->op_list.data)
         template_function: const_cast<void*>
          identifier: const_cast
          template_argument_list: <void*>
           <: <
           type_descriptor: void*
            primitive_type: void
            abstract_pointer_declarator: *
             *: *
           >: >
         argument_list: (lib_handle->op_list.data)
          (: (
          field_expression: lib_handle->op_list.data
           field_expression: lib_handle->op_list
            identifier: lib_handle
            ->: ->
            field_identifier: op_list
           .: .
           field_identifier: data
          ): )
        ): )
      ;: ;
     expression_statement: delete lib_handle;
      delete_expression: delete lib_handle
       delete: delete
       identifier: lib_handle
      ;: ;
     }: }
   function_definition: TF_Buffer* TF_GetAllOpList() {
  std::vector<tensorflow::OpDef> op_defs;
  tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs);
  tensorflow::OpList op_list;
  for (const auto& op : op_defs) {
    *(op_list.add_op()) = op;
  }
  TF_Buffer* ret = TF_NewBuffer();
  TF_CHECK_OK(MessageToBuffer(op_list, ret));
  return ret;
}
    type_identifier: TF_Buffer
    pointer_declarator: * TF_GetAllOpList()
     *: *
     function_declarator: TF_GetAllOpList()
      identifier: TF_GetAllOpList
      parameter_list: ()
       (: (
       ): )
    compound_statement: {
  std::vector<tensorflow::OpDef> op_defs;
  tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs);
  tensorflow::OpList op_list;
  for (const auto& op : op_defs) {
    *(op_list.add_op()) = op;
  }
  TF_Buffer* ret = TF_NewBuffer();
  TF_CHECK_OK(MessageToBuffer(op_list, ret));
  return ret;
}
     {: {
     declaration: std::vector<tensorflow::OpDef> op_defs;
      qualified_identifier: std::vector<tensorflow::OpDef>
       namespace_identifier: std
       ::: ::
       template_type: vector<tensorflow::OpDef>
        type_identifier: vector
        template_argument_list: <tensorflow::OpDef>
         <: <
         type_descriptor: tensorflow::OpDef
          qualified_identifier: tensorflow::OpDef
           namespace_identifier: tensorflow
           ::: ::
           type_identifier: OpDef
         >: >
      identifier: op_defs
      ;: ;
     expression_statement: tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs);
      call_expression: tensorflow::OpRegistry::Global()->GetRegisteredOps(&op_defs)
       field_expression: tensorflow::OpRegistry::Global()->GetRegisteredOps
        call_expression: tensorflow::OpRegistry::Global()
         qualified_identifier: tensorflow::OpRegistry::Global
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: OpRegistry::Global
           namespace_identifier: OpRegistry
           ::: ::
           identifier: Global
         argument_list: ()
          (: (
          ): )
        ->: ->
        field_identifier: GetRegisteredOps
       argument_list: (&op_defs)
        (: (
        pointer_expression: &op_defs
         &: &
         identifier: op_defs
        ): )
      ;: ;
     declaration: tensorflow::OpList op_list;
      qualified_identifier: tensorflow::OpList
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: OpList
      identifier: op_list
      ;: ;
     for_range_loop: for (const auto& op : op_defs) {
    *(op_list.add_op()) = op;
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      reference_declarator: & op
       &: &
       identifier: op
      :: :
      identifier: op_defs
      ): )
      compound_statement: {
    *(op_list.add_op()) = op;
  }
       {: {
       expression_statement: *(op_list.add_op()) = op;
        assignment_expression: *(op_list.add_op()) = op
         pointer_expression: *(op_list.add_op())
          *: *
          parenthesized_expression: (op_list.add_op())
           (: (
           call_expression: op_list.add_op()
            field_expression: op_list.add_op
             identifier: op_list
             .: .
             field_identifier: add_op
            argument_list: ()
             (: (
             ): )
           ): )
         =: =
         identifier: op
        ;: ;
       }: }
     declaration: TF_Buffer* ret = TF_NewBuffer();
      type_identifier: TF_Buffer
      init_declarator: * ret = TF_NewBuffer()
       pointer_declarator: * ret
        *: *
        identifier: ret
       =: =
       call_expression: TF_NewBuffer()
        identifier: TF_NewBuffer
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: TF_CHECK_OK(MessageToBuffer(op_list, ret));
      call_expression: TF_CHECK_OK(MessageToBuffer(op_list, ret))
       identifier: TF_CHECK_OK
       argument_list: (MessageToBuffer(op_list, ret))
        (: (
        call_expression: MessageToBuffer(op_list, ret)
         identifier: MessageToBuffer
         argument_list: (op_list, ret)
          (: (
          identifier: op_list
          ,: ,
          identifier: ret
          ): )
        ): )
      ;: ;
     return_statement: return ret;
      return: return
      identifier: ret
      ;: ;
     }: }
   comment: // --------------------------------------------------------------------------
   comment: // ListDevices & SessionListDevices API
   function_definition: void TF_DeleteDeviceList(TF_DeviceList* list) { delete list; }
    primitive_type: void
    function_declarator: TF_DeleteDeviceList(TF_DeviceList* list)
     identifier: TF_DeleteDeviceList
     parameter_list: (TF_DeviceList* list)
      (: (
      parameter_declaration: TF_DeviceList* list
       type_identifier: TF_DeviceList
       pointer_declarator: * list
        *: *
        identifier: list
      ): )
    compound_statement: { delete list; }
     {: {
     expression_statement: delete list;
      delete_expression: delete list
       delete: delete
       identifier: list
      ;: ;
     }: }
   function_definition: TF_DeviceList* TF_SessionListDevices(TF_Session* session, TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}
    type_identifier: TF_DeviceList
    pointer_declarator: * TF_SessionListDevices(TF_Session* session, TF_Status* status)
     *: *
     function_declarator: TF_SessionListDevices(TF_Session* session, TF_Status* status)
      identifier: TF_SessionListDevices
      parameter_list: (TF_Session* session, TF_Status* status)
       (: (
       parameter_declaration: TF_Session* session
        type_identifier: TF_Session
        pointer_declarator: * session
         *: *
         identifier: session
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}
     {: {
     declaration: TF_DeviceList* response = new TF_DeviceList;
      type_identifier: TF_DeviceList
      init_declarator: * response = new TF_DeviceList
       pointer_declarator: * response
        *: *
        identifier: response
       =: =
       new_expression: new TF_DeviceList
        new: new
        type_identifier: TF_DeviceList
      ;: ;
     if_statement: if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
      if: if
      condition_clause: (session && session->session)
       (: (
       binary_expression: session && session->session
        identifier: session
        &&: &&
        field_expression: session->session
         identifier: session
         ->: ->
         field_identifier: session
       ): )
      expression_statement: status->status = session->session->ListDevices(&response->response);
       assignment_expression: status->status = session->session->ListDevices(&response->response)
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: session->session->ListDevices(&response->response)
         field_expression: session->session->ListDevices
          field_expression: session->session
           identifier: session
           ->: ->
           field_identifier: session
          ->: ->
          field_identifier: ListDevices
         argument_list: (&response->response)
          (: (
          pointer_expression: &response->response
           &: &
           field_expression: response->response
            identifier: response
            ->: ->
            field_identifier: response
          ): )
       ;: ;
     return_statement: return response;
      return: return
      identifier: response
      ;: ;
     }: }
   function_definition: TF_DeviceList* TF_DeprecatedSessionListDevices(TF_DeprecatedSession* session,
                                               TF_Status* status) {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}
    type_identifier: TF_DeviceList
    pointer_declarator: * TF_DeprecatedSessionListDevices(TF_DeprecatedSession* session,
                                               TF_Status* status)
     *: *
     function_declarator: TF_DeprecatedSessionListDevices(TF_DeprecatedSession* session,
                                               TF_Status* status)
      identifier: TF_DeprecatedSessionListDevices
      parameter_list: (TF_DeprecatedSession* session,
                                               TF_Status* status)
       (: (
       parameter_declaration: TF_DeprecatedSession* session
        type_identifier: TF_DeprecatedSession
        pointer_declarator: * session
         *: *
         identifier: session
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  TF_DeviceList* response = new TF_DeviceList;
  if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
  return response;
}
     {: {
     declaration: TF_DeviceList* response = new TF_DeviceList;
      type_identifier: TF_DeviceList
      init_declarator: * response = new TF_DeviceList
       pointer_declarator: * response
        *: *
        identifier: response
       =: =
       new_expression: new TF_DeviceList
        new: new
        type_identifier: TF_DeviceList
      ;: ;
     if_statement: if (session && session->session)
    status->status = session->session->ListDevices(&response->response);
      if: if
      condition_clause: (session && session->session)
       (: (
       binary_expression: session && session->session
        identifier: session
        &&: &&
        field_expression: session->session
         identifier: session
         ->: ->
         field_identifier: session
       ): )
      expression_statement: status->status = session->session->ListDevices(&response->response);
       assignment_expression: status->status = session->session->ListDevices(&response->response)
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: session->session->ListDevices(&response->response)
         field_expression: session->session->ListDevices
          field_expression: session->session
           identifier: session
           ->: ->
           field_identifier: session
          ->: ->
          field_identifier: ListDevices
         argument_list: (&response->response)
          (: (
          pointer_expression: &response->response
           &: &
           field_expression: response->response
            identifier: response
            ->: ->
            field_identifier: response
          ): )
       ;: ;
     return_statement: return response;
      return: return
      identifier: response
      ;: ;
     }: }
   function_definition: int TF_DeviceListCount(const TF_DeviceList* list) {
  return list->response.size();
}
    primitive_type: int
    function_declarator: TF_DeviceListCount(const TF_DeviceList* list)
     identifier: TF_DeviceListCount
     parameter_list: (const TF_DeviceList* list)
      (: (
      parameter_declaration: const TF_DeviceList* list
       type_qualifier: const
        const: const
       type_identifier: TF_DeviceList
       pointer_declarator: * list
        *: *
        identifier: list
      ): )
    compound_statement: {
  return list->response.size();
}
     {: {
     return_statement: return list->response.size();
      return: return
      call_expression: list->response.size()
       field_expression: list->response.size
        field_expression: list->response
         identifier: list
         ->: ->
         field_identifier: response
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   preproc_function_def: #define TF_DEVICELIST_METHOD(return_type, method_name, accessor, err_val) \
  return_type method_name(const TF_DeviceList* list, const int index,     \
                          TF_Status* status) {                            \
    if (list == nullptr) {                                                \
      status->status = InvalidArgument("list is null!");                  \
      return err_val;                                                     \
    }                                                                     \
    if (index < 0 || index >= list->response.size()) {                    \
      status->status = InvalidArgument("index out of bounds");            \
      return err_val;                                                     \
    }                                                                     \
    status->status = ::tensorflow::OkStatus();                            \
    return list->response[index].accessor;                                \
  }

    #define: #define
    identifier: TF_DEVICELIST_METHOD
    preproc_params: (return_type, method_name, accessor, err_val)
     (: (
     identifier: return_type
     ,: ,
     identifier: method_name
     ,: ,
     identifier: accessor
     ,: ,
     identifier: err_val
     ): )
    preproc_arg: return_type method_name(const TF_DeviceList* list, const int index,     \
                          TF_Status* status) {                            \
    if (list == nullptr) {                                                \
      status->status = InvalidArgument("list is null!");                  \
      return err_val;                                                     \
    }                                                                     \
    if (index < 0 || index >= list->response.size()) {                    \
      status->status = InvalidArgument("index out of bounds");            \
      return err_val;                                                     \
    }                                                                     \
    status->status = ::tensorflow::OkStatus();                            \
    return list->response[index].accessor;                                \
  }
   ERROR: TF_DEVICELIST_METHOD(const char*, TF_DeviceListName, name().c_str(), nullptr)
    function_declarator: TF_DEVICELIST_METHOD(const char*, TF_DeviceListName, name().c_str(), nullptr)
     identifier: TF_DEVICELIST_METHOD
     parameter_list: (const char*, TF_DeviceListName, name().c_str(), nullptr)
      (: (
      parameter_declaration: const char*
       type_qualifier: const
        const: const
       primitive_type: char
       abstract_pointer_declarator: *
        *: *
      ,: ,
      parameter_declaration: TF_DeviceListName
       type_identifier: TF_DeviceListName
      ,: ,
      parameter_declaration: name().c_str()
       type_identifier: name
       ERROR: ().
        parameter_list: ()
         (: (
         ): )
        .: .
       function_declarator: c_str()
        identifier: c_str
        parameter_list: ()
         (: (
         ): )
      ,: ,
      parameter_declaration: nullptr
       type_identifier: nullptr
      ): )
   expression_statement: ;
    ;: ;
   ERROR: TF_DEVICELIST_METHOD(const char*, TF_DeviceListType, device_type().c_str(),
                     nullptr)
    function_declarator: TF_DEVICELIST_METHOD(const char*, TF_DeviceListType, device_type().c_str(),
                     nullptr)
     identifier: TF_DEVICELIST_METHOD
     parameter_list: (const char*, TF_DeviceListType, device_type().c_str(),
                     nullptr)
      (: (
      parameter_declaration: const char*
       type_qualifier: const
        const: const
       primitive_type: char
       abstract_pointer_declarator: *
        *: *
      ,: ,
      parameter_declaration: TF_DeviceListType
       type_identifier: TF_DeviceListType
      ,: ,
      parameter_declaration: device_type().c_str()
       type_identifier: device_type
       ERROR: ().
        parameter_list: ()
         (: (
         ): )
        .: .
       function_declarator: c_str()
        identifier: c_str
        parameter_list: ()
         (: (
         ): )
      ,: ,
      parameter_declaration: nullptr
       type_identifier: nullptr
      ): )
   expression_statement: ;
    ;: ;
   function_definition: TF_DEVICELIST_METHOD(int64_t, TF_DeviceListMemoryBytes, memory_limit(), -1);
TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output) {
  return TensorId(output.oper->node.name(), output.index);
}
    function_declarator: TF_DEVICELIST_METHOD(int64_t, TF_DeviceListMemoryBytes, memory_limit(), -1);
TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output)
     identifier: TF_DEVICELIST_METHOD
     parameter_list: (int64_t, TF_DeviceListMemoryBytes, memory_limit(), -1);
TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output)
      (: (
      parameter_declaration: int64_t
       primitive_type: int64_t
      ,: ,
      parameter_declaration: TF_DeviceListMemoryBytes
       type_identifier: TF_DeviceListMemoryBytes
      ,: ,
      parameter_declaration: memory_limit()
       type_identifier: memory_limit
       abstract_function_declarator: ()
        parameter_list: ()
         (: (
         ): )
      ,: ,
      ERROR: -1);
       number_literal: -1
       ): )
       ;: ;
      parameter_declaration: TF_DEVICELIST_METHOD(uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output)
       type_identifier: TF_DEVICELIST_METHOD
       abstract_function_declarator: (uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output)
        parameter_list: (uint64_t, TF_DeviceListIncarnation, incarnation(), 0);

#undef TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output)
         (: (
         parameter_declaration: uint64_t
          primitive_type: uint64_t
         ,: ,
         parameter_declaration: TF_DeviceListIncarnation
          type_identifier: TF_DeviceListIncarnation
         ,: ,
         parameter_declaration: incarnation()
          type_identifier: incarnation
          abstract_function_declarator: ()
           parameter_list: ()
            (: (
            ): )
         ,: ,
         ERROR: 0);

#undef
          number_literal: 0
          ): )
          ;: ;
          preproc_directive: #undef
         optional_parameter_declaration: TF_DEVICELIST_METHOD

}  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation* ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr = oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output
          type_identifier: TF_DEVICELIST_METHOD
          ERROR: }  // end extern "C"

// --------------------------------------------------------------------------
// New Graph and Session API

// Helper functions -----------------------------------------------------------

namespace {

TF_Operation
           }: }
           comment: // end extern "C"
           comment: // --------------------------------------------------------------------------
           comment: // New Graph and Session API
           comment: // Helper functions -----------------------------------------------------------
           namespace: namespace
           {: {
           identifier: TF_Operation
          pointer_declarator: * ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue* attr
           *: *
           ERROR: ToOperation(Node* node) {
  return static_cast<TF_Operation*>(static_cast<void*>(node));
}

string OutputName(const TF_Output& output) {
  return StrCat(output.oper->node.name(), ":", output.index);
}

const tensorflow::AttrValue* GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const tensorflow::AttrValue
            function_declarator: ToOperation(Node* node)
             identifier: ToOperation
             parameter_list: (Node* node)
              (: (
              parameter_declaration: Node* node
               type_identifier: Node
               pointer_declarator: * node
                *: *
                identifier: node
              ): )
            {: {
            return: return
            function_declarator: static_cast<TF_Operation*>(static_cast<void*>(node))
             template_function: static_cast<TF_Operation*>
              identifier: static_cast
              template_argument_list: <TF_Operation*>
               <: <
               type_descriptor: TF_Operation*
                type_identifier: TF_Operation
                abstract_pointer_declarator: *
                 *: *
               >: >
             parameter_list: (static_cast<void*>(node))
              (: (
              parameter_declaration: static_cast<void*>(node)
               template_type: static_cast<void*>
                type_identifier: static_cast
                template_argument_list: <void*>
                 <: <
                 type_descriptor: void*
                  primitive_type: void
                  abstract_pointer_declarator: *
                   *: *
                 >: >
               abstract_function_declarator: (node)
                parameter_list: (node)
                 (: (
                 parameter_declaration: node
                  type_identifier: node
                 ): )
              ): )
            ;: ;
            }: }
            identifier: string
            function_declarator: OutputName(const TF_Output& output)
             identifier: OutputName
             parameter_list: (const TF_Output& output)
              (: (
              parameter_declaration: const TF_Output& output
               type_qualifier: const
                const: const
               type_identifier: TF_Output
               reference_declarator: & output
                &: &
                identifier: output
              ): )
            {: {
            return: return
            function_declarator: StrCat(output.oper->node.name(), ":", output.index);
}

const
             identifier: StrCat
             parameter_list: (output.oper->node.name(), ":", output.index)
              (: (
              parameter_declaration: output.oper->node.name()
               type_identifier: output
               ERROR: .oper->node.
                .: .
                identifier: oper
                ->: ->
                identifier: node
                .: .
               function_declarator: name()
                identifier: name
                parameter_list: ()
                 (: (
                 ): )
              ,: ,
              ERROR: ":",
               ": "
               :: :
               ": "
               ,: ,
              parameter_declaration: output.index
               type_identifier: output
               ERROR: .
                .: .
               identifier: index
              ): )
             ERROR: ;
}
              ;: ;
              }: }
             type_qualifier: const
              const: const
            qualified_identifier: tensorflow::AttrValue
             namespace_identifier: tensorflow
             ::: ::
             identifier: AttrValue
            pointer_declarator: * GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const
             *: *
             function_declarator: GetAttrValue(TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status) {
  const
              identifier: GetAttrValue
              parameter_list: (TF_Operation* oper,
                                          const char* attr_name,
                                          TF_Status* status)
               (: (
               parameter_declaration: TF_Operation* oper
                type_identifier: TF_Operation
                pointer_declarator: * oper
                 *: *
                 identifier: oper
               ,: ,
               parameter_declaration: const char* attr_name
                type_qualifier: const
                 const: const
                primitive_type: char
                pointer_declarator: * attr_name
                 *: *
                 identifier: attr_name
               ,: ,
               parameter_declaration: TF_Status* status
                type_identifier: TF_Status
                pointer_declarator: * status
                 *: *
                 identifier: status
               ): )
              ERROR: {
               {: {
              type_qualifier: const
               const: const
            qualified_identifier: tensorflow::AttrValue
             namespace_identifier: tensorflow
             ::: ::
             identifier: AttrValue
           pointer_declarator: * attr
            *: *
            identifier: attr
          =: =
          binary_expression: oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.");
  }
  return attr;
}

TensorId ToTensorId(const TF_Output& output
           assignment_expression: oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status = InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.")
            field_expression: oper->node.attrs().Find(attr_name);
  if (attr == nullptr) {
    status->status
             call_expression: oper->node.attrs().Find(attr_name);
  if (attr == nullptr)
              call_expression: oper->node.attrs().Find(attr_name)
               field_expression: oper->node.attrs().Find
                call_expression: oper->node.attrs()
                 field_expression: oper->node.attrs
                  field_expression: oper->node
                   identifier: oper
                   ->: ->
                   field_identifier: node
                  .: .
                  field_identifier: attrs
                 argument_list: ()
                  (: (
                  ): )
                .: .
                field_identifier: Find
               argument_list: (attr_name)
                (: (
                identifier: attr_name
                ): )
              ERROR: ;
  if
               ;: ;
               if: if
              argument_list: (attr == nullptr)
               (: (
               binary_expression: attr == nullptr
                identifier: attr
                ==: ==
                null: nullptr
                 nullptr: nullptr
               ): )
             ERROR: {
    status
              {: {
              identifier: status
             ->: ->
             field_identifier: status
            =: =
            call_expression: InvalidArgument("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.")
             identifier: InvalidArgument
             argument_list: ("Operation '", oper->node.name(),
                                     "' has no attr named '", attr_name, "'.")
              (: (
              string_literal: "Operation '"
               ": "
               string_content: Operation '
               ": "
              ,: ,
              call_expression: oper->node.name()
               field_expression: oper->node.name
                field_expression: oper->node
                 identifier: oper
                 ->: ->
                 field_identifier: node
                .: .
                field_identifier: name
               argument_list: ()
                (: (
                ): )
              ,: ,
              string_literal: "' has no attr named '"
               ": "
               string_content: ' has no attr named '
               ": "
              ,: ,
              identifier: attr_name
              ,: ,
              string_literal: "'."
               ": "
               string_content: '.
               ": "
              ): )
           ERROR: ;
  }
  return attr;
}

TensorId ToTensorId(const TF_Output
            ;: ;
            }: }
            return: return
            identifier: attr
            ;: ;
            }: }
            identifier: TensorId
            identifier: ToTensorId
            (: (
            const: const
            identifier: TF_Output
           &: &
           identifier: output
         ): )
      ): 
    compound_statement: {
  return TensorId(output.oper->node.name(), output.index);
}
     {: {
     return_statement: return TensorId(output.oper->node.name(), output.index);
      return: return
      call_expression: TensorId(output.oper->node.name(), output.index)
       identifier: TensorId
       argument_list: (output.oper->node.name(), output.index)
        (: (
        call_expression: output.oper->node.name()
         field_expression: output.oper->node.name
          field_expression: output.oper->node
           field_expression: output.oper
            identifier: output
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: name
         argument_list: ()
          (: (
          ): )
        ,: ,
        field_expression: output.index
         identifier: output
         .: .
         field_identifier: index
        ): )
      ;: ;
     }: }
   preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
std::vector<tensorflow::Output> OutputsFromTFOutputs(TF_Output* tf_outputs,
                                                     int n) {
  std::vector<tensorflow::Output> outputs(n);
  for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
  return outputs;
}

void TFOutputsFromOutputs(const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs) {
  for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
}
#endif
    #if: #if
    binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
     unary_expression: !defined(IS_MOBILE_PLATFORM)
      !: !
      preproc_defined: defined(IS_MOBILE_PLATFORM)
       defined: defined
       (: (
       identifier: IS_MOBILE_PLATFORM
       ): )
     &&: &&
     unary_expression: !defined(IS_SLIM_BUILD)
      !: !
      preproc_defined: defined(IS_SLIM_BUILD)
       defined: defined
       (: (
       identifier: IS_SLIM_BUILD
       ): )
    
: 

    function_definition: std::vector<tensorflow::Output> OutputsFromTFOutputs(TF_Output* tf_outputs,
                                                     int n) {
  std::vector<tensorflow::Output> outputs(n);
  for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
  return outputs;
}
     qualified_identifier: std::vector<tensorflow::Output>
      namespace_identifier: std
      ::: ::
      template_type: vector<tensorflow::Output>
       type_identifier: vector
       template_argument_list: <tensorflow::Output>
        <: <
        type_descriptor: tensorflow::Output
         qualified_identifier: tensorflow::Output
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: Output
        >: >
     function_declarator: OutputsFromTFOutputs(TF_Output* tf_outputs,
                                                     int n)
      identifier: OutputsFromTFOutputs
      parameter_list: (TF_Output* tf_outputs,
                                                     int n)
       (: (
       parameter_declaration: TF_Output* tf_outputs
        type_identifier: TF_Output
        pointer_declarator: * tf_outputs
         *: *
         identifier: tf_outputs
       ,: ,
       parameter_declaration: int n
        primitive_type: int
        identifier: n
       ): )
     compound_statement: {
  std::vector<tensorflow::Output> outputs(n);
  for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
  return outputs;
}
      {: {
      declaration: std::vector<tensorflow::Output> outputs(n);
       qualified_identifier: std::vector<tensorflow::Output>
        namespace_identifier: std
        ::: ::
        template_type: vector<tensorflow::Output>
         type_identifier: vector
         template_argument_list: <tensorflow::Output>
          <: <
          type_descriptor: tensorflow::Output
           qualified_identifier: tensorflow::Output
            namespace_identifier: tensorflow
            ::: ::
            type_identifier: Output
          >: >
       function_declarator: outputs(n)
        identifier: outputs
        parameter_list: (n)
         (: (
         parameter_declaration: n
          type_identifier: n
         ): )
       ;: ;
      for_statement: for (int i = 0; i < n; ++i) {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
       for: for
       (: (
       declaration: int i = 0;
        primitive_type: int
        init_declarator: i = 0
         identifier: i
         =: =
         number_literal: 0
        ;: ;
       binary_expression: i < n
        identifier: i
        <: <
        identifier: n
       ;: ;
       update_expression: ++i
        ++: ++
        identifier: i
       ): )
       compound_statement: {
    outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
  }
        {: {
        expression_statement: outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index);
         assignment_expression: outputs[i] =
        tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index)
          subscript_expression: outputs[i]
           identifier: outputs
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          =: =
          call_expression: tensorflow::Output(&tf_outputs[i].oper->node, tf_outputs[i].index)
           qualified_identifier: tensorflow::Output
            namespace_identifier: tensorflow
            ::: ::
            identifier: Output
           argument_list: (&tf_outputs[i].oper->node, tf_outputs[i].index)
            (: (
            pointer_expression: &tf_outputs[i].oper->node
             &: &
             field_expression: tf_outputs[i].oper->node
              field_expression: tf_outputs[i].oper
               subscript_expression: tf_outputs[i]
                identifier: tf_outputs
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
               .: .
               field_identifier: oper
              ->: ->
              field_identifier: node
            ,: ,
            field_expression: tf_outputs[i].index
             subscript_expression: tf_outputs[i]
              identifier: tf_outputs
              subscript_argument_list: [i]
               [: [
               identifier: i
               ]: ]
             .: .
             field_identifier: index
            ): )
         ;: ;
        }: }
      return_statement: return outputs;
       return: return
       identifier: outputs
       ;: ;
      }: }
    function_definition: void TFOutputsFromOutputs(const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs) {
  for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
}
     primitive_type: void
     function_declarator: TFOutputsFromOutputs(const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs)
      identifier: TFOutputsFromOutputs
      parameter_list: (const std::vector<tensorflow::Output>& outputs,
                          TF_Output* tf_outputs)
       (: (
       parameter_declaration: const std::vector<tensorflow::Output>& outputs
        type_qualifier: const
         const: const
        qualified_identifier: std::vector<tensorflow::Output>
         namespace_identifier: std
         ::: ::
         template_type: vector<tensorflow::Output>
          type_identifier: vector
          template_argument_list: <tensorflow::Output>
           <: <
           type_descriptor: tensorflow::Output
            qualified_identifier: tensorflow::Output
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: Output
           >: >
        reference_declarator: & outputs
         &: &
         identifier: outputs
       ,: ,
       parameter_declaration: TF_Output* tf_outputs
        type_identifier: TF_Output
        pointer_declarator: * tf_outputs
         *: *
         identifier: tf_outputs
       ): )
     compound_statement: {
  for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
}
      {: {
      for_statement: for (int i = 0; i < outputs.size(); i++) {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
       for: for
       (: (
       declaration: int i = 0;
        primitive_type: int
        init_declarator: i = 0
         identifier: i
         =: =
         number_literal: 0
        ;: ;
       binary_expression: i < outputs.size()
        identifier: i
        <: <
        call_expression: outputs.size()
         field_expression: outputs.size
          identifier: outputs
          .: .
          field_identifier: size
         argument_list: ()
          (: (
          ): )
       ;: ;
       update_expression: i++
        identifier: i
        ++: ++
       ): )
       compound_statement: {
    tf_outputs[i].oper = ToOperation(outputs[i].node());
    tf_outputs[i].index = outputs[i].index();
  }
        {: {
        expression_statement: tf_outputs[i].oper = ToOperation(outputs[i].node());
         assignment_expression: tf_outputs[i].oper = ToOperation(outputs[i].node())
          field_expression: tf_outputs[i].oper
           subscript_expression: tf_outputs[i]
            identifier: tf_outputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           .: .
           field_identifier: oper
          =: =
          call_expression: ToOperation(outputs[i].node())
           identifier: ToOperation
           argument_list: (outputs[i].node())
            (: (
            call_expression: outputs[i].node()
             field_expression: outputs[i].node
              subscript_expression: outputs[i]
               identifier: outputs
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              .: .
              field_identifier: node
             argument_list: ()
              (: (
              ): )
            ): )
         ;: ;
        expression_statement: tf_outputs[i].index = outputs[i].index();
         assignment_expression: tf_outputs[i].index = outputs[i].index()
          field_expression: tf_outputs[i].index
           subscript_expression: tf_outputs[i]
            identifier: tf_outputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           .: .
           field_identifier: index
          =: =
          call_expression: outputs[i].index()
           field_expression: outputs[i].index
            subscript_expression: outputs[i]
             identifier: outputs
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            .: .
            field_identifier: index
           argument_list: ()
            (: (
            ): )
         ;: ;
        }: }
      }: }
    #endif: #endif
   comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
   }: }
 comment: // namespace
 comment: // Shape functions -----------------------------------------------------------
 function_definition: void TF_GraphSetTensorShape(TF_Graph* graph, TF_Output output,
                            const int64_t* dims, const int num_dims,
                            TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
  tensorflow::shape_inference::ShapeHandle new_shape =
      tensorflow::ShapeHandleFromDims(ic, num_dims, dims);
  status->status = graph->refiner.SetShape(node, output.index, new_shape);
}
  primitive_type: void
  function_declarator: TF_GraphSetTensorShape(TF_Graph* graph, TF_Output output,
                            const int64_t* dims, const int num_dims,
                            TF_Status* status)
   identifier: TF_GraphSetTensorShape
   parameter_list: (TF_Graph* graph, TF_Output output,
                            const int64_t* dims, const int num_dims,
                            TF_Status* status)
    (: (
    parameter_declaration: TF_Graph* graph
     type_identifier: TF_Graph
     pointer_declarator: * graph
      *: *
      identifier: graph
    ,: ,
    parameter_declaration: TF_Output output
     type_identifier: TF_Output
     identifier: output
    ,: ,
    parameter_declaration: const int64_t* dims
     type_qualifier: const
      const: const
     primitive_type: int64_t
     pointer_declarator: * dims
      *: *
      identifier: dims
    ,: ,
    parameter_declaration: const int num_dims
     type_qualifier: const
      const: const
     primitive_type: int
     identifier: num_dims
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
  tensorflow::shape_inference::ShapeHandle new_shape =
      tensorflow::ShapeHandleFromDims(ic, num_dims, dims);
  status->status = graph->refiner.SetShape(node, output.index, new_shape);
}
   {: {
   declaration: Node* node = &output.oper->node;
    type_identifier: Node
    init_declarator: * node = &output.oper->node
     pointer_declarator: * node
      *: *
      identifier: node
     =: =
     pointer_expression: &output.oper->node
      &: &
      field_expression: output.oper->node
       field_expression: output.oper
        identifier: output
        .: .
        field_identifier: oper
       ->: ->
       field_identifier: node
    ;: ;
   declaration: mutex_lock l(graph->mu);
    type_identifier: mutex_lock
    init_declarator: l(graph->mu)
     identifier: l
     argument_list: (graph->mu)
      (: (
      field_expression: graph->mu
       identifier: graph
       ->: ->
       field_identifier: mu
      ): )
    ;: ;
   declaration: tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
    qualified_identifier: tensorflow::shape_inference::InferenceContext
     namespace_identifier: tensorflow
     ::: ::
     qualified_identifier: shape_inference::InferenceContext
      namespace_identifier: shape_inference
      ::: ::
      type_identifier: InferenceContext
    init_declarator: * ic =
      graph->refiner.GetContext(node)
     pointer_declarator: * ic
      *: *
      identifier: ic
     =: =
     call_expression: graph->refiner.GetContext(node)
      field_expression: graph->refiner.GetContext
       field_expression: graph->refiner
        identifier: graph
        ->: ->
        field_identifier: refiner
       .: .
       field_identifier: GetContext
      argument_list: (node)
       (: (
       identifier: node
       ): )
    ;: ;
   if_statement: if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
    if: if
    condition_clause: (ic == nullptr)
     (: (
     binary_expression: ic == nullptr
      identifier: ic
      ==: ==
      null: nullptr
       nullptr: nullptr
     ): )
    compound_statement: {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
     {: {
     expression_statement: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
      assignment_expression: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph")
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: InvalidArgument("Node ", node->name(), " was not found in the graph")
        identifier: InvalidArgument
        argument_list: ("Node ", node->name(), " was not found in the graph")
         (: (
         string_literal: "Node "
          ": "
          string_content: Node 
          ": "
         ,: ,
         call_expression: node->name()
          field_expression: node->name
           identifier: node
           ->: ->
           field_identifier: name
          argument_list: ()
           (: (
           ): )
         ,: ,
         string_literal: " was not found in the graph"
          ": "
          string_content:  was not found in the graph
          ": "
         ): )
      ;: ;
     return_statement: return;
      return: return
      ;: ;
     }: }
   declaration: tensorflow::shape_inference::ShapeHandle new_shape =
      tensorflow::ShapeHandleFromDims(ic, num_dims, dims);
    qualified_identifier: tensorflow::shape_inference::ShapeHandle
     namespace_identifier: tensorflow
     ::: ::
     qualified_identifier: shape_inference::ShapeHandle
      namespace_identifier: shape_inference
      ::: ::
      type_identifier: ShapeHandle
    init_declarator: new_shape =
      tensorflow::ShapeHandleFromDims(ic, num_dims, dims)
     identifier: new_shape
     =: =
     call_expression: tensorflow::ShapeHandleFromDims(ic, num_dims, dims)
      qualified_identifier: tensorflow::ShapeHandleFromDims
       namespace_identifier: tensorflow
       ::: ::
       identifier: ShapeHandleFromDims
      argument_list: (ic, num_dims, dims)
       (: (
       identifier: ic
       ,: ,
       identifier: num_dims
       ,: ,
       identifier: dims
       ): )
    ;: ;
   expression_statement: status->status = graph->refiner.SetShape(node, output.index, new_shape);
    assignment_expression: status->status = graph->refiner.SetShape(node, output.index, new_shape)
     field_expression: status->status
      identifier: status
      ->: ->
      field_identifier: status
     =: =
     call_expression: graph->refiner.SetShape(node, output.index, new_shape)
      field_expression: graph->refiner.SetShape
       field_expression: graph->refiner
        identifier: graph
        ->: ->
        field_identifier: refiner
       .: .
       field_identifier: SetShape
      argument_list: (node, output.index, new_shape)
       (: (
       identifier: node
       ,: ,
       field_expression: output.index
        identifier: output
        .: .
        field_identifier: index
       ,: ,
       identifier: new_shape
       ): )
    ;: ;
   }: }
 function_definition: int TF_GraphGetTensorNumDims(TF_Graph* graph, TF_Output output,
                             TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return -1;
  }

  tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);

  // Unknown rank means the number of dimensions is -1.
  if (!ic->RankKnown(shape)) {
    return -1;
  }

  return ic->Rank(shape);
}
  primitive_type: int
  function_declarator: TF_GraphGetTensorNumDims(TF_Graph* graph, TF_Output output,
                             TF_Status* status)
   identifier: TF_GraphGetTensorNumDims
   parameter_list: (TF_Graph* graph, TF_Output output,
                             TF_Status* status)
    (: (
    parameter_declaration: TF_Graph* graph
     type_identifier: TF_Graph
     pointer_declarator: * graph
      *: *
      identifier: graph
    ,: ,
    parameter_declaration: TF_Output output
     type_identifier: TF_Output
     identifier: output
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return -1;
  }

  tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);

  // Unknown rank means the number of dimensions is -1.
  if (!ic->RankKnown(shape)) {
    return -1;
  }

  return ic->Rank(shape);
}
   {: {
   declaration: Node* node = &output.oper->node;
    type_identifier: Node
    init_declarator: * node = &output.oper->node
     pointer_declarator: * node
      *: *
      identifier: node
     =: =
     pointer_expression: &output.oper->node
      &: &
      field_expression: output.oper->node
       field_expression: output.oper
        identifier: output
        .: .
        field_identifier: oper
       ->: ->
       field_identifier: node
    ;: ;
   declaration: mutex_lock l(graph->mu);
    type_identifier: mutex_lock
    init_declarator: l(graph->mu)
     identifier: l
     argument_list: (graph->mu)
      (: (
      field_expression: graph->mu
       identifier: graph
       ->: ->
       field_identifier: mu
      ): )
    ;: ;
   declaration: tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
    qualified_identifier: tensorflow::shape_inference::InferenceContext
     namespace_identifier: tensorflow
     ::: ::
     qualified_identifier: shape_inference::InferenceContext
      namespace_identifier: shape_inference
      ::: ::
      type_identifier: InferenceContext
    init_declarator: * ic =
      graph->refiner.GetContext(node)
     pointer_declarator: * ic
      *: *
      identifier: ic
     =: =
     call_expression: graph->refiner.GetContext(node)
      field_expression: graph->refiner.GetContext
       field_expression: graph->refiner
        identifier: graph
        ->: ->
        field_identifier: refiner
       .: .
       field_identifier: GetContext
      argument_list: (node)
       (: (
       identifier: node
       ): )
    ;: ;
   if_statement: if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return -1;
  }
    if: if
    condition_clause: (ic == nullptr)
     (: (
     binary_expression: ic == nullptr
      identifier: ic
      ==: ==
      null: nullptr
       nullptr: nullptr
     ): )
    compound_statement: {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return -1;
  }
     {: {
     expression_statement: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
      assignment_expression: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph")
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: InvalidArgument("Node ", node->name(), " was not found in the graph")
        identifier: InvalidArgument
        argument_list: ("Node ", node->name(), " was not found in the graph")
         (: (
         string_literal: "Node "
          ": "
          string_content: Node 
          ": "
         ,: ,
         call_expression: node->name()
          field_expression: node->name
           identifier: node
           ->: ->
           field_identifier: name
          argument_list: ()
           (: (
           ): )
         ,: ,
         string_literal: " was not found in the graph"
          ": "
          string_content:  was not found in the graph
          ": "
         ): )
      ;: ;
     return_statement: return -1;
      return: return
      number_literal: -1
      ;: ;
     }: }
   declaration: tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);
    qualified_identifier: tensorflow::shape_inference::ShapeHandle
     namespace_identifier: tensorflow
     ::: ::
     qualified_identifier: shape_inference::ShapeHandle
      namespace_identifier: shape_inference
      ::: ::
      type_identifier: ShapeHandle
    init_declarator: shape = ic->output(output.index)
     identifier: shape
     =: =
     call_expression: ic->output(output.index)
      field_expression: ic->output
       identifier: ic
       ->: ->
       field_identifier: output
      argument_list: (output.index)
       (: (
       field_expression: output.index
        identifier: output
        .: .
        field_identifier: index
       ): )
    ;: ;
   comment: // Unknown rank means the number of dimensions is -1.
   if_statement: if (!ic->RankKnown(shape)) {
    return -1;
  }
    if: if
    condition_clause: (!ic->RankKnown(shape))
     (: (
     unary_expression: !ic->RankKnown(shape)
      !: !
      call_expression: ic->RankKnown(shape)
       field_expression: ic->RankKnown
        identifier: ic
        ->: ->
        field_identifier: RankKnown
       argument_list: (shape)
        (: (
        identifier: shape
        ): )
     ): )
    compound_statement: {
    return -1;
  }
     {: {
     return_statement: return -1;
      return: return
      number_literal: -1
      ;: ;
     }: }
   return_statement: return ic->Rank(shape);
    return: return
    call_expression: ic->Rank(shape)
     field_expression: ic->Rank
      identifier: ic
      ->: ->
      field_identifier: Rank
     argument_list: (shape)
      (: (
      identifier: shape
      ): )
    ;: ;
   }: }
 function_definition: void TF_GraphGetTensorShape(TF_Graph* graph, TF_Output output, int64_t* dims,
                            int num_dims, TF_Status* status) {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);

  int rank = -1;
  if (ic->RankKnown(shape)) {
    rank = ic->Rank(shape);
  }

  if (num_dims != rank) {
    status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank);
    return;
  }

  if (num_dims == 0) {
    // Output shape is a scalar.
    return;
  }

  // Rank is greater than 0, so fill in the values, if known, and
  // -1 for unknown values.
  for (int i = 0; i < num_dims; ++i) {
    auto dim = ic->Dim(shape, i);
    int64_t value = -1;
    if (ic->ValueKnown(dim)) {
      value = ic->Value(dim);
    }
    dims[i] = value;
  }
}
  primitive_type: void
  function_declarator: TF_GraphGetTensorShape(TF_Graph* graph, TF_Output output, int64_t* dims,
                            int num_dims, TF_Status* status)
   identifier: TF_GraphGetTensorShape
   parameter_list: (TF_Graph* graph, TF_Output output, int64_t* dims,
                            int num_dims, TF_Status* status)
    (: (
    parameter_declaration: TF_Graph* graph
     type_identifier: TF_Graph
     pointer_declarator: * graph
      *: *
      identifier: graph
    ,: ,
    parameter_declaration: TF_Output output
     type_identifier: TF_Output
     identifier: output
    ,: ,
    parameter_declaration: int64_t* dims
     primitive_type: int64_t
     pointer_declarator: * dims
      *: *
      identifier: dims
    ,: ,
    parameter_declaration: int num_dims
     primitive_type: int
     identifier: num_dims
    ,: ,
    parameter_declaration: TF_Status* status
     type_identifier: TF_Status
     pointer_declarator: * status
      *: *
      identifier: status
    ): )
  compound_statement: {
  Node* node = &output.oper->node;

  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
  if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }

  tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);

  int rank = -1;
  if (ic->RankKnown(shape)) {
    rank = ic->Rank(shape);
  }

  if (num_dims != rank) {
    status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank);
    return;
  }

  if (num_dims == 0) {
    // Output shape is a scalar.
    return;
  }

  // Rank is greater than 0, so fill in the values, if known, and
  // -1 for unknown values.
  for (int i = 0; i < num_dims; ++i) {
    auto dim = ic->Dim(shape, i);
    int64_t value = -1;
    if (ic->ValueKnown(dim)) {
      value = ic->Value(dim);
    }
    dims[i] = value;
  }
}
   {: {
   declaration: Node* node = &output.oper->node;
    type_identifier: Node
    init_declarator: * node = &output.oper->node
     pointer_declarator: * node
      *: *
      identifier: node
     =: =
     pointer_expression: &output.oper->node
      &: &
      field_expression: output.oper->node
       field_expression: output.oper
        identifier: output
        .: .
        field_identifier: oper
       ->: ->
       field_identifier: node
    ;: ;
   declaration: mutex_lock l(graph->mu);
    type_identifier: mutex_lock
    init_declarator: l(graph->mu)
     identifier: l
     argument_list: (graph->mu)
      (: (
      field_expression: graph->mu
       identifier: graph
       ->: ->
       field_identifier: mu
      ): )
    ;: ;
   declaration: tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(node);
    qualified_identifier: tensorflow::shape_inference::InferenceContext
     namespace_identifier: tensorflow
     ::: ::
     qualified_identifier: shape_inference::InferenceContext
      namespace_identifier: shape_inference
      ::: ::
      type_identifier: InferenceContext
    init_declarator: * ic =
      graph->refiner.GetContext(node)
     pointer_declarator: * ic
      *: *
      identifier: ic
     =: =
     call_expression: graph->refiner.GetContext(node)
      field_expression: graph->refiner.GetContext
       field_expression: graph->refiner
        identifier: graph
        ->: ->
        field_identifier: refiner
       .: .
       field_identifier: GetContext
      argument_list: (node)
       (: (
       identifier: node
       ): )
    ;: ;
   if_statement: if (ic == nullptr) {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
    if: if
    condition_clause: (ic == nullptr)
     (: (
     binary_expression: ic == nullptr
      identifier: ic
      ==: ==
      null: nullptr
       nullptr: nullptr
     ): )
    compound_statement: {
    status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
    return;
  }
     {: {
     expression_statement: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph");
      assignment_expression: status->status =
        InvalidArgument("Node ", node->name(), " was not found in the graph")
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: InvalidArgument("Node ", node->name(), " was not found in the graph")
        identifier: InvalidArgument
        argument_list: ("Node ", node->name(), " was not found in the graph")
         (: (
         string_literal: "Node "
          ": "
          string_content: Node 
          ": "
         ,: ,
         call_expression: node->name()
          field_expression: node->name
           identifier: node
           ->: ->
           field_identifier: name
          argument_list: ()
           (: (
           ): )
         ,: ,
         string_literal: " was not found in the graph"
          ": "
          string_content:  was not found in the graph
          ": "
         ): )
      ;: ;
     return_statement: return;
      return: return
      ;: ;
     }: }
   declaration: tensorflow::shape_inference::ShapeHandle shape = ic->output(output.index);
    qualified_identifier: tensorflow::shape_inference::ShapeHandle
     namespace_identifier: tensorflow
     ::: ::
     qualified_identifier: shape_inference::ShapeHandle
      namespace_identifier: shape_inference
      ::: ::
      type_identifier: ShapeHandle
    init_declarator: shape = ic->output(output.index)
     identifier: shape
     =: =
     call_expression: ic->output(output.index)
      field_expression: ic->output
       identifier: ic
       ->: ->
       field_identifier: output
      argument_list: (output.index)
       (: (
       field_expression: output.index
        identifier: output
        .: .
        field_identifier: index
       ): )
    ;: ;
   declaration: int rank = -1;
    primitive_type: int
    init_declarator: rank = -1
     identifier: rank
     =: =
     number_literal: -1
    ;: ;
   if_statement: if (ic->RankKnown(shape)) {
    rank = ic->Rank(shape);
  }
    if: if
    condition_clause: (ic->RankKnown(shape))
     (: (
     call_expression: ic->RankKnown(shape)
      field_expression: ic->RankKnown
       identifier: ic
       ->: ->
       field_identifier: RankKnown
      argument_list: (shape)
       (: (
       identifier: shape
       ): )
     ): )
    compound_statement: {
    rank = ic->Rank(shape);
  }
     {: {
     expression_statement: rank = ic->Rank(shape);
      assignment_expression: rank = ic->Rank(shape)
       identifier: rank
       =: =
       call_expression: ic->Rank(shape)
        field_expression: ic->Rank
         identifier: ic
         ->: ->
         field_identifier: Rank
        argument_list: (shape)
         (: (
         identifier: shape
         ): )
      ;: ;
     }: }
   if_statement: if (num_dims != rank) {
    status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank);
    return;
  }
    if: if
    condition_clause: (num_dims != rank)
     (: (
     binary_expression: num_dims != rank
      identifier: num_dims
      !=: !=
      identifier: rank
     ): )
    compound_statement: {
    status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank);
    return;
  }
     {: {
     expression_statement: status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank);
      assignment_expression: status->status = InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: InvalidArgument("Expected rank is ", num_dims,
                                     " but actual rank is ", rank)
        identifier: InvalidArgument
        argument_list: ("Expected rank is ", num_dims,
                                     " but actual rank is ", rank)
         (: (
         string_literal: "Expected rank is "
          ": "
          string_content: Expected rank is 
          ": "
         ,: ,
         identifier: num_dims
         ,: ,
         string_literal: " but actual rank is "
          ": "
          string_content:  but actual rank is 
          ": "
         ,: ,
         identifier: rank
         ): )
      ;: ;
     return_statement: return;
      return: return
      ;: ;
     }: }
   if_statement: if (num_dims == 0) {
    // Output shape is a scalar.
    return;
  }
    if: if
    condition_clause: (num_dims == 0)
     (: (
     binary_expression: num_dims == 0
      identifier: num_dims
      ==: ==
      number_literal: 0
     ): )
    compound_statement: {
    // Output shape is a scalar.
    return;
  }
     {: {
     comment: // Output shape is a scalar.
     return_statement: return;
      return: return
      ;: ;
     }: }
   comment: // Rank is greater than 0, so fill in the values, if known, and
   comment: // -1 for unknown values.
   for_statement: for (int i = 0; i < num_dims; ++i) {
    auto dim = ic->Dim(shape, i);
    int64_t value = -1;
    if (ic->ValueKnown(dim)) {
      value = ic->Value(dim);
    }
    dims[i] = value;
  }
    for: for
    (: (
    declaration: int i = 0;
     primitive_type: int
     init_declarator: i = 0
      identifier: i
      =: =
      number_literal: 0
     ;: ;
    binary_expression: i < num_dims
     identifier: i
     <: <
     identifier: num_dims
    ;: ;
    update_expression: ++i
     ++: ++
     identifier: i
    ): )
    compound_statement: {
    auto dim = ic->Dim(shape, i);
    int64_t value = -1;
    if (ic->ValueKnown(dim)) {
      value = ic->Value(dim);
    }
    dims[i] = value;
  }
     {: {
     declaration: auto dim = ic->Dim(shape, i);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: dim = ic->Dim(shape, i)
       identifier: dim
       =: =
       call_expression: ic->Dim(shape, i)
        field_expression: ic->Dim
         identifier: ic
         ->: ->
         field_identifier: Dim
        argument_list: (shape, i)
         (: (
         identifier: shape
         ,: ,
         identifier: i
         ): )
      ;: ;
     declaration: int64_t value = -1;
      primitive_type: int64_t
      init_declarator: value = -1
       identifier: value
       =: =
       number_literal: -1
      ;: ;
     if_statement: if (ic->ValueKnown(dim)) {
      value = ic->Value(dim);
    }
      if: if
      condition_clause: (ic->ValueKnown(dim))
       (: (
       call_expression: ic->ValueKnown(dim)
        field_expression: ic->ValueKnown
         identifier: ic
         ->: ->
         field_identifier: ValueKnown
        argument_list: (dim)
         (: (
         identifier: dim
         ): )
       ): )
      compound_statement: {
      value = ic->Value(dim);
    }
       {: {
       expression_statement: value = ic->Value(dim);
        assignment_expression: value = ic->Value(dim)
         identifier: value
         =: =
         call_expression: ic->Value(dim)
          field_expression: ic->Value
           identifier: ic
           ->: ->
           field_identifier: Value
          argument_list: (dim)
           (: (
           identifier: dim
           ): )
        ;: ;
       }: }
     expression_statement: dims[i] = value;
      assignment_expression: dims[i] = value
       subscript_expression: dims[i]
        identifier: dims
        subscript_argument_list: [i]
         [: [
         identifier: i
         ]: ]
       =: =
       identifier: value
      ;: ;
     }: }
   }: }
 comment: // TF_OperationDescription functions ------------------------------------------
 linkage_specification: extern "C" {

TF_OperationDescription* TF_NewOperationLocked(TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
    TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu) {
  return new TF_OperationDescription(graph, op_type, oper_name);
}

TF_OperationDescription* TF_NewOperation(TF_Graph* graph, const char* op_type,
                                         const char* oper_name) {
  mutex_lock l(graph->mu);
  return TF_NewOperationLocked(graph, op_type, oper_name);
}

void TF_SetDevice(TF_OperationDescription* desc, const char* device) {
  desc->node_builder.Device(device);
}

void TF_AddInput(TF_OperationDescription* desc, TF_Output input) {
  desc->node_builder.Input(&input.oper->node, input.index);
}

void TF_AddInputList(TF_OperationDescription* desc, const TF_Output* inputs,
                     int num_inputs) {
  std::vector<NodeBuilder::NodeOut> input_list;
  input_list.reserve(num_inputs);
  for (int i = 0; i < num_inputs; ++i) {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
  desc->node_builder.Input(input_list);
}

void TF_AddControlInput(TF_OperationDescription* desc, TF_Operation* input) {
  desc->node_builder.ControlInput(&input->node);
}

void TF_ColocateWith(TF_OperationDescription* desc, TF_Operation* op) {
  desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()));
}

void TF_SetAttrString(TF_OperationDescription* desc, const char* attr_name,
                      const void* value, size_t length) {
  absl::string_view s(static_cast<const char*>(value), length);
  desc->node_builder.Attr(attr_name, s);
}

void TF_SetAttrStringList(TF_OperationDescription* desc, const char* attr_name,
                          const void* const* values, const size_t* lengths,
                          int num_values) {
  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  } else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
}

void TF_SetAttrInt(TF_OperationDescription* desc, const char* attr_name,
                   int64_t value) {
  desc->node_builder.Attr(attr_name, static_cast<int64_t>(value));
}

void TF_SetAttrIntList(TF_OperationDescription* desc, const char* attr_name,
                       const int64_t* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values));
}

void TF_SetAttrFloat(TF_OperationDescription* desc, const char* attr_name,
                     float value) {
  desc->node_builder.Attr(attr_name, value);
}

void TF_SetAttrFloatList(TF_OperationDescription* desc, const char* attr_name,
                         const float* values, int num_values) {
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values));
}

void TF_SetAttrBool(TF_OperationDescription* desc, const char* attr_name,
                    unsigned char value) {
  desc->node_builder.Attr(attr_name, static_cast<bool>(value));
}

void TF_SetAttrBoolList(TF_OperationDescription* desc, const char* attr_name,
                        const unsigned char* values, int num_values) {
  std::unique_ptr<bool[]> b(new bool[num_values]);
  for (int i = 0; i < num_values; ++i) {
    b[i] = values[i];
  }
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values));
}

void TF_SetAttrType(TF_OperationDescription* desc, const char* attr_name,
                    TF_DataType value) {
  desc->node_builder.Attr(attr_name, static_cast<DataType>(value));
}

void TF_SetAttrTypeList(TF_OperationDescription* desc, const char* attr_name,
                        const TF_DataType* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values));
}

void TF_SetAttrPlaceholder(TF_OperationDescription* desc, const char* attr_name,
                           const char* placeholder) {
  tensorflow::AttrValue attr_value;
  attr_value.set_placeholder(placeholder);
  desc->node_builder.Attr(attr_name, attr_value);
}

void TF_SetAttrFuncName(TF_OperationDescription* desc, const char* attr_name,
                        const char* value, size_t length) {
  tensorflow::NameAttrList func_name;
  func_name.set_name(string(value, value + length));
  desc->node_builder.Attr(attr_name, func_name);
}

void TF_SetAttrShape(TF_OperationDescription* desc, const char* attr_name,
                     const int64_t* dims, int num_dims) {
  PartialTensorShape shape;
  if (num_dims >= 0) {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
  desc->node_builder.Attr(attr_name, shape);
}

void TF_SetAttrShapeList(TF_OperationDescription* desc, const char* attr_name,
                         const int64_t* const* dims, const int* num_dims,
                         int num_shapes) {
  std::vector<PartialTensorShape> shapes;
  shapes.reserve(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
}

void TF_SetAttrTensorShapeProto(TF_OperationDescription* desc,
                                const char* attr_name, const void* proto,
                                size_t proto_len, TF_Status* status) {
  // shape.ParseFromArray takes an int as length, this function takes size_t,
  // make sure there is no information loss.
  if (proto_len > std::numeric_limits<int>::max()) {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
  TensorShapeProto shape;
  if (shape.ParseFromArray(proto, static_cast<int>(proto_len))) {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  } else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
}

void TF_SetAttrTensorShapeProtoList(TF_OperationDescription* desc,
                                    const char* attr_name,
                                    const void* const* protos,
                                    const size_t* proto_lens, int num_shapes,
                                    TF_Status* status) {
  std::vector<TensorShapeProto> shapes;
  shapes.resize(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
  status->status = absl::OkStatus();
}

void TF_SetAttrTensor(TF_OperationDescription* desc, const char* attr_name,
                      TF_Tensor* value, TF_Status* status) {
  Tensor t;
  status->status = TF_TensorToTensor(value, &t);
  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}

void TF_SetAttrTensorList(TF_OperationDescription* desc, const char* attr_name,
                          TF_Tensor* const* values, int num_values,
                          TF_Status* status) {
  status->status = absl::OkStatus();
  std::vector<Tensor> t;
  t.reserve(num_values);

  for (int i = 0; i < num_values && status->status.ok(); ++i) {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }

  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}

void TF_SetAttrValueProto(TF_OperationDescription* desc, const char* attr_name,
                          const void* proto, size_t proto_len,
                          TF_Status* status) {
  tensorflow::AttrValue attr_value;
  if (!attr_value.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }

  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  } else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }

  status->status = absl::OkStatus();
}

TF_Operation* TF_FinishOperationLocked(TF_OperationDescription* desc,
                                       TF_Status* status)
    TF_EXCLUSIVE_LOCKS_REQUIRED(desc->graph->mu) {
  Node* ret = nullptr;

  if (desc->graph->name_map.count(desc->node_builder.node_name())) {
    status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
  } else {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }

  delete desc;

  return ToOperation(ret);
}

TF_Operation* TF_FinishOperation(TF_OperationDescription* desc,
                                 TF_Status* status) {
  mutex_lock l(desc->graph->mu);
  return TF_FinishOperationLocked(desc, status);
}

// TF_Operation functions
// ----------------------------------------------------------

const char* TF_OperationName(TF_Operation* oper) {
  return oper->node.name().c_str();
}

const char* TF_OperationOpType(TF_Operation* oper) {
  return oper->node.type_string().c_str();
}

const char* TF_OperationDevice(TF_Operation* oper) {
  return oper->node.requested_device().c_str();
}

int TF_OperationNumOutputs(TF_Operation* oper) {
  return oper->node.num_outputs();
}

TF_DataType TF_OperationOutputType(TF_Output oper_out) {
  return static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index));
}

int TF_OperationOutputListLength(TF_Operation* oper, const char* arg_name,
                                 TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}

int TF_OperationNumInputs(TF_Operation* oper) {
  return oper->node.num_inputs();
}

TF_DataType TF_OperationInputType(TF_Input oper_in) {
  return static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index));
}

int TF_OperationInputListLength(TF_Operation* oper, const char* arg_name,
                                TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}

TF_Output TF_OperationInput(TF_Input oper_in) {
  const tensorflow::Edge* edge;
  Status s = oper_in.oper->node.input_edge(oper_in.index, &edge);
  if (!s.ok()) {
    return {nullptr, -1};
  }

  return {ToOperation(edge->src()), edge->src_output()};
}

void TF_OperationAllInputs(TF_Operation* oper, TF_Output* inputs,
                           int max_inputs) {
  for (auto* edge : oper->node.in_edges()) {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
}

int TF_OperationOutputNumConsumers(TF_Output oper_out) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
  return count;
}

int TF_OperationOutputConsumers(TF_Output oper_out, TF_Input* consumers,
                                int max_consumers) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
  return count;
}

int TF_OperationNumControlInputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
  return count;
}

int TF_OperationGetControlInputs(TF_Operation* oper,
                                 TF_Operation** control_inputs,
                                 int max_control_inputs) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
  return count;
}

int TF_OperationNumControlOutputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
  return count;
}

int TF_OperationGetControlOutputs(TF_Operation* oper,
                                  TF_Operation** control_outputs,
                                  int max_control_outputs) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
  return count;
}

TF_AttrMetadata TF_OperationGetAttrMetadata(TF_Operation* oper,
                                            const char* attr_name,
                                            TF_Status* status) {
  TF_AttrMetadata metadata;
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return metadata;
  switch (attr->value_case()) {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
  return metadata;
}

void TF_OperationGetAttrString(TF_Operation* oper, const char* attr_name,
                               void* value, size_t max_length,
                               TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kS) {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
  if (max_length <= 0) {
    return;
  }
  const auto& s = attr->s();
  std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length));
}

void TF_OperationGetAttrStringList(TF_Operation* oper, const char* attr_name,
                                   void** values, size_t* lengths,
                                   int max_values, void* storage,
                                   size_t storage_size, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().s_size());
  char* p = static_cast<char*>(storage);
  for (int i = 0; i < len; ++i) {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
}

#define DEFINE_GETATTR(func, c_type, cpp_type, list_field)                   \
  void func(TF_Operation* oper, const char* attr_name, c_type* value,        \
            TF_Status* status) {                                             \
    cpp_type v;                                                              \
    status->status =                                                         \
        tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &v);          \
    if (!status->status.ok()) return;                                        \
    *value = static_cast<c_type>(v);                                         \
  }                                                                          \
  void func##List(TF_Operation* oper, const char* attr_name, c_type* values, \
                  int max_values, TF_Status* status) {                       \
    const auto* attr = GetAttrValue(oper, attr_name, status);                \
    if (!status->status.ok()) return;                                        \
    if (attr->value_case() != tensorflow::AttrValue::kList) {                \
      status->status =                                                       \
          InvalidArgument("Value for '", attr_name, "' is not a list.");     \
      return;                                                                \
    }                                                                        \
    const auto len = std::min(max_values, attr->list().list_field##_size()); \
    for (int i = 0; i < len; ++i) {                                          \
      values[i] = static_cast<c_type>(attr->list().list_field(i));           \
    }                                                                        \
  }
DEFINE_GETATTR(TF_OperationGetAttrInt, int64_t, int64_t, i);
DEFINE_GETATTR(TF_OperationGetAttrFloat, float, float, f);
DEFINE_GETATTR(TF_OperationGetAttrBool, unsigned char, bool, b);
DEFINE_GETATTR(TF_OperationGetAttrType, TF_DataType, DataType, type);
#undef DEFINE_GETATTR

void TF_OperationGetAttrShape(TF_Operation* oper, const char* attr_name,
                              int64_t* value, int num_dims, TF_Status* status) {
  PartialTensorShape shape;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape);
  if (!status->status.ok()) return;
  auto len = std::min(shape.dims(), num_dims);
  for (int i = 0; i < len; ++i) {
    value[i] = shape.dim_size(i);
  }
}

void TF_OperationGetAttrShapeList(TF_Operation* oper, const char* attr_name,
                                  int64_t** dims, int* num_dims, int num_shapes,
                                  int64_t* storage, int storage_size,
                                  TF_Status* status) {
  std::vector<PartialTensorShape> shapes;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes);
  if (!status->status.ok()) return;
  auto len = std::min(static_cast<int>(shapes.size()), num_shapes);
  int64_t* p = storage;
  int storage_left = storage_size;
  for (int i = 0; i < len; ++i) {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
}

void TF_OperationGetAttrTensorShapeProto(TF_Operation* oper,
                                         const char* attr_name,
                                         TF_Buffer* value, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kShape) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
  status->status = MessageToBuffer(attr->shape(), value);
}

void TF_OperationGetAttrTensorShapeProtoList(TF_Operation* oper,
                                             const char* attr_name,
                                             TF_Buffer** values, int max_values,
                                             TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().shape_size());
  for (int i = 0; i < len; ++i) {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
}

void TF_OperationGetAttrTensor(TF_Operation* oper, const char* attr_name,
                               TF_Tensor** value, TF_Status* status) {
  *value = nullptr;
  Tensor t;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t);
  if (!status->status.ok()) return;
  *value = TF_TensorFromTensor(t, &status->status);
}

void TF_OperationGetAttrTensorList(TF_Operation* oper, const char* attr_name,
                                   TF_Tensor** values, int max_values,
                                   TF_Status* status) {
  std::vector<Tensor> ts;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts);
  if (!status->status.ok()) return;
  const auto len = std::min(max_values, static_cast<int>(ts.size()));
  for (int i = 0; i < len; ++i) {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
}

void TF_OperationGetAttrValueProto(TF_Operation* oper, const char* attr_name,
                                   TF_Buffer* output_attr_value,
                                   TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  status->status = MessageToBuffer(*attr, output_attr_value);
}

int TF_OperationGetNumAttrs(TF_Operation* oper) {
  return oper->node.attrs().size();
}

int TF_OperationGetAttrNameLength(TF_Operation* oper, int i) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
  return -1;
}

void TF_OperationGetAttrName(TF_Operation* oper, int i, char* output,
                             TF_Status* status) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
  status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th");
}

void TF_OperationToNodeDef(TF_Operation* oper, TF_Buffer* output_node_def,
                           TF_Status* status) {
  status->status = MessageToBuffer(oper->node.def(), output_node_def);
}

// TF_Graph functions ---------------------------------------------------------

TF_Graph::TF_Graph()
    : graph(tensorflow::OpRegistry::Global()),
      refiner(graph.versions().producer(), graph.op_registry()),
      delete_requested(false),
      parent(nullptr),
      parent_inputs(nullptr) {
  // Tell the shape refiner to also run shape inference on functions.
  refiner.set_function_library_for_shape_inference(&graph.flib_def());
}

TF_Graph* TF_NewGraph() { return new TF_Graph; }

void TF_DeleteGraph(TF_Graph* g) {
  if (g == nullptr) return;
  g->mu.lock();
  g->delete_requested = true;
  const bool del = g->sessions.empty();
  g->mu.unlock();
  if (del) delete g;
}

TF_Operation* TF_GraphOperationByName(TF_Graph* graph, const char* oper_name) {
  mutex_lock l(graph->mu);
  auto iter = graph->name_map.find(oper_name);
  if (iter == graph->name_map.end()) {
    return nullptr;
  } else {
    return ToOperation(iter->second);
  }
}

TF_Operation* TF_GraphNextOperation(TF_Graph* graph, size_t* pos) {
  if (*pos == 0) {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  } else {
    // Advance to the next node.
    *pos += 1;
  }

  mutex_lock l(graph->mu);
  while (*pos < static_cast<size_t>(graph->graph.num_node_ids())) {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }

  // No more nodes.
  return nullptr;
}

void TF_GraphToGraphDef(TF_Graph* graph, TF_Buffer* output_graph_def,
                        TF_Status* status) {
  GraphDef def;
  {
    mutex_lock l(graph->mu);
    graph->graph.ToGraphDef(&def);
  }
  status->status = MessageToBuffer(def, output_graph_def);
}

void TF_GraphGetOpDef(TF_Graph* graph, const char* op_name,
                      TF_Buffer* output_op_def, TF_Status* status) {
  const OpDef* op_def;
  {
    mutex_lock l(graph->mu);
    status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
    if (!status->status.ok()) return;
  }
  status->status = MessageToBuffer(*op_def, output_op_def);
}

void TF_GraphVersions(TF_Graph* graph, TF_Buffer* output_version_def,
                      TF_Status* status) {
  VersionDef versions;
  {
    mutex_lock l(graph->mu);
    versions = graph->graph.versions();
  }
  status->status = MessageToBuffer(versions, output_version_def);
}

TF_ImportGraphDefOptions* TF_NewImportGraphDefOptions() {
  return new TF_ImportGraphDefOptions;
}
void TF_DeleteImportGraphDefOptions(TF_ImportGraphDefOptions* opts) {
  delete opts;
}
void TF_ImportGraphDefOptionsSetPrefix(TF_ImportGraphDefOptions* opts,
                                       const char* prefix) {
  opts->opts.prefix = prefix;
}
void TF_ImportGraphDefOptionsSetDefaultDevice(TF_ImportGraphDefOptions* opts,
                                              const char* device) {
  opts->opts.default_device = device;
}

void TF_ImportGraphDefOptionsSetUniquifyNames(TF_ImportGraphDefOptions* opts,
                                              unsigned char uniquify_names) {
  opts->opts.uniquify_names = uniquify_names;
}

void TF_ImportGraphDefOptionsSetUniquifyPrefix(TF_ImportGraphDefOptions* opts,
                                               unsigned char uniquify_prefix) {
  opts->opts.uniquify_prefix = uniquify_prefix;
}

void TF_ImportGraphDefOptionsAddInputMapping(TF_ImportGraphDefOptions* opts,
                                             const char* src_name,
                                             int src_index, TF_Output dst) {
  opts->tensor_id_data.push_back(src_name);
  const string& src_name_str = opts->tensor_id_data.back();
  // We don't need to store dst's name in tensor_id_data, since `dst` must
  // outlive the ImportGraphDef call.
  opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst);
}

void TF_ImportGraphDefOptionsRemapControlDependency(
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst) {
  opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot);
}

extern void TF_ImportGraphDefOptionsAddControlDependency(
    TF_ImportGraphDefOptions* opts, TF_Operation* oper) {
  opts->opts.control_dependencies.push_back(oper->node.name());
}

void TF_ImportGraphDefOptionsAddReturnOutput(TF_ImportGraphDefOptions* opts,
                                             const char* oper_name, int index) {
  opts->tensor_id_data.push_back(oper_name);
  const string& oper_name_str = opts->tensor_id_data.back();
  opts->opts.return_tensors.emplace_back(oper_name_str, index);
}

int TF_ImportGraphDefOptionsNumReturnOutputs(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_tensors.size();
}

void TF_ImportGraphDefOptionsAddReturnOperation(TF_ImportGraphDefOptions* opts,
                                                const char* oper_name) {
  opts->opts.return_nodes.push_back(oper_name);
}

int TF_ImportGraphDefOptionsNumReturnOperations(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_nodes.size();
}

void TF_ImportGraphDefResultsReturnOutputs(TF_ImportGraphDefResults* results,
                                           int* num_outputs,
                                           TF_Output** outputs) {
  *num_outputs = results->return_tensors.size();
  *outputs = results->return_tensors.data();
}

void TF_ImportGraphDefResultsReturnOperations(TF_ImportGraphDefResults* results,
                                              int* num_opers,
                                              TF_Operation*** opers) {
  *num_opers = results->return_nodes.size();
  *opers = results->return_nodes.data();
}

void TF_ImportGraphDefResultsMissingUnusedInputMappings(
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes) {
  *num_missing_unused_input_mappings = results->missing_unused_key_names.size();
  *src_names = results->missing_unused_key_names.data();
  *src_indexes = results->missing_unused_key_indexes.data();
}

void TF_DeleteImportGraphDefResults(TF_ImportGraphDefResults* results) {
  delete results;
}

static void GraphImportGraphDefLocked(TF_Graph* graph, const GraphDef& def,
                                      const TF_ImportGraphDefOptions* opts,
                                      TF_ImportGraphDefResults* tf_results,
                                      TF_Status* status)
    TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu) {
  const int last_node_id = graph->graph.num_node_ids();
  tensorflow::ImportGraphDefResults results;
  status->status = tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results);
  if (!status->status.ok()) return;

  // Add new nodes to name_map
  for (int i = last_node_id; i < graph->graph.num_node_ids(); ++i) {
    auto* node = graph->graph.FindNodeId(i);
    if (node != nullptr) graph->name_map[node->name()] = node;
  }

  // Populate return_tensors
  DCHECK(tf_results->return_tensors.empty());
  tf_results->return_tensors.resize(results.return_tensors.size());
  for (int i = 0; i < results.return_tensors.size(); ++i) {
    tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
    tf_results->return_tensors[i].index = results.return_tensors[i].second;
  }

  // Populate return_nodes
  DCHECK(tf_results->return_nodes.empty());
  tf_results->return_nodes.resize(results.return_nodes.size());
  for (int i = 0; i < results.return_nodes.size(); ++i) {
    tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
  }

  // Populate missing unused map keys
  DCHECK(tf_results->missing_unused_key_names.empty());
  DCHECK(tf_results->missing_unused_key_indexes.empty());
  DCHECK(tf_results->missing_unused_key_names_data.empty());

  size_t size = results.missing_unused_input_map_keys.size();
  tf_results->missing_unused_key_names.resize(size);
  tf_results->missing_unused_key_indexes.resize(size);

  for (int i = 0; i < size; ++i) {
    TensorId id = results.missing_unused_input_map_keys[i];
    tf_results->missing_unused_key_names_data.emplace_back(id.first);
    tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
    tf_results->missing_unused_key_indexes[i] = id.second;
  }
}

TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResults(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}

TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResultsNoSerialization(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  const GraphDef* graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data);
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}

void TF_GraphImportGraphDefWithReturnOutputs(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status) {
  if (num_return_outputs != options->opts.return_tensors.size()) {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
  if (num_return_outputs > 0 && return_outputs == nullptr) {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  TF_ImportGraphDefResults results;
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, &results, status);
  DCHECK_EQ(results.return_tensors.size(), num_return_outputs);
  memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output));
}

void TF_GraphImportGraphDef(TF_Graph* graph, const TF_Buffer* graph_def,
                            const TF_ImportGraphDefOptions* options,
                            TF_Status* status) {
  TF_ImportGraphDefResults* results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status);
  TF_DeleteImportGraphDefResults(results);
}

// While loop functions -------------------------------------------------------

namespace {

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

// Creates a placeholder representing an input to the cond or body graph.
// TODO(skyewm): remove these from final graph
bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}

// Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
// `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
// will be prepended to copied node names. `control_deps` are nodes in
// `dst_graph` that the copied `src_graph` nodes will have control dependencies
// on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
// in `dst_graph` will be returned. `return_nodes` must be non-null.
Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}

bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}

bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}

#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void FreeWhileResources(const TF_WhileParams* params) {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}

TF_WhileParams EmptyWhileParams() {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}

}  // namespace

TF_WhileParams TF_NewWhile(TF_Graph* g, TF_Output* inputs, int ninputs,
                           TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return EmptyWhileParams();
#else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
namespace {

// TODO(skyewm): make nodes in while loop unfetchable like in Python version
void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}

}  // namespace
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void TF_FinishWhile(const TF_WhileParams* params, TF_Status* status,
                    TF_Output* outputs) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_AbortWhile(const TF_WhileParams* params) { FreeWhileResources(params); }

void TF_AddGradients(TF_Graph* g, TF_Output* y, int ny, TF_Output* x, int nx,
                     TF_Output* dx, TF_Status* status, TF_Output* dy) {
  TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy);
}

void TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix, TF_Output* y,
                               int ny, TF_Output* x, int nx, TF_Output* dx,
                               TF_Status* status, TF_Output* dy) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

// TF_Session functions ----------------------------------------------

TF_Session::TF_Session(tensorflow::Session* s, TF_Graph* g)
    : session(s), graph(g), last_num_graph_nodes(0), extend_before_run(true) {}

TF_Session* TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  } else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}

TF_Session* TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status) {
// TODO(sjr): Remove the IS_MOBILE_PLATFORM guard. This will require ensuring
// that the tensorflow/cc/saved_model:loader build target is mobile friendly.
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return nullptr;
#else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_CloseSession(TF_Session* s, TF_Status* status) {
  status->status = s->session->Close();
}

void TF_DeleteSession(TF_Session* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  TF_Graph* const graph = s->graph;
  if (graph != nullptr) {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
  delete s->session;
  delete s;
}

void TF_SessionRun(TF_Session* session, const TF_Buffer* run_options,
                   const TF_Output* inputs, TF_Tensor* const* input_values,
                   int ninputs, const TF_Output* outputs,
                   TF_Tensor** output_values, int noutputs,
                   const TF_Operation* const* target_opers, int ntargets,
                   TF_Buffer* run_metadata, TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  // Actually run.
  TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status);
}

void TF_SessionPRunSetup(TF_Session* session, const TF_Output* inputs,
                         int ninputs, const TF_Output* outputs, int noutputs,
                         const TF_Operation* const* target_opers, int ntargets,
                         const char** handle, TF_Status* status) {
  *handle = nullptr;

  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  std::vector<string> input_names(ninputs);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = OutputName(inputs[i]);
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  string new_handle;
  status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}

void TF_DeletePRunHandle(const char* handle) {
  delete[] handle;
  // TODO(suharshs): Free up any resources held by the partial run state.
}

void TF_SessionPRun(TF_Session* session, const char* handle,
                    const TF_Output* inputs, TF_Tensor* const* input_values,
                    int ninputs, const TF_Output* outputs,
                    TF_Tensor** output_values, int noutputs,
                    const TF_Operation* const* target_opers, int ntargets,
                    TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status);
}

unsigned char TF_TryEvaluateConstant(TF_Graph* graph, TF_Output output,
                                     TF_Tensor** result, TF_Status* status) {
  mutex_lock l(graph->mu);
  auto status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      });
  if (!status_or.ok() || !status_or->has_value()) {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
  *result = TF_TensorFromTensor(**status_or, &status->status);
  return status->status.ok();
}

TF_ApiDefMap* TF_NewApiDefMap(TF_Buffer* op_list_buffer, TF_Status* status) {
  tensorflow::OpList op_list;
  if (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)) {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
  status->status = absl::OkStatus();
  return new TF_ApiDefMap(op_list);
}

void TF_DeleteApiDefMap(TF_ApiDefMap* apimap) { delete apimap; }

void TF_ApiDefMapPut(TF_ApiDefMap* api_def_map, const char* text,
                     size_t text_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
#else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

TF_Buffer* TF_ApiDefMapGet(TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
  return nullptr;
#else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

TF_Buffer* TF_GetAllRegisteredKernels(TF_Status* status) {
  tensorflow::KernelList kernel_list = tensorflow::GetAllRegisteredKernels();
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}

TF_Buffer* TF_GetRegisteredKernelsForOp(const char* name, TF_Status* status) {
  tensorflow::KernelList kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name);
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}

void TF_UpdateEdge(TF_Graph* graph, TF_Output new_src, TF_Input dst,
                   TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&new_src.oper->node);

  if (ic->num_outputs() <= new_src.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
  tensorflow::shape_inference::ShapeHandle shape = ic->output(new_src.index);

  tensorflow::shape_inference::InferenceContext* ic_dst =
      graph->refiner.GetContext(&dst.oper->node);
  if (ic_dst->num_inputs() <= dst.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
  if (!ic_dst->MergeInput(dst.index, shape)) {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
  status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index);

  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
}

// Apis that are corresponding to python c api. --------------------------

void TF_AddOperationControlInput(TF_Graph* graph, TF_Operation* op,
                                 TF_Operation* input) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  graph->graph.AddControlEdge(&input->node, &op->node);
  RecordMutation(graph, *op, "adding control input");
}

void TF_SetAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                TF_Buffer* attr_value_proto, TF_Status* status) {
  using tensorflow::RecordMutation;
  tensorflow::AttrValue attr_val;
  if (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)) {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }

  mutex_lock l(graph->mu);
  op->node.AddAttr(attr_name, attr_val);
  RecordMutation(graph, *op, "setting attribute");
}

void TF_ClearAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                  TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.ClearAttr(attr_name);
  RecordMutation(graph, *op, "clearing attribute");
}

void TF_SetFullType(TF_Graph* graph, TF_Operation* op,
                    const TF_Buffer* full_type_proto) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  FullTypeDef full_type;
  full_type.ParseFromArray(full_type_proto->data, full_type_proto->length);
  *op->node.mutable_def()->mutable_experimental_type() = full_type;
  RecordMutation(graph, *op, "setting fulltype");
}

void TF_SetRequestedDevice(TF_Graph* graph, TF_Operation* op,
                           const char* device) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.set_requested_device(device);
  RecordMutation(graph, *op, "setting device");
}

void TF_RemoveAllControlInputs(TF_Graph* graph, TF_Operation* op) {
  mutex_lock l(graph->mu);
  std::vector<const tensorflow::Edge*> control_edges;
  for (const tensorflow::Edge* edge : op->node.in_edges()) {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
  for (const tensorflow::Edge* edge : control_edges) {
    graph->graph.RemoveControlEdge(edge);
  }
}

void TF_SetRequireShapeInferenceFns(TF_Graph* graph, bool require) {
  mutex_lock l(graph->mu);
  graph->refiner.set_require_shape_inference_fns(require);
}

void TF_ExtendSession(TF_Session* session, TF_Status* status) {
  ExtendSessionGraphHelper(session, status);
  session->extend_before_run = false;
}

TF_Buffer* TF_GetHandleShapeAndType(TF_Graph* graph, TF_Output output) {
  Node* node = &output.oper->node;
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  handle_data.set_is_set(true);
  {
    mutex_lock l(graph->mu);
    tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
    CHECK(ic != nullptr);                       // Crash OK
    CHECK_LT(output.index, ic->num_outputs());  // Crash OK
    const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
    if (shapes_and_types == nullptr) return nullptr;

    for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
  }
  string str_data;
  handle_data.SerializeToString(&str_data);

  TF_Buffer* result = TF_NewBufferFromString(str_data.c_str(), str_data.size());
  return result;
}

void TF_SetHandleShapeAndType(TF_Graph* graph, TF_Output output,
                              const void* proto, size_t proto_len,
                              TF_Status* status) {
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  if (!handle_data.ParseFromArray(proto, proto_len)) {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
  DCHECK(handle_data.is_set());

  tensorflow::mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&output.oper->node);

  std::vector<tensorflow::shape_inference::ShapeAndType> shapes_and_types;
  for (const auto& shape_and_type_proto : handle_data.shape_and_type()) {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
  ic->set_output_handle_shapes_and_types(output.index, shapes_and_types);
}

void TF_AddWhileInputHack(TF_Graph* graph, TF_Output new_src, TF_Operation* dst,
                          TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node);
  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
}

// -------------------------------------------------------------------

// TF_Server functions ----------------------------------------------

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
TF_Server::TF_Server(std::unique_ptr<tensorflow::ServerInterface> server)
    : target(server->target()), server(std::move(server)) {}
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

TF_Server* TF_NewServer(const void* proto, size_t proto_len,
                        TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
  return nullptr;
#else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerStart(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Start();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerStop(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Stop();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerJoin(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Join();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

const char* TF_ServerTarget(TF_Server* server) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  return nullptr;
#else
  return server->target.c_str();
#endif
}

void TF_DeleteServer(TF_Server* server) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  delete server;
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}

void TF_RegisterLogListener(void (*listener)(const char*)) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  tensorflow::logging::RegisterListener(listener);
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}

void TF_RegisterFilesystemPlugin(const char* plugin_filename,
                                 TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
#else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

}
  extern: extern
  string_literal: "C"
   ": "
   string_content: C
   ": "
  declaration_list: {

TF_OperationDescription* TF_NewOperationLocked(TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
    TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu) {
  return new TF_OperationDescription(graph, op_type, oper_name);
}

TF_OperationDescription* TF_NewOperation(TF_Graph* graph, const char* op_type,
                                         const char* oper_name) {
  mutex_lock l(graph->mu);
  return TF_NewOperationLocked(graph, op_type, oper_name);
}

void TF_SetDevice(TF_OperationDescription* desc, const char* device) {
  desc->node_builder.Device(device);
}

void TF_AddInput(TF_OperationDescription* desc, TF_Output input) {
  desc->node_builder.Input(&input.oper->node, input.index);
}

void TF_AddInputList(TF_OperationDescription* desc, const TF_Output* inputs,
                     int num_inputs) {
  std::vector<NodeBuilder::NodeOut> input_list;
  input_list.reserve(num_inputs);
  for (int i = 0; i < num_inputs; ++i) {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
  desc->node_builder.Input(input_list);
}

void TF_AddControlInput(TF_OperationDescription* desc, TF_Operation* input) {
  desc->node_builder.ControlInput(&input->node);
}

void TF_ColocateWith(TF_OperationDescription* desc, TF_Operation* op) {
  desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()));
}

void TF_SetAttrString(TF_OperationDescription* desc, const char* attr_name,
                      const void* value, size_t length) {
  absl::string_view s(static_cast<const char*>(value), length);
  desc->node_builder.Attr(attr_name, s);
}

void TF_SetAttrStringList(TF_OperationDescription* desc, const char* attr_name,
                          const void* const* values, const size_t* lengths,
                          int num_values) {
  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  } else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
}

void TF_SetAttrInt(TF_OperationDescription* desc, const char* attr_name,
                   int64_t value) {
  desc->node_builder.Attr(attr_name, static_cast<int64_t>(value));
}

void TF_SetAttrIntList(TF_OperationDescription* desc, const char* attr_name,
                       const int64_t* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values));
}

void TF_SetAttrFloat(TF_OperationDescription* desc, const char* attr_name,
                     float value) {
  desc->node_builder.Attr(attr_name, value);
}

void TF_SetAttrFloatList(TF_OperationDescription* desc, const char* attr_name,
                         const float* values, int num_values) {
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values));
}

void TF_SetAttrBool(TF_OperationDescription* desc, const char* attr_name,
                    unsigned char value) {
  desc->node_builder.Attr(attr_name, static_cast<bool>(value));
}

void TF_SetAttrBoolList(TF_OperationDescription* desc, const char* attr_name,
                        const unsigned char* values, int num_values) {
  std::unique_ptr<bool[]> b(new bool[num_values]);
  for (int i = 0; i < num_values; ++i) {
    b[i] = values[i];
  }
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values));
}

void TF_SetAttrType(TF_OperationDescription* desc, const char* attr_name,
                    TF_DataType value) {
  desc->node_builder.Attr(attr_name, static_cast<DataType>(value));
}

void TF_SetAttrTypeList(TF_OperationDescription* desc, const char* attr_name,
                        const TF_DataType* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values));
}

void TF_SetAttrPlaceholder(TF_OperationDescription* desc, const char* attr_name,
                           const char* placeholder) {
  tensorflow::AttrValue attr_value;
  attr_value.set_placeholder(placeholder);
  desc->node_builder.Attr(attr_name, attr_value);
}

void TF_SetAttrFuncName(TF_OperationDescription* desc, const char* attr_name,
                        const char* value, size_t length) {
  tensorflow::NameAttrList func_name;
  func_name.set_name(string(value, value + length));
  desc->node_builder.Attr(attr_name, func_name);
}

void TF_SetAttrShape(TF_OperationDescription* desc, const char* attr_name,
                     const int64_t* dims, int num_dims) {
  PartialTensorShape shape;
  if (num_dims >= 0) {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
  desc->node_builder.Attr(attr_name, shape);
}

void TF_SetAttrShapeList(TF_OperationDescription* desc, const char* attr_name,
                         const int64_t* const* dims, const int* num_dims,
                         int num_shapes) {
  std::vector<PartialTensorShape> shapes;
  shapes.reserve(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
}

void TF_SetAttrTensorShapeProto(TF_OperationDescription* desc,
                                const char* attr_name, const void* proto,
                                size_t proto_len, TF_Status* status) {
  // shape.ParseFromArray takes an int as length, this function takes size_t,
  // make sure there is no information loss.
  if (proto_len > std::numeric_limits<int>::max()) {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
  TensorShapeProto shape;
  if (shape.ParseFromArray(proto, static_cast<int>(proto_len))) {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  } else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
}

void TF_SetAttrTensorShapeProtoList(TF_OperationDescription* desc,
                                    const char* attr_name,
                                    const void* const* protos,
                                    const size_t* proto_lens, int num_shapes,
                                    TF_Status* status) {
  std::vector<TensorShapeProto> shapes;
  shapes.resize(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
  status->status = absl::OkStatus();
}

void TF_SetAttrTensor(TF_OperationDescription* desc, const char* attr_name,
                      TF_Tensor* value, TF_Status* status) {
  Tensor t;
  status->status = TF_TensorToTensor(value, &t);
  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}

void TF_SetAttrTensorList(TF_OperationDescription* desc, const char* attr_name,
                          TF_Tensor* const* values, int num_values,
                          TF_Status* status) {
  status->status = absl::OkStatus();
  std::vector<Tensor> t;
  t.reserve(num_values);

  for (int i = 0; i < num_values && status->status.ok(); ++i) {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }

  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}

void TF_SetAttrValueProto(TF_OperationDescription* desc, const char* attr_name,
                          const void* proto, size_t proto_len,
                          TF_Status* status) {
  tensorflow::AttrValue attr_value;
  if (!attr_value.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }

  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  } else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }

  status->status = absl::OkStatus();
}

TF_Operation* TF_FinishOperationLocked(TF_OperationDescription* desc,
                                       TF_Status* status)
    TF_EXCLUSIVE_LOCKS_REQUIRED(desc->graph->mu) {
  Node* ret = nullptr;

  if (desc->graph->name_map.count(desc->node_builder.node_name())) {
    status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
  } else {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }

  delete desc;

  return ToOperation(ret);
}

TF_Operation* TF_FinishOperation(TF_OperationDescription* desc,
                                 TF_Status* status) {
  mutex_lock l(desc->graph->mu);
  return TF_FinishOperationLocked(desc, status);
}

// TF_Operation functions
// ----------------------------------------------------------

const char* TF_OperationName(TF_Operation* oper) {
  return oper->node.name().c_str();
}

const char* TF_OperationOpType(TF_Operation* oper) {
  return oper->node.type_string().c_str();
}

const char* TF_OperationDevice(TF_Operation* oper) {
  return oper->node.requested_device().c_str();
}

int TF_OperationNumOutputs(TF_Operation* oper) {
  return oper->node.num_outputs();
}

TF_DataType TF_OperationOutputType(TF_Output oper_out) {
  return static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index));
}

int TF_OperationOutputListLength(TF_Operation* oper, const char* arg_name,
                                 TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}

int TF_OperationNumInputs(TF_Operation* oper) {
  return oper->node.num_inputs();
}

TF_DataType TF_OperationInputType(TF_Input oper_in) {
  return static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index));
}

int TF_OperationInputListLength(TF_Operation* oper, const char* arg_name,
                                TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}

TF_Output TF_OperationInput(TF_Input oper_in) {
  const tensorflow::Edge* edge;
  Status s = oper_in.oper->node.input_edge(oper_in.index, &edge);
  if (!s.ok()) {
    return {nullptr, -1};
  }

  return {ToOperation(edge->src()), edge->src_output()};
}

void TF_OperationAllInputs(TF_Operation* oper, TF_Output* inputs,
                           int max_inputs) {
  for (auto* edge : oper->node.in_edges()) {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
}

int TF_OperationOutputNumConsumers(TF_Output oper_out) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
  return count;
}

int TF_OperationOutputConsumers(TF_Output oper_out, TF_Input* consumers,
                                int max_consumers) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
  return count;
}

int TF_OperationNumControlInputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
  return count;
}

int TF_OperationGetControlInputs(TF_Operation* oper,
                                 TF_Operation** control_inputs,
                                 int max_control_inputs) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
  return count;
}

int TF_OperationNumControlOutputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
  return count;
}

int TF_OperationGetControlOutputs(TF_Operation* oper,
                                  TF_Operation** control_outputs,
                                  int max_control_outputs) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
  return count;
}

TF_AttrMetadata TF_OperationGetAttrMetadata(TF_Operation* oper,
                                            const char* attr_name,
                                            TF_Status* status) {
  TF_AttrMetadata metadata;
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return metadata;
  switch (attr->value_case()) {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
  return metadata;
}

void TF_OperationGetAttrString(TF_Operation* oper, const char* attr_name,
                               void* value, size_t max_length,
                               TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kS) {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
  if (max_length <= 0) {
    return;
  }
  const auto& s = attr->s();
  std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length));
}

void TF_OperationGetAttrStringList(TF_Operation* oper, const char* attr_name,
                                   void** values, size_t* lengths,
                                   int max_values, void* storage,
                                   size_t storage_size, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().s_size());
  char* p = static_cast<char*>(storage);
  for (int i = 0; i < len; ++i) {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
}

#define DEFINE_GETATTR(func, c_type, cpp_type, list_field)                   \
  void func(TF_Operation* oper, const char* attr_name, c_type* value,        \
            TF_Status* status) {                                             \
    cpp_type v;                                                              \
    status->status =                                                         \
        tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &v);          \
    if (!status->status.ok()) return;                                        \
    *value = static_cast<c_type>(v);                                         \
  }                                                                          \
  void func##List(TF_Operation* oper, const char* attr_name, c_type* values, \
                  int max_values, TF_Status* status) {                       \
    const auto* attr = GetAttrValue(oper, attr_name, status);                \
    if (!status->status.ok()) return;                                        \
    if (attr->value_case() != tensorflow::AttrValue::kList) {                \
      status->status =                                                       \
          InvalidArgument("Value for '", attr_name, "' is not a list.");     \
      return;                                                                \
    }                                                                        \
    const auto len = std::min(max_values, attr->list().list_field##_size()); \
    for (int i = 0; i < len; ++i) {                                          \
      values[i] = static_cast<c_type>(attr->list().list_field(i));           \
    }                                                                        \
  }
DEFINE_GETATTR(TF_OperationGetAttrInt, int64_t, int64_t, i);
DEFINE_GETATTR(TF_OperationGetAttrFloat, float, float, f);
DEFINE_GETATTR(TF_OperationGetAttrBool, unsigned char, bool, b);
DEFINE_GETATTR(TF_OperationGetAttrType, TF_DataType, DataType, type);
#undef DEFINE_GETATTR

void TF_OperationGetAttrShape(TF_Operation* oper, const char* attr_name,
                              int64_t* value, int num_dims, TF_Status* status) {
  PartialTensorShape shape;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape);
  if (!status->status.ok()) return;
  auto len = std::min(shape.dims(), num_dims);
  for (int i = 0; i < len; ++i) {
    value[i] = shape.dim_size(i);
  }
}

void TF_OperationGetAttrShapeList(TF_Operation* oper, const char* attr_name,
                                  int64_t** dims, int* num_dims, int num_shapes,
                                  int64_t* storage, int storage_size,
                                  TF_Status* status) {
  std::vector<PartialTensorShape> shapes;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes);
  if (!status->status.ok()) return;
  auto len = std::min(static_cast<int>(shapes.size()), num_shapes);
  int64_t* p = storage;
  int storage_left = storage_size;
  for (int i = 0; i < len; ++i) {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
}

void TF_OperationGetAttrTensorShapeProto(TF_Operation* oper,
                                         const char* attr_name,
                                         TF_Buffer* value, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kShape) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
  status->status = MessageToBuffer(attr->shape(), value);
}

void TF_OperationGetAttrTensorShapeProtoList(TF_Operation* oper,
                                             const char* attr_name,
                                             TF_Buffer** values, int max_values,
                                             TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().shape_size());
  for (int i = 0; i < len; ++i) {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
}

void TF_OperationGetAttrTensor(TF_Operation* oper, const char* attr_name,
                               TF_Tensor** value, TF_Status* status) {
  *value = nullptr;
  Tensor t;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t);
  if (!status->status.ok()) return;
  *value = TF_TensorFromTensor(t, &status->status);
}

void TF_OperationGetAttrTensorList(TF_Operation* oper, const char* attr_name,
                                   TF_Tensor** values, int max_values,
                                   TF_Status* status) {
  std::vector<Tensor> ts;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts);
  if (!status->status.ok()) return;
  const auto len = std::min(max_values, static_cast<int>(ts.size()));
  for (int i = 0; i < len; ++i) {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
}

void TF_OperationGetAttrValueProto(TF_Operation* oper, const char* attr_name,
                                   TF_Buffer* output_attr_value,
                                   TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  status->status = MessageToBuffer(*attr, output_attr_value);
}

int TF_OperationGetNumAttrs(TF_Operation* oper) {
  return oper->node.attrs().size();
}

int TF_OperationGetAttrNameLength(TF_Operation* oper, int i) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
  return -1;
}

void TF_OperationGetAttrName(TF_Operation* oper, int i, char* output,
                             TF_Status* status) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
  status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th");
}

void TF_OperationToNodeDef(TF_Operation* oper, TF_Buffer* output_node_def,
                           TF_Status* status) {
  status->status = MessageToBuffer(oper->node.def(), output_node_def);
}

// TF_Graph functions ---------------------------------------------------------

TF_Graph::TF_Graph()
    : graph(tensorflow::OpRegistry::Global()),
      refiner(graph.versions().producer(), graph.op_registry()),
      delete_requested(false),
      parent(nullptr),
      parent_inputs(nullptr) {
  // Tell the shape refiner to also run shape inference on functions.
  refiner.set_function_library_for_shape_inference(&graph.flib_def());
}

TF_Graph* TF_NewGraph() { return new TF_Graph; }

void TF_DeleteGraph(TF_Graph* g) {
  if (g == nullptr) return;
  g->mu.lock();
  g->delete_requested = true;
  const bool del = g->sessions.empty();
  g->mu.unlock();
  if (del) delete g;
}

TF_Operation* TF_GraphOperationByName(TF_Graph* graph, const char* oper_name) {
  mutex_lock l(graph->mu);
  auto iter = graph->name_map.find(oper_name);
  if (iter == graph->name_map.end()) {
    return nullptr;
  } else {
    return ToOperation(iter->second);
  }
}

TF_Operation* TF_GraphNextOperation(TF_Graph* graph, size_t* pos) {
  if (*pos == 0) {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  } else {
    // Advance to the next node.
    *pos += 1;
  }

  mutex_lock l(graph->mu);
  while (*pos < static_cast<size_t>(graph->graph.num_node_ids())) {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }

  // No more nodes.
  return nullptr;
}

void TF_GraphToGraphDef(TF_Graph* graph, TF_Buffer* output_graph_def,
                        TF_Status* status) {
  GraphDef def;
  {
    mutex_lock l(graph->mu);
    graph->graph.ToGraphDef(&def);
  }
  status->status = MessageToBuffer(def, output_graph_def);
}

void TF_GraphGetOpDef(TF_Graph* graph, const char* op_name,
                      TF_Buffer* output_op_def, TF_Status* status) {
  const OpDef* op_def;
  {
    mutex_lock l(graph->mu);
    status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
    if (!status->status.ok()) return;
  }
  status->status = MessageToBuffer(*op_def, output_op_def);
}

void TF_GraphVersions(TF_Graph* graph, TF_Buffer* output_version_def,
                      TF_Status* status) {
  VersionDef versions;
  {
    mutex_lock l(graph->mu);
    versions = graph->graph.versions();
  }
  status->status = MessageToBuffer(versions, output_version_def);
}

TF_ImportGraphDefOptions* TF_NewImportGraphDefOptions() {
  return new TF_ImportGraphDefOptions;
}
void TF_DeleteImportGraphDefOptions(TF_ImportGraphDefOptions* opts) {
  delete opts;
}
void TF_ImportGraphDefOptionsSetPrefix(TF_ImportGraphDefOptions* opts,
                                       const char* prefix) {
  opts->opts.prefix = prefix;
}
void TF_ImportGraphDefOptionsSetDefaultDevice(TF_ImportGraphDefOptions* opts,
                                              const char* device) {
  opts->opts.default_device = device;
}

void TF_ImportGraphDefOptionsSetUniquifyNames(TF_ImportGraphDefOptions* opts,
                                              unsigned char uniquify_names) {
  opts->opts.uniquify_names = uniquify_names;
}

void TF_ImportGraphDefOptionsSetUniquifyPrefix(TF_ImportGraphDefOptions* opts,
                                               unsigned char uniquify_prefix) {
  opts->opts.uniquify_prefix = uniquify_prefix;
}

void TF_ImportGraphDefOptionsAddInputMapping(TF_ImportGraphDefOptions* opts,
                                             const char* src_name,
                                             int src_index, TF_Output dst) {
  opts->tensor_id_data.push_back(src_name);
  const string& src_name_str = opts->tensor_id_data.back();
  // We don't need to store dst's name in tensor_id_data, since `dst` must
  // outlive the ImportGraphDef call.
  opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst);
}

void TF_ImportGraphDefOptionsRemapControlDependency(
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst) {
  opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot);
}

extern void TF_ImportGraphDefOptionsAddControlDependency(
    TF_ImportGraphDefOptions* opts, TF_Operation* oper) {
  opts->opts.control_dependencies.push_back(oper->node.name());
}

void TF_ImportGraphDefOptionsAddReturnOutput(TF_ImportGraphDefOptions* opts,
                                             const char* oper_name, int index) {
  opts->tensor_id_data.push_back(oper_name);
  const string& oper_name_str = opts->tensor_id_data.back();
  opts->opts.return_tensors.emplace_back(oper_name_str, index);
}

int TF_ImportGraphDefOptionsNumReturnOutputs(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_tensors.size();
}

void TF_ImportGraphDefOptionsAddReturnOperation(TF_ImportGraphDefOptions* opts,
                                                const char* oper_name) {
  opts->opts.return_nodes.push_back(oper_name);
}

int TF_ImportGraphDefOptionsNumReturnOperations(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_nodes.size();
}

void TF_ImportGraphDefResultsReturnOutputs(TF_ImportGraphDefResults* results,
                                           int* num_outputs,
                                           TF_Output** outputs) {
  *num_outputs = results->return_tensors.size();
  *outputs = results->return_tensors.data();
}

void TF_ImportGraphDefResultsReturnOperations(TF_ImportGraphDefResults* results,
                                              int* num_opers,
                                              TF_Operation*** opers) {
  *num_opers = results->return_nodes.size();
  *opers = results->return_nodes.data();
}

void TF_ImportGraphDefResultsMissingUnusedInputMappings(
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes) {
  *num_missing_unused_input_mappings = results->missing_unused_key_names.size();
  *src_names = results->missing_unused_key_names.data();
  *src_indexes = results->missing_unused_key_indexes.data();
}

void TF_DeleteImportGraphDefResults(TF_ImportGraphDefResults* results) {
  delete results;
}

static void GraphImportGraphDefLocked(TF_Graph* graph, const GraphDef& def,
                                      const TF_ImportGraphDefOptions* opts,
                                      TF_ImportGraphDefResults* tf_results,
                                      TF_Status* status)
    TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu) {
  const int last_node_id = graph->graph.num_node_ids();
  tensorflow::ImportGraphDefResults results;
  status->status = tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results);
  if (!status->status.ok()) return;

  // Add new nodes to name_map
  for (int i = last_node_id; i < graph->graph.num_node_ids(); ++i) {
    auto* node = graph->graph.FindNodeId(i);
    if (node != nullptr) graph->name_map[node->name()] = node;
  }

  // Populate return_tensors
  DCHECK(tf_results->return_tensors.empty());
  tf_results->return_tensors.resize(results.return_tensors.size());
  for (int i = 0; i < results.return_tensors.size(); ++i) {
    tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
    tf_results->return_tensors[i].index = results.return_tensors[i].second;
  }

  // Populate return_nodes
  DCHECK(tf_results->return_nodes.empty());
  tf_results->return_nodes.resize(results.return_nodes.size());
  for (int i = 0; i < results.return_nodes.size(); ++i) {
    tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
  }

  // Populate missing unused map keys
  DCHECK(tf_results->missing_unused_key_names.empty());
  DCHECK(tf_results->missing_unused_key_indexes.empty());
  DCHECK(tf_results->missing_unused_key_names_data.empty());

  size_t size = results.missing_unused_input_map_keys.size();
  tf_results->missing_unused_key_names.resize(size);
  tf_results->missing_unused_key_indexes.resize(size);

  for (int i = 0; i < size; ++i) {
    TensorId id = results.missing_unused_input_map_keys[i];
    tf_results->missing_unused_key_names_data.emplace_back(id.first);
    tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
    tf_results->missing_unused_key_indexes[i] = id.second;
  }
}

TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResults(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}

TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResultsNoSerialization(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  const GraphDef* graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data);
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}

void TF_GraphImportGraphDefWithReturnOutputs(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status) {
  if (num_return_outputs != options->opts.return_tensors.size()) {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
  if (num_return_outputs > 0 && return_outputs == nullptr) {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  TF_ImportGraphDefResults results;
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, &results, status);
  DCHECK_EQ(results.return_tensors.size(), num_return_outputs);
  memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output));
}

void TF_GraphImportGraphDef(TF_Graph* graph, const TF_Buffer* graph_def,
                            const TF_ImportGraphDefOptions* options,
                            TF_Status* status) {
  TF_ImportGraphDefResults* results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status);
  TF_DeleteImportGraphDefResults(results);
}

// While loop functions -------------------------------------------------------

namespace {

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

// Creates a placeholder representing an input to the cond or body graph.
// TODO(skyewm): remove these from final graph
bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}

// Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
// `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
// will be prepended to copied node names. `control_deps` are nodes in
// `dst_graph` that the copied `src_graph` nodes will have control dependencies
// on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
// in `dst_graph` will be returned. `return_nodes` must be non-null.
Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}

bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}

bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}

#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void FreeWhileResources(const TF_WhileParams* params) {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}

TF_WhileParams EmptyWhileParams() {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}

}  // namespace

TF_WhileParams TF_NewWhile(TF_Graph* g, TF_Output* inputs, int ninputs,
                           TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return EmptyWhileParams();
#else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
namespace {

// TODO(skyewm): make nodes in while loop unfetchable like in Python version
void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}

}  // namespace
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void TF_FinishWhile(const TF_WhileParams* params, TF_Status* status,
                    TF_Output* outputs) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_AbortWhile(const TF_WhileParams* params) { FreeWhileResources(params); }

void TF_AddGradients(TF_Graph* g, TF_Output* y, int ny, TF_Output* x, int nx,
                     TF_Output* dx, TF_Status* status, TF_Output* dy) {
  TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy);
}

void TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix, TF_Output* y,
                               int ny, TF_Output* x, int nx, TF_Output* dx,
                               TF_Status* status, TF_Output* dy) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

// TF_Session functions ----------------------------------------------

TF_Session::TF_Session(tensorflow::Session* s, TF_Graph* g)
    : session(s), graph(g), last_num_graph_nodes(0), extend_before_run(true) {}

TF_Session* TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  } else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}

TF_Session* TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status) {
// TODO(sjr): Remove the IS_MOBILE_PLATFORM guard. This will require ensuring
// that the tensorflow/cc/saved_model:loader build target is mobile friendly.
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return nullptr;
#else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_CloseSession(TF_Session* s, TF_Status* status) {
  status->status = s->session->Close();
}

void TF_DeleteSession(TF_Session* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  TF_Graph* const graph = s->graph;
  if (graph != nullptr) {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
  delete s->session;
  delete s;
}

void TF_SessionRun(TF_Session* session, const TF_Buffer* run_options,
                   const TF_Output* inputs, TF_Tensor* const* input_values,
                   int ninputs, const TF_Output* outputs,
                   TF_Tensor** output_values, int noutputs,
                   const TF_Operation* const* target_opers, int ntargets,
                   TF_Buffer* run_metadata, TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  // Actually run.
  TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status);
}

void TF_SessionPRunSetup(TF_Session* session, const TF_Output* inputs,
                         int ninputs, const TF_Output* outputs, int noutputs,
                         const TF_Operation* const* target_opers, int ntargets,
                         const char** handle, TF_Status* status) {
  *handle = nullptr;

  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  std::vector<string> input_names(ninputs);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = OutputName(inputs[i]);
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  string new_handle;
  status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}

void TF_DeletePRunHandle(const char* handle) {
  delete[] handle;
  // TODO(suharshs): Free up any resources held by the partial run state.
}

void TF_SessionPRun(TF_Session* session, const char* handle,
                    const TF_Output* inputs, TF_Tensor* const* input_values,
                    int ninputs, const TF_Output* outputs,
                    TF_Tensor** output_values, int noutputs,
                    const TF_Operation* const* target_opers, int ntargets,
                    TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status);
}

unsigned char TF_TryEvaluateConstant(TF_Graph* graph, TF_Output output,
                                     TF_Tensor** result, TF_Status* status) {
  mutex_lock l(graph->mu);
  auto status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      });
  if (!status_or.ok() || !status_or->has_value()) {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
  *result = TF_TensorFromTensor(**status_or, &status->status);
  return status->status.ok();
}

TF_ApiDefMap* TF_NewApiDefMap(TF_Buffer* op_list_buffer, TF_Status* status) {
  tensorflow::OpList op_list;
  if (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)) {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
  status->status = absl::OkStatus();
  return new TF_ApiDefMap(op_list);
}

void TF_DeleteApiDefMap(TF_ApiDefMap* apimap) { delete apimap; }

void TF_ApiDefMapPut(TF_ApiDefMap* api_def_map, const char* text,
                     size_t text_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
#else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

TF_Buffer* TF_ApiDefMapGet(TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
  return nullptr;
#else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

TF_Buffer* TF_GetAllRegisteredKernels(TF_Status* status) {
  tensorflow::KernelList kernel_list = tensorflow::GetAllRegisteredKernels();
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}

TF_Buffer* TF_GetRegisteredKernelsForOp(const char* name, TF_Status* status) {
  tensorflow::KernelList kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name);
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}

void TF_UpdateEdge(TF_Graph* graph, TF_Output new_src, TF_Input dst,
                   TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&new_src.oper->node);

  if (ic->num_outputs() <= new_src.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
  tensorflow::shape_inference::ShapeHandle shape = ic->output(new_src.index);

  tensorflow::shape_inference::InferenceContext* ic_dst =
      graph->refiner.GetContext(&dst.oper->node);
  if (ic_dst->num_inputs() <= dst.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
  if (!ic_dst->MergeInput(dst.index, shape)) {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
  status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index);

  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
}

// Apis that are corresponding to python c api. --------------------------

void TF_AddOperationControlInput(TF_Graph* graph, TF_Operation* op,
                                 TF_Operation* input) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  graph->graph.AddControlEdge(&input->node, &op->node);
  RecordMutation(graph, *op, "adding control input");
}

void TF_SetAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                TF_Buffer* attr_value_proto, TF_Status* status) {
  using tensorflow::RecordMutation;
  tensorflow::AttrValue attr_val;
  if (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)) {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }

  mutex_lock l(graph->mu);
  op->node.AddAttr(attr_name, attr_val);
  RecordMutation(graph, *op, "setting attribute");
}

void TF_ClearAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                  TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.ClearAttr(attr_name);
  RecordMutation(graph, *op, "clearing attribute");
}

void TF_SetFullType(TF_Graph* graph, TF_Operation* op,
                    const TF_Buffer* full_type_proto) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  FullTypeDef full_type;
  full_type.ParseFromArray(full_type_proto->data, full_type_proto->length);
  *op->node.mutable_def()->mutable_experimental_type() = full_type;
  RecordMutation(graph, *op, "setting fulltype");
}

void TF_SetRequestedDevice(TF_Graph* graph, TF_Operation* op,
                           const char* device) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.set_requested_device(device);
  RecordMutation(graph, *op, "setting device");
}

void TF_RemoveAllControlInputs(TF_Graph* graph, TF_Operation* op) {
  mutex_lock l(graph->mu);
  std::vector<const tensorflow::Edge*> control_edges;
  for (const tensorflow::Edge* edge : op->node.in_edges()) {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
  for (const tensorflow::Edge* edge : control_edges) {
    graph->graph.RemoveControlEdge(edge);
  }
}

void TF_SetRequireShapeInferenceFns(TF_Graph* graph, bool require) {
  mutex_lock l(graph->mu);
  graph->refiner.set_require_shape_inference_fns(require);
}

void TF_ExtendSession(TF_Session* session, TF_Status* status) {
  ExtendSessionGraphHelper(session, status);
  session->extend_before_run = false;
}

TF_Buffer* TF_GetHandleShapeAndType(TF_Graph* graph, TF_Output output) {
  Node* node = &output.oper->node;
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  handle_data.set_is_set(true);
  {
    mutex_lock l(graph->mu);
    tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
    CHECK(ic != nullptr);                       // Crash OK
    CHECK_LT(output.index, ic->num_outputs());  // Crash OK
    const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
    if (shapes_and_types == nullptr) return nullptr;

    for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
  }
  string str_data;
  handle_data.SerializeToString(&str_data);

  TF_Buffer* result = TF_NewBufferFromString(str_data.c_str(), str_data.size());
  return result;
}

void TF_SetHandleShapeAndType(TF_Graph* graph, TF_Output output,
                              const void* proto, size_t proto_len,
                              TF_Status* status) {
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  if (!handle_data.ParseFromArray(proto, proto_len)) {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
  DCHECK(handle_data.is_set());

  tensorflow::mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&output.oper->node);

  std::vector<tensorflow::shape_inference::ShapeAndType> shapes_and_types;
  for (const auto& shape_and_type_proto : handle_data.shape_and_type()) {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
  ic->set_output_handle_shapes_and_types(output.index, shapes_and_types);
}

void TF_AddWhileInputHack(TF_Graph* graph, TF_Output new_src, TF_Operation* dst,
                          TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node);
  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
}

// -------------------------------------------------------------------

// TF_Server functions ----------------------------------------------

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
TF_Server::TF_Server(std::unique_ptr<tensorflow::ServerInterface> server)
    : target(server->target()), server(std::move(server)) {}
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

TF_Server* TF_NewServer(const void* proto, size_t proto_len,
                        TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
  return nullptr;
#else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerStart(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Start();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerStop(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Stop();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

void TF_ServerJoin(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Join();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

const char* TF_ServerTarget(TF_Server* server) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  return nullptr;
#else
  return server->target.c_str();
#endif
}

void TF_DeleteServer(TF_Server* server) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  delete server;
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}

void TF_RegisterLogListener(void (*listener)(const char*)) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  tensorflow::logging::RegisterListener(listener);
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}

void TF_RegisterFilesystemPlugin(const char* plugin_filename,
                                 TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
#else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}

}
   {: {
   declaration: TF_OperationDescription* TF_NewOperationLocked(TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
    type_identifier: TF_OperationDescription
    pointer_declarator: * TF_NewOperationLocked(TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
     *: *
     function_declarator: TF_NewOperationLocked(TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
      identifier: TF_NewOperationLocked
      parameter_list: (TF_Graph* graph,
                                               const char* op_type,
                                               const char* oper_name)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: const char* op_type
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * op_type
         *: *
         identifier: op_type
       ,: ,
       parameter_declaration: const char* oper_name
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * oper_name
         *: *
         identifier: oper_name
       ): )
    ;: 
   expression_statement: TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu)
    call_expression: TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu)
     identifier: TF_EXCLUSIVE_LOCKS_REQUIRED
     argument_list: (graph->mu)
      (: (
      field_expression: graph->mu
       identifier: graph
       ->: ->
       field_identifier: mu
      ): )
    ;: 
   compound_statement: {
  return new TF_OperationDescription(graph, op_type, oper_name);
}
    {: {
    return_statement: return new TF_OperationDescription(graph, op_type, oper_name);
     return: return
     new_expression: new TF_OperationDescription(graph, op_type, oper_name)
      new: new
      type_identifier: TF_OperationDescription
      argument_list: (graph, op_type, oper_name)
       (: (
       identifier: graph
       ,: ,
       identifier: op_type
       ,: ,
       identifier: oper_name
       ): )
     ;: ;
    }: }
   function_definition: TF_OperationDescription* TF_NewOperation(TF_Graph* graph, const char* op_type,
                                         const char* oper_name) {
  mutex_lock l(graph->mu);
  return TF_NewOperationLocked(graph, op_type, oper_name);
}
    type_identifier: TF_OperationDescription
    pointer_declarator: * TF_NewOperation(TF_Graph* graph, const char* op_type,
                                         const char* oper_name)
     *: *
     function_declarator: TF_NewOperation(TF_Graph* graph, const char* op_type,
                                         const char* oper_name)
      identifier: TF_NewOperation
      parameter_list: (TF_Graph* graph, const char* op_type,
                                         const char* oper_name)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: const char* op_type
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * op_type
         *: *
         identifier: op_type
       ,: ,
       parameter_declaration: const char* oper_name
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * oper_name
         *: *
         identifier: oper_name
       ): )
    compound_statement: {
  mutex_lock l(graph->mu);
  return TF_NewOperationLocked(graph, op_type, oper_name);
}
     {: {
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     return_statement: return TF_NewOperationLocked(graph, op_type, oper_name);
      return: return
      call_expression: TF_NewOperationLocked(graph, op_type, oper_name)
       identifier: TF_NewOperationLocked
       argument_list: (graph, op_type, oper_name)
        (: (
        identifier: graph
        ,: ,
        identifier: op_type
        ,: ,
        identifier: oper_name
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetDevice(TF_OperationDescription* desc, const char* device) {
  desc->node_builder.Device(device);
}
    primitive_type: void
    function_declarator: TF_SetDevice(TF_OperationDescription* desc, const char* device)
     identifier: TF_SetDevice
     parameter_list: (TF_OperationDescription* desc, const char* device)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* device
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * device
        *: *
        identifier: device
      ): )
    compound_statement: {
  desc->node_builder.Device(device);
}
     {: {
     expression_statement: desc->node_builder.Device(device);
      call_expression: desc->node_builder.Device(device)
       field_expression: desc->node_builder.Device
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Device
       argument_list: (device)
        (: (
        identifier: device
        ): )
      ;: ;
     }: }
   function_definition: void TF_AddInput(TF_OperationDescription* desc, TF_Output input) {
  desc->node_builder.Input(&input.oper->node, input.index);
}
    primitive_type: void
    function_declarator: TF_AddInput(TF_OperationDescription* desc, TF_Output input)
     identifier: TF_AddInput
     parameter_list: (TF_OperationDescription* desc, TF_Output input)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: TF_Output input
       type_identifier: TF_Output
       identifier: input
      ): )
    compound_statement: {
  desc->node_builder.Input(&input.oper->node, input.index);
}
     {: {
     expression_statement: desc->node_builder.Input(&input.oper->node, input.index);
      call_expression: desc->node_builder.Input(&input.oper->node, input.index)
       field_expression: desc->node_builder.Input
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Input
       argument_list: (&input.oper->node, input.index)
        (: (
        pointer_expression: &input.oper->node
         &: &
         field_expression: input.oper->node
          field_expression: input.oper
           identifier: input
           .: .
           field_identifier: oper
          ->: ->
          field_identifier: node
        ,: ,
        field_expression: input.index
         identifier: input
         .: .
         field_identifier: index
        ): )
      ;: ;
     }: }
   function_definition: void TF_AddInputList(TF_OperationDescription* desc, const TF_Output* inputs,
                     int num_inputs) {
  std::vector<NodeBuilder::NodeOut> input_list;
  input_list.reserve(num_inputs);
  for (int i = 0; i < num_inputs; ++i) {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
  desc->node_builder.Input(input_list);
}
    primitive_type: void
    function_declarator: TF_AddInputList(TF_OperationDescription* desc, const TF_Output* inputs,
                     int num_inputs)
     identifier: TF_AddInputList
     parameter_list: (TF_OperationDescription* desc, const TF_Output* inputs,
                     int num_inputs)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const TF_Output* inputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * inputs
        *: *
        identifier: inputs
      ,: ,
      parameter_declaration: int num_inputs
       primitive_type: int
       identifier: num_inputs
      ): )
    compound_statement: {
  std::vector<NodeBuilder::NodeOut> input_list;
  input_list.reserve(num_inputs);
  for (int i = 0; i < num_inputs; ++i) {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
  desc->node_builder.Input(input_list);
}
     {: {
     declaration: std::vector<NodeBuilder::NodeOut> input_list;
      qualified_identifier: std::vector<NodeBuilder::NodeOut>
       namespace_identifier: std
       ::: ::
       template_type: vector<NodeBuilder::NodeOut>
        type_identifier: vector
        template_argument_list: <NodeBuilder::NodeOut>
         <: <
         type_descriptor: NodeBuilder::NodeOut
          qualified_identifier: NodeBuilder::NodeOut
           namespace_identifier: NodeBuilder
           ::: ::
           type_identifier: NodeOut
         >: >
      identifier: input_list
      ;: ;
     expression_statement: input_list.reserve(num_inputs);
      call_expression: input_list.reserve(num_inputs)
       field_expression: input_list.reserve
        identifier: input_list
        .: .
        field_identifier: reserve
       argument_list: (num_inputs)
        (: (
        identifier: num_inputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < num_inputs; ++i) {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < num_inputs
       identifier: i
       <: <
       identifier: num_inputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
  }
       {: {
       expression_statement: input_list.emplace_back(&inputs[i].oper->node, inputs[i].index);
        call_expression: input_list.emplace_back(&inputs[i].oper->node, inputs[i].index)
         field_expression: input_list.emplace_back
          identifier: input_list
          .: .
          field_identifier: emplace_back
         argument_list: (&inputs[i].oper->node, inputs[i].index)
          (: (
          pointer_expression: &inputs[i].oper->node
           &: &
           field_expression: inputs[i].oper->node
            field_expression: inputs[i].oper
             subscript_expression: inputs[i]
              identifier: inputs
              subscript_argument_list: [i]
               [: [
               identifier: i
               ]: ]
             .: .
             field_identifier: oper
            ->: ->
            field_identifier: node
          ,: ,
          field_expression: inputs[i].index
           subscript_expression: inputs[i]
            identifier: inputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           .: .
           field_identifier: index
          ): )
        ;: ;
       }: }
     expression_statement: desc->node_builder.Input(input_list);
      call_expression: desc->node_builder.Input(input_list)
       field_expression: desc->node_builder.Input
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Input
       argument_list: (input_list)
        (: (
        identifier: input_list
        ): )
      ;: ;
     }: }
   function_definition: void TF_AddControlInput(TF_OperationDescription* desc, TF_Operation* input) {
  desc->node_builder.ControlInput(&input->node);
}
    primitive_type: void
    function_declarator: TF_AddControlInput(TF_OperationDescription* desc, TF_Operation* input)
     identifier: TF_AddControlInput
     parameter_list: (TF_OperationDescription* desc, TF_Operation* input)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: TF_Operation* input
       type_identifier: TF_Operation
       pointer_declarator: * input
        *: *
        identifier: input
      ): )
    compound_statement: {
  desc->node_builder.ControlInput(&input->node);
}
     {: {
     expression_statement: desc->node_builder.ControlInput(&input->node);
      call_expression: desc->node_builder.ControlInput(&input->node)
       field_expression: desc->node_builder.ControlInput
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: ControlInput
       argument_list: (&input->node)
        (: (
        pointer_expression: &input->node
         &: &
         field_expression: input->node
          identifier: input
          ->: ->
          field_identifier: node
        ): )
      ;: ;
     }: }
   function_definition: void TF_ColocateWith(TF_OperationDescription* desc, TF_Operation* op) {
  desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()));
}
    primitive_type: void
    function_declarator: TF_ColocateWith(TF_OperationDescription* desc, TF_Operation* op)
     identifier: TF_ColocateWith
     parameter_list: (TF_OperationDescription* desc, TF_Operation* op)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ): )
    compound_statement: {
  desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()));
}
     {: {
     expression_statement: desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()));
      call_expression: desc->colocation_constraints.emplace(
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()))
       field_expression: desc->colocation_constraints.emplace
        field_expression: desc->colocation_constraints
         identifier: desc
         ->: ->
         field_identifier: colocation_constraints
        .: .
        field_identifier: emplace
       argument_list: (
      StrCat(tensorflow::kColocationGroupPrefix, op->node.name()))
        (: (
        call_expression: StrCat(tensorflow::kColocationGroupPrefix, op->node.name())
         identifier: StrCat
         argument_list: (tensorflow::kColocationGroupPrefix, op->node.name())
          (: (
          qualified_identifier: tensorflow::kColocationGroupPrefix
           namespace_identifier: tensorflow
           ::: ::
           identifier: kColocationGroupPrefix
          ,: ,
          call_expression: op->node.name()
           field_expression: op->node.name
            field_expression: op->node
             identifier: op
             ->: ->
             field_identifier: node
            .: .
            field_identifier: name
           argument_list: ()
            (: (
            ): )
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrString(TF_OperationDescription* desc, const char* attr_name,
                      const void* value, size_t length) {
  absl::string_view s(static_cast<const char*>(value), length);
  desc->node_builder.Attr(attr_name, s);
}
    primitive_type: void
    function_declarator: TF_SetAttrString(TF_OperationDescription* desc, const char* attr_name,
                      const void* value, size_t length)
     identifier: TF_SetAttrString
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                      const void* value, size_t length)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const void* value
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * value
        *: *
        identifier: value
      ,: ,
      parameter_declaration: size_t length
       primitive_type: size_t
       identifier: length
      ): )
    compound_statement: {
  absl::string_view s(static_cast<const char*>(value), length);
  desc->node_builder.Attr(attr_name, s);
}
     {: {
     declaration: absl::string_view s(static_cast<const char*>(value), length);
      qualified_identifier: absl::string_view
       namespace_identifier: absl
       ::: ::
       type_identifier: string_view
      function_declarator: s(static_cast<const char*>(value), length)
       identifier: s
       parameter_list: (static_cast<const char*>(value), length)
        (: (
        parameter_declaration: static_cast<const char*>(value)
         template_type: static_cast<const char*>
          type_identifier: static_cast
          template_argument_list: <const char*>
           <: <
           type_descriptor: const char*
            type_qualifier: const
             const: const
            primitive_type: char
            abstract_pointer_declarator: *
             *: *
           >: >
         abstract_function_declarator: (value)
          parameter_list: (value)
           (: (
           parameter_declaration: value
            type_identifier: value
           ): )
        ,: ,
        parameter_declaration: length
         type_identifier: length
        ): )
      ;: ;
     expression_statement: desc->node_builder.Attr(attr_name, s);
      call_expression: desc->node_builder.Attr(attr_name, s)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, s)
        (: (
        identifier: attr_name
        ,: ,
        identifier: s
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrStringList(TF_OperationDescription* desc, const char* attr_name,
                          const void* const* values, const size_t* lengths,
                          int num_values) {
  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  } else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
}
    primitive_type: void
    function_declarator: TF_SetAttrStringList(TF_OperationDescription* desc, const char* attr_name,
                          const void* const* values, const size_t* lengths,
                          int num_values)
     identifier: TF_SetAttrStringList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                          const void* const* values, const size_t* lengths,
                          int num_values)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const void* const* values
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * const* values
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * values
         *: *
         identifier: values
      ,: ,
      parameter_declaration: const size_t* lengths
       type_qualifier: const
        const: const
       primitive_type: size_t
       pointer_declarator: * lengths
        *: *
        identifier: lengths
      ,: ,
      parameter_declaration: int num_values
       primitive_type: int
       identifier: num_values
      ): )
    compound_statement: {
  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  } else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
}
     {: {
     if_statement: if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  } else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
      if: if
      condition_clause: (strcmp(attr_name, tensorflow::kColocationAttrName) == 0)
       (: (
       binary_expression: strcmp(attr_name, tensorflow::kColocationAttrName) == 0
        call_expression: strcmp(attr_name, tensorflow::kColocationAttrName)
         identifier: strcmp
         argument_list: (attr_name, tensorflow::kColocationAttrName)
          (: (
          identifier: attr_name
          ,: ,
          qualified_identifier: tensorflow::kColocationAttrName
           namespace_identifier: tensorflow
           ::: ::
           identifier: kColocationAttrName
          ): )
        ==: ==
        number_literal: 0
       ): )
      compound_statement: {
    desc->colocation_constraints.clear();
    for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
  }
       {: {
       expression_statement: desc->colocation_constraints.clear();
        call_expression: desc->colocation_constraints.clear()
         field_expression: desc->colocation_constraints.clear
          field_expression: desc->colocation_constraints
           identifier: desc
           ->: ->
           field_identifier: colocation_constraints
          .: .
          field_identifier: clear
         argument_list: ()
          (: (
          ): )
        ;: ;
       for_statement: for (int i = 0; i < num_values; ++i) {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
        for: for
        (: (
        declaration: int i = 0;
         primitive_type: int
         init_declarator: i = 0
          identifier: i
          =: =
          number_literal: 0
         ;: ;
        binary_expression: i < num_values
         identifier: i
         <: <
         identifier: num_values
        ;: ;
        update_expression: ++i
         ++: ++
         identifier: i
        ): )
        compound_statement: {
      desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
    }
         {: {
         expression_statement: desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i]);
          call_expression: desc->colocation_constraints.emplace(static_cast<const char*>(values[i]),
                                           lengths[i])
           field_expression: desc->colocation_constraints.emplace
            field_expression: desc->colocation_constraints
             identifier: desc
             ->: ->
             field_identifier: colocation_constraints
            .: .
            field_identifier: emplace
           argument_list: (static_cast<const char*>(values[i]),
                                           lengths[i])
            (: (
            call_expression: static_cast<const char*>(values[i])
             template_function: static_cast<const char*>
              identifier: static_cast
              template_argument_list: <const char*>
               <: <
               type_descriptor: const char*
                type_qualifier: const
                 const: const
                primitive_type: char
                abstract_pointer_declarator: *
                 *: *
               >: >
             argument_list: (values[i])
              (: (
              subscript_expression: values[i]
               identifier: values
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              ): )
            ,: ,
            subscript_expression: lengths[i]
             identifier: lengths
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            ): )
          ;: ;
         }: }
       }: }
      else_clause: else {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
       else: else
       compound_statement: {
    std::vector<absl::string_view> v;
    v.reserve(num_values);
    for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
    desc->node_builder.Attr(attr_name, v);
  }
        {: {
        declaration: std::vector<absl::string_view> v;
         qualified_identifier: std::vector<absl::string_view>
          namespace_identifier: std
          ::: ::
          template_type: vector<absl::string_view>
           type_identifier: vector
           template_argument_list: <absl::string_view>
            <: <
            type_descriptor: absl::string_view
             qualified_identifier: absl::string_view
              namespace_identifier: absl
              ::: ::
              type_identifier: string_view
            >: >
         identifier: v
         ;: ;
        expression_statement: v.reserve(num_values);
         call_expression: v.reserve(num_values)
          field_expression: v.reserve
           identifier: v
           .: .
           field_identifier: reserve
          argument_list: (num_values)
           (: (
           identifier: num_values
           ): )
         ;: ;
        for_statement: for (int i = 0; i < num_values; ++i) {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
         for: for
         (: (
         declaration: int i = 0;
          primitive_type: int
          init_declarator: i = 0
           identifier: i
           =: =
           number_literal: 0
          ;: ;
         binary_expression: i < num_values
          identifier: i
          <: <
          identifier: num_values
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
      v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
    }
          {: {
          expression_statement: v.emplace_back(static_cast<const char*>(values[i]), lengths[i]);
           call_expression: v.emplace_back(static_cast<const char*>(values[i]), lengths[i])
            field_expression: v.emplace_back
             identifier: v
             .: .
             field_identifier: emplace_back
            argument_list: (static_cast<const char*>(values[i]), lengths[i])
             (: (
             call_expression: static_cast<const char*>(values[i])
              template_function: static_cast<const char*>
               identifier: static_cast
               template_argument_list: <const char*>
                <: <
                type_descriptor: const char*
                 type_qualifier: const
                  const: const
                 primitive_type: char
                 abstract_pointer_declarator: *
                  *: *
                >: >
              argument_list: (values[i])
               (: (
               subscript_expression: values[i]
                identifier: values
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
               ): )
             ,: ,
             subscript_expression: lengths[i]
              identifier: lengths
              subscript_argument_list: [i]
               [: [
               identifier: i
               ]: ]
             ): )
           ;: ;
          }: }
        expression_statement: desc->node_builder.Attr(attr_name, v);
         call_expression: desc->node_builder.Attr(attr_name, v)
          field_expression: desc->node_builder.Attr
           field_expression: desc->node_builder
            identifier: desc
            ->: ->
            field_identifier: node_builder
           .: .
           field_identifier: Attr
          argument_list: (attr_name, v)
           (: (
           identifier: attr_name
           ,: ,
           identifier: v
           ): )
         ;: ;
        }: }
     }: }
   function_definition: void TF_SetAttrInt(TF_OperationDescription* desc, const char* attr_name,
                   int64_t value) {
  desc->node_builder.Attr(attr_name, static_cast<int64_t>(value));
}
    primitive_type: void
    function_declarator: TF_SetAttrInt(TF_OperationDescription* desc, const char* attr_name,
                   int64_t value)
     identifier: TF_SetAttrInt
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                   int64_t value)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: int64_t value
       primitive_type: int64_t
       identifier: value
      ): )
    compound_statement: {
  desc->node_builder.Attr(attr_name, static_cast<int64_t>(value));
}
     {: {
     expression_statement: desc->node_builder.Attr(attr_name, static_cast<int64_t>(value));
      call_expression: desc->node_builder.Attr(attr_name, static_cast<int64_t>(value))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, static_cast<int64_t>(value))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: static_cast<int64_t>(value)
         template_function: static_cast<int64_t>
          identifier: static_cast
          template_argument_list: <int64_t>
           <: <
           type_descriptor: int64_t
            primitive_type: int64_t
           >: >
         argument_list: (value)
          (: (
          identifier: value
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrIntList(TF_OperationDescription* desc, const char* attr_name,
                       const int64_t* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values));
}
    primitive_type: void
    function_declarator: TF_SetAttrIntList(TF_OperationDescription* desc, const char* attr_name,
                       const int64_t* values, int num_values)
     identifier: TF_SetAttrIntList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                       const int64_t* values, int num_values)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const int64_t* values
       type_qualifier: const
        const: const
       primitive_type: int64_t
       pointer_declarator: * values
        *: *
        identifier: values
      ,: ,
      parameter_declaration: int num_values
       primitive_type: int
       identifier: num_values
      ): )
    compound_statement: {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values));
}
     {: {
     expression_statement: desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values));
      call_expression: desc->node_builder.Attr(
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (
      attr_name, ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: ArraySlice<const int64_t>(
                     reinterpret_cast<const int64_t*>(values), num_values)
         template_function: ArraySlice<const int64_t>
          identifier: ArraySlice
          template_argument_list: <const int64_t>
           <: <
           type_descriptor: const int64_t
            type_qualifier: const
             const: const
            primitive_type: int64_t
           >: >
         argument_list: (
                     reinterpret_cast<const int64_t*>(values), num_values)
          (: (
          call_expression: reinterpret_cast<const int64_t*>(values)
           template_function: reinterpret_cast<const int64_t*>
            identifier: reinterpret_cast
            template_argument_list: <const int64_t*>
             <: <
             type_descriptor: const int64_t*
              type_qualifier: const
               const: const
              primitive_type: int64_t
              abstract_pointer_declarator: *
               *: *
             >: >
           argument_list: (values)
            (: (
            identifier: values
            ): )
          ,: ,
          identifier: num_values
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrFloat(TF_OperationDescription* desc, const char* attr_name,
                     float value) {
  desc->node_builder.Attr(attr_name, value);
}
    primitive_type: void
    function_declarator: TF_SetAttrFloat(TF_OperationDescription* desc, const char* attr_name,
                     float value)
     identifier: TF_SetAttrFloat
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                     float value)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: float value
       primitive_type: float
       identifier: value
      ): )
    compound_statement: {
  desc->node_builder.Attr(attr_name, value);
}
     {: {
     expression_statement: desc->node_builder.Attr(attr_name, value);
      call_expression: desc->node_builder.Attr(attr_name, value)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, value)
        (: (
        identifier: attr_name
        ,: ,
        identifier: value
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrFloatList(TF_OperationDescription* desc, const char* attr_name,
                         const float* values, int num_values) {
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values));
}
    primitive_type: void
    function_declarator: TF_SetAttrFloatList(TF_OperationDescription* desc, const char* attr_name,
                         const float* values, int num_values)
     identifier: TF_SetAttrFloatList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                         const float* values, int num_values)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const float* values
       type_qualifier: const
        const: const
       primitive_type: float
       pointer_declarator: * values
        *: *
        identifier: values
      ,: ,
      parameter_declaration: int num_values
       primitive_type: int
       identifier: num_values
      ): )
    compound_statement: {
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values));
}
     {: {
     expression_statement: desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values));
      call_expression: desc->node_builder.Attr(attr_name,
                          ArraySlice<const float>(values, num_values))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name,
                          ArraySlice<const float>(values, num_values))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: ArraySlice<const float>(values, num_values)
         template_function: ArraySlice<const float>
          identifier: ArraySlice
          template_argument_list: <const float>
           <: <
           type_descriptor: const float
            type_qualifier: const
             const: const
            primitive_type: float
           >: >
         argument_list: (values, num_values)
          (: (
          identifier: values
          ,: ,
          identifier: num_values
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrBool(TF_OperationDescription* desc, const char* attr_name,
                    unsigned char value) {
  desc->node_builder.Attr(attr_name, static_cast<bool>(value));
}
    primitive_type: void
    function_declarator: TF_SetAttrBool(TF_OperationDescription* desc, const char* attr_name,
                    unsigned char value)
     identifier: TF_SetAttrBool
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                    unsigned char value)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: unsigned char value
       sized_type_specifier: unsigned char
        unsigned: unsigned
        primitive_type: char
       identifier: value
      ): )
    compound_statement: {
  desc->node_builder.Attr(attr_name, static_cast<bool>(value));
}
     {: {
     expression_statement: desc->node_builder.Attr(attr_name, static_cast<bool>(value));
      call_expression: desc->node_builder.Attr(attr_name, static_cast<bool>(value))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, static_cast<bool>(value))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: static_cast<bool>(value)
         template_function: static_cast<bool>
          identifier: static_cast
          template_argument_list: <bool>
           <: <
           type_descriptor: bool
            primitive_type: bool
           >: >
         argument_list: (value)
          (: (
          identifier: value
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrBoolList(TF_OperationDescription* desc, const char* attr_name,
                        const unsigned char* values, int num_values) {
  std::unique_ptr<bool[]> b(new bool[num_values]);
  for (int i = 0; i < num_values; ++i) {
    b[i] = values[i];
  }
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values));
}
    primitive_type: void
    function_declarator: TF_SetAttrBoolList(TF_OperationDescription* desc, const char* attr_name,
                        const unsigned char* values, int num_values)
     identifier: TF_SetAttrBoolList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                        const unsigned char* values, int num_values)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const unsigned char* values
       type_qualifier: const
        const: const
       sized_type_specifier: unsigned char
        unsigned: unsigned
        primitive_type: char
       pointer_declarator: * values
        *: *
        identifier: values
      ,: ,
      parameter_declaration: int num_values
       primitive_type: int
       identifier: num_values
      ): )
    compound_statement: {
  std::unique_ptr<bool[]> b(new bool[num_values]);
  for (int i = 0; i < num_values; ++i) {
    b[i] = values[i];
  }
  desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values));
}
     {: {
     declaration: std::unique_ptr<bool[]> b(new bool[num_values]);
      qualified_identifier: std::unique_ptr<bool[]>
       namespace_identifier: std
       ::: ::
       template_type: unique_ptr<bool[]>
        type_identifier: unique_ptr
        template_argument_list: <bool[]>
         <: <
         type_descriptor: bool[]
          primitive_type: bool
          abstract_array_declarator: []
           [: [
           ]: ]
         >: >
      init_declarator: b(new bool[num_values])
       identifier: b
       argument_list: (new bool[num_values])
        (: (
        new_expression: new bool[num_values]
         new: new
         primitive_type: bool
         new_declarator: [num_values]
          [: [
          identifier: num_values
          ]: ]
        ): )
      ;: ;
     for_statement: for (int i = 0; i < num_values; ++i) {
    b[i] = values[i];
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < num_values
       identifier: i
       <: <
       identifier: num_values
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    b[i] = values[i];
  }
       {: {
       expression_statement: b[i] = values[i];
        assignment_expression: b[i] = values[i]
         subscript_expression: b[i]
          identifier: b
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         subscript_expression: values[i]
          identifier: values
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
        ;: ;
       }: }
     expression_statement: desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values));
      call_expression: desc->node_builder.Attr(attr_name,
                          ArraySlice<const bool>(b.get(), num_values))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name,
                          ArraySlice<const bool>(b.get(), num_values))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: ArraySlice<const bool>(b.get(), num_values)
         template_function: ArraySlice<const bool>
          identifier: ArraySlice
          template_argument_list: <const bool>
           <: <
           type_descriptor: const bool
            type_qualifier: const
             const: const
            primitive_type: bool
           >: >
         argument_list: (b.get(), num_values)
          (: (
          call_expression: b.get()
           field_expression: b.get
            identifier: b
            .: .
            field_identifier: get
           argument_list: ()
            (: (
            ): )
          ,: ,
          identifier: num_values
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrType(TF_OperationDescription* desc, const char* attr_name,
                    TF_DataType value) {
  desc->node_builder.Attr(attr_name, static_cast<DataType>(value));
}
    primitive_type: void
    function_declarator: TF_SetAttrType(TF_OperationDescription* desc, const char* attr_name,
                    TF_DataType value)
     identifier: TF_SetAttrType
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                    TF_DataType value)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_DataType value
       type_identifier: TF_DataType
       identifier: value
      ): )
    compound_statement: {
  desc->node_builder.Attr(attr_name, static_cast<DataType>(value));
}
     {: {
     expression_statement: desc->node_builder.Attr(attr_name, static_cast<DataType>(value));
      call_expression: desc->node_builder.Attr(attr_name, static_cast<DataType>(value))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, static_cast<DataType>(value))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: static_cast<DataType>(value)
         template_function: static_cast<DataType>
          identifier: static_cast
          template_argument_list: <DataType>
           <: <
           type_descriptor: DataType
            type_identifier: DataType
           >: >
         argument_list: (value)
          (: (
          identifier: value
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrTypeList(TF_OperationDescription* desc, const char* attr_name,
                        const TF_DataType* values, int num_values) {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values));
}
    primitive_type: void
    function_declarator: TF_SetAttrTypeList(TF_OperationDescription* desc, const char* attr_name,
                        const TF_DataType* values, int num_values)
     identifier: TF_SetAttrTypeList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                        const TF_DataType* values, int num_values)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const TF_DataType* values
       type_qualifier: const
        const: const
       type_identifier: TF_DataType
       pointer_declarator: * values
        *: *
        identifier: values
      ,: ,
      parameter_declaration: int num_values
       primitive_type: int
       identifier: num_values
      ): )
    compound_statement: {
  desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values));
}
     {: {
     expression_statement: desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values));
      call_expression: desc->node_builder.Attr(
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values))
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (
      attr_name, ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values))
        (: (
        identifier: attr_name
        ,: ,
        call_expression: ArraySlice<const DataType>(
                     reinterpret_cast<const DataType*>(values), num_values)
         template_function: ArraySlice<const DataType>
          identifier: ArraySlice
          template_argument_list: <const DataType>
           <: <
           type_descriptor: const DataType
            type_qualifier: const
             const: const
            type_identifier: DataType
           >: >
         argument_list: (
                     reinterpret_cast<const DataType*>(values), num_values)
          (: (
          call_expression: reinterpret_cast<const DataType*>(values)
           template_function: reinterpret_cast<const DataType*>
            identifier: reinterpret_cast
            template_argument_list: <const DataType*>
             <: <
             type_descriptor: const DataType*
              type_qualifier: const
               const: const
              type_identifier: DataType
              abstract_pointer_declarator: *
               *: *
             >: >
           argument_list: (values)
            (: (
            identifier: values
            ): )
          ,: ,
          identifier: num_values
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrPlaceholder(TF_OperationDescription* desc, const char* attr_name,
                           const char* placeholder) {
  tensorflow::AttrValue attr_value;
  attr_value.set_placeholder(placeholder);
  desc->node_builder.Attr(attr_name, attr_value);
}
    primitive_type: void
    function_declarator: TF_SetAttrPlaceholder(TF_OperationDescription* desc, const char* attr_name,
                           const char* placeholder)
     identifier: TF_SetAttrPlaceholder
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                           const char* placeholder)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const char* placeholder
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * placeholder
        *: *
        identifier: placeholder
      ): )
    compound_statement: {
  tensorflow::AttrValue attr_value;
  attr_value.set_placeholder(placeholder);
  desc->node_builder.Attr(attr_name, attr_value);
}
     {: {
     declaration: tensorflow::AttrValue attr_value;
      qualified_identifier: tensorflow::AttrValue
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: AttrValue
      identifier: attr_value
      ;: ;
     expression_statement: attr_value.set_placeholder(placeholder);
      call_expression: attr_value.set_placeholder(placeholder)
       field_expression: attr_value.set_placeholder
        identifier: attr_value
        .: .
        field_identifier: set_placeholder
       argument_list: (placeholder)
        (: (
        identifier: placeholder
        ): )
      ;: ;
     expression_statement: desc->node_builder.Attr(attr_name, attr_value);
      call_expression: desc->node_builder.Attr(attr_name, attr_value)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, attr_value)
        (: (
        identifier: attr_name
        ,: ,
        identifier: attr_value
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrFuncName(TF_OperationDescription* desc, const char* attr_name,
                        const char* value, size_t length) {
  tensorflow::NameAttrList func_name;
  func_name.set_name(string(value, value + length));
  desc->node_builder.Attr(attr_name, func_name);
}
    primitive_type: void
    function_declarator: TF_SetAttrFuncName(TF_OperationDescription* desc, const char* attr_name,
                        const char* value, size_t length)
     identifier: TF_SetAttrFuncName
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                        const char* value, size_t length)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const char* value
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * value
        *: *
        identifier: value
      ,: ,
      parameter_declaration: size_t length
       primitive_type: size_t
       identifier: length
      ): )
    compound_statement: {
  tensorflow::NameAttrList func_name;
  func_name.set_name(string(value, value + length));
  desc->node_builder.Attr(attr_name, func_name);
}
     {: {
     declaration: tensorflow::NameAttrList func_name;
      qualified_identifier: tensorflow::NameAttrList
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: NameAttrList
      identifier: func_name
      ;: ;
     expression_statement: func_name.set_name(string(value, value + length));
      call_expression: func_name.set_name(string(value, value + length))
       field_expression: func_name.set_name
        identifier: func_name
        .: .
        field_identifier: set_name
       argument_list: (string(value, value + length))
        (: (
        call_expression: string(value, value + length)
         identifier: string
         argument_list: (value, value + length)
          (: (
          identifier: value
          ,: ,
          binary_expression: value + length
           identifier: value
           +: +
           identifier: length
          ): )
        ): )
      ;: ;
     expression_statement: desc->node_builder.Attr(attr_name, func_name);
      call_expression: desc->node_builder.Attr(attr_name, func_name)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, func_name)
        (: (
        identifier: attr_name
        ,: ,
        identifier: func_name
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrShape(TF_OperationDescription* desc, const char* attr_name,
                     const int64_t* dims, int num_dims) {
  PartialTensorShape shape;
  if (num_dims >= 0) {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
  desc->node_builder.Attr(attr_name, shape);
}
    primitive_type: void
    function_declarator: TF_SetAttrShape(TF_OperationDescription* desc, const char* attr_name,
                     const int64_t* dims, int num_dims)
     identifier: TF_SetAttrShape
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                     const int64_t* dims, int num_dims)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const int64_t* dims
       type_qualifier: const
        const: const
       primitive_type: int64_t
       pointer_declarator: * dims
        *: *
        identifier: dims
      ,: ,
      parameter_declaration: int num_dims
       primitive_type: int
       identifier: num_dims
      ): )
    compound_statement: {
  PartialTensorShape shape;
  if (num_dims >= 0) {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
  desc->node_builder.Attr(attr_name, shape);
}
     {: {
     declaration: PartialTensorShape shape;
      type_identifier: PartialTensorShape
      identifier: shape
      ;: ;
     if_statement: if (num_dims >= 0) {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
      if: if
      condition_clause: (num_dims >= 0)
       (: (
       binary_expression: num_dims >= 0
        identifier: num_dims
        >=: >=
        number_literal: 0
       ): )
      compound_statement: {
    shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
  }
       {: {
       expression_statement: shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims));
        assignment_expression: shape = PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims))
         identifier: shape
         =: =
         call_expression: PartialTensorShape(
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims))
          identifier: PartialTensorShape
          argument_list: (
        ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims))
           (: (
           call_expression: ArraySlice<int64_t>(reinterpret_cast<const int64_t*>(dims), num_dims)
            template_function: ArraySlice<int64_t>
             identifier: ArraySlice
             template_argument_list: <int64_t>
              <: <
              type_descriptor: int64_t
               primitive_type: int64_t
              >: >
            argument_list: (reinterpret_cast<const int64_t*>(dims), num_dims)
             (: (
             call_expression: reinterpret_cast<const int64_t*>(dims)
              template_function: reinterpret_cast<const int64_t*>
               identifier: reinterpret_cast
               template_argument_list: <const int64_t*>
                <: <
                type_descriptor: const int64_t*
                 type_qualifier: const
                  const: const
                 primitive_type: int64_t
                 abstract_pointer_declarator: *
                  *: *
                >: >
              argument_list: (dims)
               (: (
               identifier: dims
               ): )
             ,: ,
             identifier: num_dims
             ): )
           ): )
        ;: ;
       }: }
     expression_statement: desc->node_builder.Attr(attr_name, shape);
      call_expression: desc->node_builder.Attr(attr_name, shape)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, shape)
        (: (
        identifier: attr_name
        ,: ,
        identifier: shape
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrShapeList(TF_OperationDescription* desc, const char* attr_name,
                         const int64_t* const* dims, const int* num_dims,
                         int num_shapes) {
  std::vector<PartialTensorShape> shapes;
  shapes.reserve(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
}
    primitive_type: void
    function_declarator: TF_SetAttrShapeList(TF_OperationDescription* desc, const char* attr_name,
                         const int64_t* const* dims, const int* num_dims,
                         int num_shapes)
     identifier: TF_SetAttrShapeList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                         const int64_t* const* dims, const int* num_dims,
                         int num_shapes)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const int64_t* const* dims
       type_qualifier: const
        const: const
       primitive_type: int64_t
       pointer_declarator: * const* dims
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * dims
         *: *
         identifier: dims
      ,: ,
      parameter_declaration: const int* num_dims
       type_qualifier: const
        const: const
       primitive_type: int
       pointer_declarator: * num_dims
        *: *
        identifier: num_dims
      ,: ,
      parameter_declaration: int num_shapes
       primitive_type: int
       identifier: num_shapes
      ): )
    compound_statement: {
  std::vector<PartialTensorShape> shapes;
  shapes.reserve(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
}
     {: {
     declaration: std::vector<PartialTensorShape> shapes;
      qualified_identifier: std::vector<PartialTensorShape>
       namespace_identifier: std
       ::: ::
       template_type: vector<PartialTensorShape>
        type_identifier: vector
        template_argument_list: <PartialTensorShape>
         <: <
         type_descriptor: PartialTensorShape
          type_identifier: PartialTensorShape
         >: >
      identifier: shapes
      ;: ;
     expression_statement: shapes.reserve(num_shapes);
      call_expression: shapes.reserve(num_shapes)
       field_expression: shapes.reserve
        identifier: shapes
        .: .
        field_identifier: reserve
       argument_list: (num_shapes)
        (: (
        identifier: num_shapes
        ): )
      ;: ;
     for_statement: for (int i = 0; i < num_shapes; ++i) {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < num_shapes
       identifier: i
       <: <
       identifier: num_shapes
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
  }
       {: {
       if_statement: if (num_dims[i] < 0) {
      shapes.emplace_back();
    } else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
        if: if
        condition_clause: (num_dims[i] < 0)
         (: (
         binary_expression: num_dims[i] < 0
          subscript_expression: num_dims[i]
           identifier: num_dims
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          <: <
          number_literal: 0
         ): )
        compound_statement: {
      shapes.emplace_back();
    }
         {: {
         expression_statement: shapes.emplace_back();
          call_expression: shapes.emplace_back()
           field_expression: shapes.emplace_back
            identifier: shapes
            .: .
            field_identifier: emplace_back
           argument_list: ()
            (: (
            ): )
          ;: ;
         }: }
        else_clause: else {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
         else: else
         compound_statement: {
      shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
    }
          {: {
          expression_statement: shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]));
           call_expression: shapes.emplace_back(ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]))
            field_expression: shapes.emplace_back
             identifier: shapes
             .: .
             field_identifier: emplace_back
            argument_list: (ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i]))
             (: (
             call_expression: ArraySlice<int64_t>(
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i])
              template_function: ArraySlice<int64_t>
               identifier: ArraySlice
               template_argument_list: <int64_t>
                <: <
                type_descriptor: int64_t
                 primitive_type: int64_t
                >: >
              argument_list: (
          reinterpret_cast<const int64_t*>(dims[i]), num_dims[i])
               (: (
               call_expression: reinterpret_cast<const int64_t*>(dims[i])
                template_function: reinterpret_cast<const int64_t*>
                 identifier: reinterpret_cast
                 template_argument_list: <const int64_t*>
                  <: <
                  type_descriptor: const int64_t*
                   type_qualifier: const
                    const: const
                   primitive_type: int64_t
                   abstract_pointer_declarator: *
                    *: *
                  >: >
                argument_list: (dims[i])
                 (: (
                 subscript_expression: dims[i]
                  identifier: dims
                  subscript_argument_list: [i]
                   [: [
                   identifier: i
                   ]: ]
                 ): )
               ,: ,
               subscript_expression: num_dims[i]
                identifier: num_dims
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
               ): )
             ): )
           ;: ;
          }: }
       }: }
     expression_statement: desc->node_builder.Attr(attr_name, shapes);
      call_expression: desc->node_builder.Attr(attr_name, shapes)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, shapes)
        (: (
        identifier: attr_name
        ,: ,
        identifier: shapes
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrTensorShapeProto(TF_OperationDescription* desc,
                                const char* attr_name, const void* proto,
                                size_t proto_len, TF_Status* status) {
  // shape.ParseFromArray takes an int as length, this function takes size_t,
  // make sure there is no information loss.
  if (proto_len > std::numeric_limits<int>::max()) {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
  TensorShapeProto shape;
  if (shape.ParseFromArray(proto, static_cast<int>(proto_len))) {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  } else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
}
    primitive_type: void
    function_declarator: TF_SetAttrTensorShapeProto(TF_OperationDescription* desc,
                                const char* attr_name, const void* proto,
                                size_t proto_len, TF_Status* status)
     identifier: TF_SetAttrTensorShapeProto
     parameter_list: (TF_OperationDescription* desc,
                                const char* attr_name, const void* proto,
                                size_t proto_len, TF_Status* status)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const void* proto
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * proto
        *: *
        identifier: proto
      ,: ,
      parameter_declaration: size_t proto_len
       primitive_type: size_t
       identifier: proto_len
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  // shape.ParseFromArray takes an int as length, this function takes size_t,
  // make sure there is no information loss.
  if (proto_len > std::numeric_limits<int>::max()) {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
  TensorShapeProto shape;
  if (shape.ParseFromArray(proto, static_cast<int>(proto_len))) {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  } else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
}
     {: {
     comment: // shape.ParseFromArray takes an int as length, this function takes size_t,
     comment: // make sure there is no information loss.
     if_statement: if (proto_len > std::numeric_limits<int>::max()) {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
      if: if
      condition_clause: (proto_len > std::numeric_limits<int>::max())
       (: (
       binary_expression: proto_len > std::numeric_limits<int>::max()
        identifier: proto_len
        >: >
        call_expression: std::numeric_limits<int>::max()
         qualified_identifier: std::numeric_limits<int>::max
          namespace_identifier: std
          ::: ::
          qualified_identifier: numeric_limits<int>::max
           template_type: numeric_limits<int>
            type_identifier: numeric_limits
            template_argument_list: <int>
             <: <
             type_descriptor: int
              primitive_type: int
             >: >
           ::: ::
           identifier: max
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
    return;
  }
       {: {
       expression_statement: status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library");
        assignment_expression: status->status = InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument(
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library")
          identifier: InvalidArgument
          argument_list: (
        "proto_len (", proto_len,
        " bytes) is too large to be parsed by the protocol buffer library")
           (: (
           string_literal: "proto_len ("
            ": "
            string_content: proto_len (
            ": "
           ,: ,
           identifier: proto_len
           ,: ,
           string_literal: " bytes) is too large to be parsed by the protocol buffer library"
            ": "
            string_content:  bytes) is too large to be parsed by the protocol buffer library
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: TensorShapeProto shape;
      type_identifier: TensorShapeProto
      identifier: shape
      ;: ;
     if_statement: if (shape.ParseFromArray(proto, static_cast<int>(proto_len))) {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  } else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
      if: if
      condition_clause: (shape.ParseFromArray(proto, static_cast<int>(proto_len)))
       (: (
       call_expression: shape.ParseFromArray(proto, static_cast<int>(proto_len))
        field_expression: shape.ParseFromArray
         identifier: shape
         .: .
         field_identifier: ParseFromArray
        argument_list: (proto, static_cast<int>(proto_len))
         (: (
         identifier: proto
         ,: ,
         call_expression: static_cast<int>(proto_len)
          template_function: static_cast<int>
           identifier: static_cast
           template_argument_list: <int>
            <: <
            type_descriptor: int
             primitive_type: int
            >: >
          argument_list: (proto_len)
           (: (
           identifier: proto_len
           ): )
         ): )
       ): )
      compound_statement: {
    desc->node_builder.Attr(attr_name, shape);
    status->status = absl::OkStatus();
  }
       {: {
       expression_statement: desc->node_builder.Attr(attr_name, shape);
        call_expression: desc->node_builder.Attr(attr_name, shape)
         field_expression: desc->node_builder.Attr
          field_expression: desc->node_builder
           identifier: desc
           ->: ->
           field_identifier: node_builder
          .: .
          field_identifier: Attr
         argument_list: (attr_name, shape)
          (: (
          identifier: attr_name
          ,: ,
          identifier: shape
          ): )
        ;: ;
       expression_statement: status->status = absl::OkStatus();
        assignment_expression: status->status = absl::OkStatus()
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: absl::OkStatus()
          qualified_identifier: absl::OkStatus
           namespace_identifier: absl
           ::: ::
           identifier: OkStatus
          argument_list: ()
           (: (
           ): )
        ;: ;
       }: }
      else_clause: else {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
       else: else
       compound_statement: {
    status->status = InvalidArgument("Unparseable TensorShapeProto");
  }
        {: {
        expression_statement: status->status = InvalidArgument("Unparseable TensorShapeProto");
         assignment_expression: status->status = InvalidArgument("Unparseable TensorShapeProto")
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          =: =
          call_expression: InvalidArgument("Unparseable TensorShapeProto")
           identifier: InvalidArgument
           argument_list: ("Unparseable TensorShapeProto")
            (: (
            string_literal: "Unparseable TensorShapeProto"
             ": "
             string_content: Unparseable TensorShapeProto
             ": "
            ): )
         ;: ;
        }: }
     }: }
   function_definition: void TF_SetAttrTensorShapeProtoList(TF_OperationDescription* desc,
                                    const char* attr_name,
                                    const void* const* protos,
                                    const size_t* proto_lens, int num_shapes,
                                    TF_Status* status) {
  std::vector<TensorShapeProto> shapes;
  shapes.resize(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
  status->status = absl::OkStatus();
}
    primitive_type: void
    function_declarator: TF_SetAttrTensorShapeProtoList(TF_OperationDescription* desc,
                                    const char* attr_name,
                                    const void* const* protos,
                                    const size_t* proto_lens, int num_shapes,
                                    TF_Status* status)
     identifier: TF_SetAttrTensorShapeProtoList
     parameter_list: (TF_OperationDescription* desc,
                                    const char* attr_name,
                                    const void* const* protos,
                                    const size_t* proto_lens, int num_shapes,
                                    TF_Status* status)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const void* const* protos
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * const* protos
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * protos
         *: *
         identifier: protos
      ,: ,
      parameter_declaration: const size_t* proto_lens
       type_qualifier: const
        const: const
       primitive_type: size_t
       pointer_declarator: * proto_lens
        *: *
        identifier: proto_lens
      ,: ,
      parameter_declaration: int num_shapes
       primitive_type: int
       identifier: num_shapes
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  std::vector<TensorShapeProto> shapes;
  shapes.resize(num_shapes);
  for (int i = 0; i < num_shapes; ++i) {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
  desc->node_builder.Attr(attr_name, shapes);
  status->status = absl::OkStatus();
}
     {: {
     declaration: std::vector<TensorShapeProto> shapes;
      qualified_identifier: std::vector<TensorShapeProto>
       namespace_identifier: std
       ::: ::
       template_type: vector<TensorShapeProto>
        type_identifier: vector
        template_argument_list: <TensorShapeProto>
         <: <
         type_descriptor: TensorShapeProto
          type_identifier: TensorShapeProto
         >: >
      identifier: shapes
      ;: ;
     expression_statement: shapes.resize(num_shapes);
      call_expression: shapes.resize(num_shapes)
       field_expression: shapes.resize
        identifier: shapes
        .: .
        field_identifier: resize
       argument_list: (num_shapes)
        (: (
        identifier: num_shapes
        ): )
      ;: ;
     for_statement: for (int i = 0; i < num_shapes; ++i) {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < num_shapes
       identifier: i
       <: <
       identifier: num_shapes
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
    if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
  }
       {: {
       if_statement: if (proto_lens[i] > std::numeric_limits<int>::max()) {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
        if: if
        condition_clause: (proto_lens[i] > std::numeric_limits<int>::max())
         (: (
         binary_expression: proto_lens[i] > std::numeric_limits<int>::max()
          subscript_expression: proto_lens[i]
           identifier: proto_lens
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          >: >
          call_expression: std::numeric_limits<int>::max()
           qualified_identifier: std::numeric_limits<int>::max
            namespace_identifier: std
            ::: ::
            qualified_identifier: numeric_limits<int>::max
             template_type: numeric_limits<int>
              type_identifier: numeric_limits
              template_argument_list: <int>
               <: <
               type_descriptor: int
                primitive_type: int
               >: >
             ::: ::
             identifier: max
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
      status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
      return;
    }
         {: {
         expression_statement: status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library");
          assignment_expression: status->status = InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument(
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library")
            identifier: InvalidArgument
            argument_list: (
          "length of element ", i, " in the list (", proto_lens[i],
          " bytes) is too large to be parsed by the protocol buffer library")
             (: (
             string_literal: "length of element "
              ": "
              string_content: length of element 
              ": "
             ,: ,
             identifier: i
             ,: ,
             string_literal: " in the list ("
              ": "
              string_content:  in the list (
              ": "
             ,: ,
             subscript_expression: proto_lens[i]
              identifier: proto_lens
              subscript_argument_list: [i]
               [: [
               identifier: i
               ]: ]
             ,: ,
             string_literal: " bytes) is too large to be parsed by the protocol buffer library"
              ": "
              string_content:  bytes) is too large to be parsed by the protocol buffer library
              ": "
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       if_statement: if (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))) {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
        if: if
        condition_clause: (!shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i])))
         (: (
         unary_expression: !shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))
          !: !
          call_expression: shapes[i].ParseFromArray(protos[i], static_cast<int>(proto_lens[i]))
           field_expression: shapes[i].ParseFromArray
            subscript_expression: shapes[i]
             identifier: shapes
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            .: .
            field_identifier: ParseFromArray
           argument_list: (protos[i], static_cast<int>(proto_lens[i]))
            (: (
            subscript_expression: protos[i]
             identifier: protos
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            ,: ,
            call_expression: static_cast<int>(proto_lens[i])
             template_function: static_cast<int>
              identifier: static_cast
              template_argument_list: <int>
               <: <
               type_descriptor: int
                primitive_type: int
               >: >
             argument_list: (proto_lens[i])
              (: (
              subscript_expression: proto_lens[i]
               identifier: proto_lens
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              ): )
            ): )
         ): )
        compound_statement: {
      status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
      return;
    }
         {: {
         expression_statement: status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i);
          assignment_expression: status->status =
          InvalidArgument("Unparseable TensorShapeProto at index ", i)
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument("Unparseable TensorShapeProto at index ", i)
            identifier: InvalidArgument
            argument_list: ("Unparseable TensorShapeProto at index ", i)
             (: (
             string_literal: "Unparseable TensorShapeProto at index "
              ": "
              string_content: Unparseable TensorShapeProto at index 
              ": "
             ,: ,
             identifier: i
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       }: }
     expression_statement: desc->node_builder.Attr(attr_name, shapes);
      call_expression: desc->node_builder.Attr(attr_name, shapes)
       field_expression: desc->node_builder.Attr
        field_expression: desc->node_builder
         identifier: desc
         ->: ->
         field_identifier: node_builder
        .: .
        field_identifier: Attr
       argument_list: (attr_name, shapes)
        (: (
        identifier: attr_name
        ,: ,
        identifier: shapes
        ): )
      ;: ;
     expression_statement: status->status = absl::OkStatus();
      assignment_expression: status->status = absl::OkStatus()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: absl::OkStatus()
        qualified_identifier: absl::OkStatus
         namespace_identifier: absl
         ::: ::
         identifier: OkStatus
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttrTensor(TF_OperationDescription* desc, const char* attr_name,
                      TF_Tensor* value, TF_Status* status) {
  Tensor t;
  status->status = TF_TensorToTensor(value, &t);
  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}
    primitive_type: void
    function_declarator: TF_SetAttrTensor(TF_OperationDescription* desc, const char* attr_name,
                      TF_Tensor* value, TF_Status* status)
     identifier: TF_SetAttrTensor
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                      TF_Tensor* value, TF_Status* status)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Tensor* value
       type_identifier: TF_Tensor
       pointer_declarator: * value
        *: *
        identifier: value
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  Tensor t;
  status->status = TF_TensorToTensor(value, &t);
  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}
     {: {
     declaration: Tensor t;
      type_identifier: Tensor
      identifier: t
      ;: ;
     expression_statement: status->status = TF_TensorToTensor(value, &t);
      assignment_expression: status->status = TF_TensorToTensor(value, &t)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: TF_TensorToTensor(value, &t)
        identifier: TF_TensorToTensor
        argument_list: (value, &t)
         (: (
         identifier: value
         ,: ,
         pointer_expression: &t
          &: &
          identifier: t
         ): )
      ;: ;
     if_statement: if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
      if: if
      condition_clause: (status->status.ok())
       (: (
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
       ): )
      expression_statement: desc->node_builder.Attr(attr_name, t);
       call_expression: desc->node_builder.Attr(attr_name, t)
        field_expression: desc->node_builder.Attr
         field_expression: desc->node_builder
          identifier: desc
          ->: ->
          field_identifier: node_builder
         .: .
         field_identifier: Attr
        argument_list: (attr_name, t)
         (: (
         identifier: attr_name
         ,: ,
         identifier: t
         ): )
       ;: ;
     }: }
   function_definition: void TF_SetAttrTensorList(TF_OperationDescription* desc, const char* attr_name,
                          TF_Tensor* const* values, int num_values,
                          TF_Status* status) {
  status->status = absl::OkStatus();
  std::vector<Tensor> t;
  t.reserve(num_values);

  for (int i = 0; i < num_values && status->status.ok(); ++i) {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }

  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}
    primitive_type: void
    function_declarator: TF_SetAttrTensorList(TF_OperationDescription* desc, const char* attr_name,
                          TF_Tensor* const* values, int num_values,
                          TF_Status* status)
     identifier: TF_SetAttrTensorList
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                          TF_Tensor* const* values, int num_values,
                          TF_Status* status)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Tensor* const* values
       type_identifier: TF_Tensor
       pointer_declarator: * const* values
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * values
         *: *
         identifier: values
      ,: ,
      parameter_declaration: int num_values
       primitive_type: int
       identifier: num_values
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  status->status = absl::OkStatus();
  std::vector<Tensor> t;
  t.reserve(num_values);

  for (int i = 0; i < num_values && status->status.ok(); ++i) {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }

  if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
}
     {: {
     expression_statement: status->status = absl::OkStatus();
      assignment_expression: status->status = absl::OkStatus()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: absl::OkStatus()
        qualified_identifier: absl::OkStatus
         namespace_identifier: absl
         ::: ::
         identifier: OkStatus
        argument_list: ()
         (: (
         ): )
      ;: ;
     declaration: std::vector<Tensor> t;
      qualified_identifier: std::vector<Tensor>
       namespace_identifier: std
       ::: ::
       template_type: vector<Tensor>
        type_identifier: vector
        template_argument_list: <Tensor>
         <: <
         type_descriptor: Tensor
          type_identifier: Tensor
         >: >
      identifier: t
      ;: ;
     expression_statement: t.reserve(num_values);
      call_expression: t.reserve(num_values)
       field_expression: t.reserve
        identifier: t
        .: .
        field_identifier: reserve
       argument_list: (num_values)
        (: (
        identifier: num_values
        ): )
      ;: ;
     for_statement: for (int i = 0; i < num_values && status->status.ok(); ++i) {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < num_values && status->status.ok()
       binary_expression: i < num_values
        identifier: i
        <: <
        identifier: num_values
       &&: &&
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    Tensor v;
    status->status = TF_TensorToTensor(values[i], &v);
    t.emplace_back(v);
  }
       {: {
       declaration: Tensor v;
        type_identifier: Tensor
        identifier: v
        ;: ;
       expression_statement: status->status = TF_TensorToTensor(values[i], &v);
        assignment_expression: status->status = TF_TensorToTensor(values[i], &v)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: TF_TensorToTensor(values[i], &v)
          identifier: TF_TensorToTensor
          argument_list: (values[i], &v)
           (: (
           subscript_expression: values[i]
            identifier: values
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ,: ,
           pointer_expression: &v
            &: &
            identifier: v
           ): )
        ;: ;
       expression_statement: t.emplace_back(v);
        call_expression: t.emplace_back(v)
         field_expression: t.emplace_back
          identifier: t
          .: .
          field_identifier: emplace_back
         argument_list: (v)
          (: (
          identifier: v
          ): )
        ;: ;
       }: }
     if_statement: if (status->status.ok()) desc->node_builder.Attr(attr_name, t);
      if: if
      condition_clause: (status->status.ok())
       (: (
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
       ): )
      expression_statement: desc->node_builder.Attr(attr_name, t);
       call_expression: desc->node_builder.Attr(attr_name, t)
        field_expression: desc->node_builder.Attr
         field_expression: desc->node_builder
          identifier: desc
          ->: ->
          field_identifier: node_builder
         .: .
         field_identifier: Attr
        argument_list: (attr_name, t)
         (: (
         identifier: attr_name
         ,: ,
         identifier: t
         ): )
       ;: ;
     }: }
   function_definition: void TF_SetAttrValueProto(TF_OperationDescription* desc, const char* attr_name,
                          const void* proto, size_t proto_len,
                          TF_Status* status) {
  tensorflow::AttrValue attr_value;
  if (!attr_value.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }

  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  } else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }

  status->status = absl::OkStatus();
}
    primitive_type: void
    function_declarator: TF_SetAttrValueProto(TF_OperationDescription* desc, const char* attr_name,
                          const void* proto, size_t proto_len,
                          TF_Status* status)
     identifier: TF_SetAttrValueProto
     parameter_list: (TF_OperationDescription* desc, const char* attr_name,
                          const void* proto, size_t proto_len,
                          TF_Status* status)
      (: (
      parameter_declaration: TF_OperationDescription* desc
       type_identifier: TF_OperationDescription
       pointer_declarator: * desc
        *: *
        identifier: desc
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: const void* proto
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * proto
        *: *
        identifier: proto
      ,: ,
      parameter_declaration: size_t proto_len
       primitive_type: size_t
       identifier: proto_len
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  tensorflow::AttrValue attr_value;
  if (!attr_value.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }

  if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  } else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }

  status->status = absl::OkStatus();
}
     {: {
     declaration: tensorflow::AttrValue attr_value;
      qualified_identifier: tensorflow::AttrValue
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: AttrValue
      identifier: attr_value
      ;: ;
     if_statement: if (!attr_value.ParseFromArray(proto, proto_len)) {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }
      if: if
      condition_clause: (!attr_value.ParseFromArray(proto, proto_len))
       (: (
       unary_expression: !attr_value.ParseFromArray(proto, proto_len)
        !: !
        call_expression: attr_value.ParseFromArray(proto, proto_len)
         field_expression: attr_value.ParseFromArray
          identifier: attr_value
          .: .
          field_identifier: ParseFromArray
         argument_list: (proto, proto_len)
          (: (
          identifier: proto
          ,: ,
          identifier: proto_len
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Unparseable AttrValue proto");
    return;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Unparseable AttrValue proto");
        assignment_expression: status->status = InvalidArgument("Unparseable AttrValue proto")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Unparseable AttrValue proto")
          identifier: InvalidArgument
          argument_list: ("Unparseable AttrValue proto")
           (: (
           string_literal: "Unparseable AttrValue proto"
            ": "
            string_content: Unparseable AttrValue proto
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     if_statement: if (strcmp(attr_name, tensorflow::kColocationAttrName) == 0) {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  } else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }
      if: if
      condition_clause: (strcmp(attr_name, tensorflow::kColocationAttrName) == 0)
       (: (
       binary_expression: strcmp(attr_name, tensorflow::kColocationAttrName) == 0
        call_expression: strcmp(attr_name, tensorflow::kColocationAttrName)
         identifier: strcmp
         argument_list: (attr_name, tensorflow::kColocationAttrName)
          (: (
          identifier: attr_name
          ,: ,
          qualified_identifier: tensorflow::kColocationAttrName
           namespace_identifier: tensorflow
           ::: ::
           identifier: kColocationAttrName
          ): )
        ==: ==
        number_literal: 0
       ): )
      compound_statement: {
    if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
    desc->colocation_constraints.clear();
    for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
  }
       {: {
       if_statement: if (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET) {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
        if: if
        condition_clause: (attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET)
         (: (
         binary_expression: attr_value.value_case() != tensorflow::AttrValue::kList &&
        attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET
          binary_expression: attr_value.value_case() != tensorflow::AttrValue::kList
           call_expression: attr_value.value_case()
            field_expression: attr_value.value_case
             identifier: attr_value
             .: .
             field_identifier: value_case
            argument_list: ()
             (: (
             ): )
           !=: !=
           qualified_identifier: tensorflow::AttrValue::kList
            namespace_identifier: tensorflow
            ::: ::
            qualified_identifier: AttrValue::kList
             namespace_identifier: AttrValue
             ::: ::
             identifier: kList
          &&: &&
          binary_expression: attr_value.value_case() != tensorflow::AttrValue::VALUE_NOT_SET
           call_expression: attr_value.value_case()
            field_expression: attr_value.value_case
             identifier: attr_value
             .: .
             field_identifier: value_case
            argument_list: ()
             (: (
             ): )
           !=: !=
           qualified_identifier: tensorflow::AttrValue::VALUE_NOT_SET
            namespace_identifier: tensorflow
            ::: ::
            qualified_identifier: AttrValue::VALUE_NOT_SET
             namespace_identifier: AttrValue
             ::: ::
             identifier: VALUE_NOT_SET
         ): )
        compound_statement: {
      status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
      return;
    }
         {: {
         expression_statement: status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute");
          assignment_expression: status->status =
          InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute")
            identifier: InvalidArgument
            argument_list: ("Expected \"list\" field for \"",
                          tensorflow::kColocationAttrName, "\" attribute")
             (: (
             string_literal: "Expected \"list\" field for \""
              ": "
              string_content: Expected 
              escape_sequence: \"
              string_content: list
              escape_sequence: \"
              string_content:  field for 
              escape_sequence: \"
              ": "
             ,: ,
             qualified_identifier: tensorflow::kColocationAttrName
              namespace_identifier: tensorflow
              ::: ::
              identifier: kColocationAttrName
             ,: ,
             string_literal: "\" attribute"
              ": "
              escape_sequence: \"
              string_content:  attribute
              ": "
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       expression_statement: desc->colocation_constraints.clear();
        call_expression: desc->colocation_constraints.clear()
         field_expression: desc->colocation_constraints.clear
          field_expression: desc->colocation_constraints
           identifier: desc
           ->: ->
           field_identifier: colocation_constraints
          .: .
          field_identifier: clear
         argument_list: ()
          (: (
          ): )
        ;: ;
       for_range_loop: for (const string& location : attr_value.list().s()) {
      desc->colocation_constraints.insert(location);
    }
        for: for
        (: (
        type_qualifier: const
         const: const
        type_identifier: string
        reference_declarator: & location
         &: &
         identifier: location
        :: :
        call_expression: attr_value.list().s()
         field_expression: attr_value.list().s
          call_expression: attr_value.list()
           field_expression: attr_value.list
            identifier: attr_value
            .: .
            field_identifier: list
           argument_list: ()
            (: (
            ): )
          .: .
          field_identifier: s
         argument_list: ()
          (: (
          ): )
        ): )
        compound_statement: {
      desc->colocation_constraints.insert(location);
    }
         {: {
         expression_statement: desc->colocation_constraints.insert(location);
          call_expression: desc->colocation_constraints.insert(location)
           field_expression: desc->colocation_constraints.insert
            field_expression: desc->colocation_constraints
             identifier: desc
             ->: ->
             field_identifier: colocation_constraints
            .: .
            field_identifier: insert
           argument_list: (location)
            (: (
            identifier: location
            ): )
          ;: ;
         }: }
       }: }
      else_clause: else {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }
       else: else
       compound_statement: {
    desc->node_builder.Attr(attr_name, std::move(attr_value));
  }
        {: {
        expression_statement: desc->node_builder.Attr(attr_name, std::move(attr_value));
         call_expression: desc->node_builder.Attr(attr_name, std::move(attr_value))
          field_expression: desc->node_builder.Attr
           field_expression: desc->node_builder
            identifier: desc
            ->: ->
            field_identifier: node_builder
           .: .
           field_identifier: Attr
          argument_list: (attr_name, std::move(attr_value))
           (: (
           identifier: attr_name
           ,: ,
           call_expression: std::move(attr_value)
            qualified_identifier: std::move
             namespace_identifier: std
             ::: ::
             identifier: move
            argument_list: (attr_value)
             (: (
             identifier: attr_value
             ): )
           ): )
         ;: ;
        }: }
     expression_statement: status->status = absl::OkStatus();
      assignment_expression: status->status = absl::OkStatus()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: absl::OkStatus()
        qualified_identifier: absl::OkStatus
         namespace_identifier: absl
         ::: ::
         identifier: OkStatus
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   declaration: TF_Operation* TF_FinishOperationLocked(TF_OperationDescription* desc,
                                       TF_Status* status)
    type_identifier: TF_Operation
    pointer_declarator: * TF_FinishOperationLocked(TF_OperationDescription* desc,
                                       TF_Status* status)
     *: *
     function_declarator: TF_FinishOperationLocked(TF_OperationDescription* desc,
                                       TF_Status* status)
      identifier: TF_FinishOperationLocked
      parameter_list: (TF_OperationDescription* desc,
                                       TF_Status* status)
       (: (
       parameter_declaration: TF_OperationDescription* desc
        type_identifier: TF_OperationDescription
        pointer_declarator: * desc
         *: *
         identifier: desc
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    ;: 
   expression_statement: TF_EXCLUSIVE_LOCKS_REQUIRED(desc->graph->mu)
    call_expression: TF_EXCLUSIVE_LOCKS_REQUIRED(desc->graph->mu)
     identifier: TF_EXCLUSIVE_LOCKS_REQUIRED
     argument_list: (desc->graph->mu)
      (: (
      field_expression: desc->graph->mu
       field_expression: desc->graph
        identifier: desc
        ->: ->
        field_identifier: graph
       ->: ->
       field_identifier: mu
      ): )
    ;: 
   compound_statement: {
  Node* ret = nullptr;

  if (desc->graph->name_map.count(desc->node_builder.node_name())) {
    status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
  } else {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }

  delete desc;

  return ToOperation(ret);
}
    {: {
    declaration: Node* ret = nullptr;
     type_identifier: Node
     init_declarator: * ret = nullptr
      pointer_declarator: * ret
       *: *
       identifier: ret
      =: =
      null: nullptr
       nullptr: nullptr
     ;: ;
    if_statement: if (desc->graph->name_map.count(desc->node_builder.node_name())) {
    status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
  } else {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }
     if: if
     condition_clause: (desc->graph->name_map.count(desc->node_builder.node_name()))
      (: (
      call_expression: desc->graph->name_map.count(desc->node_builder.node_name())
       field_expression: desc->graph->name_map.count
        field_expression: desc->graph->name_map
         field_expression: desc->graph
          identifier: desc
          ->: ->
          field_identifier: graph
         ->: ->
         field_identifier: name_map
        .: .
        field_identifier: count
       argument_list: (desc->node_builder.node_name())
        (: (
        call_expression: desc->node_builder.node_name()
         field_expression: desc->node_builder.node_name
          field_expression: desc->node_builder
           identifier: desc
           ->: ->
           field_identifier: node_builder
          .: .
          field_identifier: node_name
         argument_list: ()
          (: (
          ): )
        ): )
      ): )
     compound_statement: {
    status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
  }
      {: {
      expression_statement: status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'");
       assignment_expression: status->status = InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: InvalidArgument("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'")
         identifier: InvalidArgument
         argument_list: ("Duplicate node name in graph: '",
                                     desc->node_builder.node_name(), "'")
          (: (
          string_literal: "Duplicate node name in graph: '"
           ": "
           string_content: Duplicate node name in graph: '
           ": "
          ,: ,
          call_expression: desc->node_builder.node_name()
           field_expression: desc->node_builder.node_name
            field_expression: desc->node_builder
             identifier: desc
             ->: ->
             field_identifier: node_builder
            .: .
            field_identifier: node_name
           argument_list: ()
            (: (
            ): )
          ,: ,
          string_literal: "'"
           ": "
           string_content: '
           ": "
          ): )
       ;: ;
      }: }
     else_clause: else {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }
      else: else
      compound_statement: {
    if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
    status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);

    if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
    if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
  }
       {: {
       if_statement: if (!desc->colocation_constraints.empty()) {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
        if: if
        condition_clause: (!desc->colocation_constraints.empty())
         (: (
         unary_expression: !desc->colocation_constraints.empty()
          !: !
          call_expression: desc->colocation_constraints.empty()
           field_expression: desc->colocation_constraints.empty
            field_expression: desc->colocation_constraints
             identifier: desc
             ->: ->
             field_identifier: colocation_constraints
            .: .
            field_identifier: empty
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
      desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
    }
         {: {
         expression_statement: desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()));
          call_expression: desc->node_builder.Attr(
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()))
           field_expression: desc->node_builder.Attr
            field_expression: desc->node_builder
             identifier: desc
             ->: ->
             field_identifier: node_builder
            .: .
            field_identifier: Attr
           argument_list: (
          tensorflow::kColocationAttrName,
          std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end()))
            (: (
            qualified_identifier: tensorflow::kColocationAttrName
             namespace_identifier: tensorflow
             ::: ::
             identifier: kColocationAttrName
            ,: ,
            call_expression: std::vector<string>(desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end())
             qualified_identifier: std::vector<string>
              namespace_identifier: std
              ::: ::
              template_function: vector<string>
               identifier: vector
               template_argument_list: <string>
                <: <
                type_descriptor: string
                 type_identifier: string
                >: >
             argument_list: (desc->colocation_constraints.begin(),
                              desc->colocation_constraints.end())
              (: (
              call_expression: desc->colocation_constraints.begin()
               field_expression: desc->colocation_constraints.begin
                field_expression: desc->colocation_constraints
                 identifier: desc
                 ->: ->
                 field_identifier: colocation_constraints
                .: .
                field_identifier: begin
               argument_list: ()
                (: (
                ): )
              ,: ,
              call_expression: desc->colocation_constraints.end()
               field_expression: desc->colocation_constraints.end
                field_expression: desc->colocation_constraints
                 identifier: desc
                 ->: ->
                 field_identifier: colocation_constraints
                .: .
                field_identifier: end
               argument_list: ()
                (: (
                ): )
              ): )
            ): )
          ;: ;
         }: }
       expression_statement: status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true);
        assignment_expression: status->status = desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: desc->node_builder.Finalize(&desc->graph->graph, &ret,
                                                 /*consume=*/true)
          field_expression: desc->node_builder.Finalize
           field_expression: desc->node_builder
            identifier: desc
            ->: ->
            field_identifier: node_builder
           .: .
           field_identifier: Finalize
          argument_list: (&desc->graph->graph, &ret,
                                                 /*consume=*/true)
           (: (
           pointer_expression: &desc->graph->graph
            &: &
            field_expression: desc->graph->graph
             field_expression: desc->graph
              identifier: desc
              ->: ->
              field_identifier: graph
             ->: ->
             field_identifier: graph
           ,: ,
           pointer_expression: &ret
            &: &
            identifier: ret
           ,: ,
           comment: /*consume=*/
           true: true
           ): )
        ;: ;
       if_statement: if (status->status.ok()) {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
        if: if
        condition_clause: (status->status.ok())
         (: (
         call_expression: status->status.ok()
          field_expression: status->status.ok
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           .: .
           field_identifier: ok
          argument_list: ()
           (: (
           ): )
         ): )
        compound_statement: {
      // Run shape inference function for newly added node.
      status->status = desc->graph->refiner.AddNode(ret);
    }
         {: {
         comment: // Run shape inference function for newly added node.
         expression_statement: status->status = desc->graph->refiner.AddNode(ret);
          assignment_expression: status->status = desc->graph->refiner.AddNode(ret)
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: desc->graph->refiner.AddNode(ret)
            field_expression: desc->graph->refiner.AddNode
             field_expression: desc->graph->refiner
              field_expression: desc->graph
               identifier: desc
               ->: ->
               field_identifier: graph
              ->: ->
              field_identifier: refiner
             .: .
             field_identifier: AddNode
            argument_list: (ret)
             (: (
             identifier: ret
             ): )
          ;: ;
         }: }
       if_statement: if (status->status.ok()) {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    } else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
        if: if
        condition_clause: (status->status.ok())
         (: (
         call_expression: status->status.ok()
          field_expression: status->status.ok
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           .: .
           field_identifier: ok
          argument_list: ()
           (: (
           ): )
         ): )
        compound_statement: {
      // Add the node to the name-to-node mapping.
      desc->graph->name_map[ret->name()] = ret;
    }
         {: {
         comment: // Add the node to the name-to-node mapping.
         expression_statement: desc->graph->name_map[ret->name()] = ret;
          assignment_expression: desc->graph->name_map[ret->name()] = ret
           subscript_expression: desc->graph->name_map[ret->name()]
            field_expression: desc->graph->name_map
             field_expression: desc->graph
              identifier: desc
              ->: ->
              field_identifier: graph
             ->: ->
             field_identifier: name_map
            subscript_argument_list: [ret->name()]
             [: [
             call_expression: ret->name()
              field_expression: ret->name
               identifier: ret
               ->: ->
               field_identifier: name
              argument_list: ()
               (: (
               ): )
             ]: ]
           =: =
           identifier: ret
          ;: ;
         }: }
        else_clause: else if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
         else: else
         if_statement: if (ret != nullptr) {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
          if: if
          condition_clause: (ret != nullptr)
           (: (
           binary_expression: ret != nullptr
            identifier: ret
            !=: !=
            null: nullptr
             nullptr: nullptr
           ): )
          compound_statement: {
      desc->graph->graph.RemoveNode(ret);
      ret = nullptr;
    }
           {: {
           expression_statement: desc->graph->graph.RemoveNode(ret);
            call_expression: desc->graph->graph.RemoveNode(ret)
             field_expression: desc->graph->graph.RemoveNode
              field_expression: desc->graph->graph
               field_expression: desc->graph
                identifier: desc
                ->: ->
                field_identifier: graph
               ->: ->
               field_identifier: graph
              .: .
              field_identifier: RemoveNode
             argument_list: (ret)
              (: (
              identifier: ret
              ): )
            ;: ;
           expression_statement: ret = nullptr;
            assignment_expression: ret = nullptr
             identifier: ret
             =: =
             null: nullptr
              nullptr: nullptr
            ;: ;
           }: }
       }: }
    expression_statement: delete desc;
     delete_expression: delete desc
      delete: delete
      identifier: desc
     ;: ;
    return_statement: return ToOperation(ret);
     return: return
     call_expression: ToOperation(ret)
      identifier: ToOperation
      argument_list: (ret)
       (: (
       identifier: ret
       ): )
     ;: ;
    }: }
   function_definition: TF_Operation* TF_FinishOperation(TF_OperationDescription* desc,
                                 TF_Status* status) {
  mutex_lock l(desc->graph->mu);
  return TF_FinishOperationLocked(desc, status);
}
    type_identifier: TF_Operation
    pointer_declarator: * TF_FinishOperation(TF_OperationDescription* desc,
                                 TF_Status* status)
     *: *
     function_declarator: TF_FinishOperation(TF_OperationDescription* desc,
                                 TF_Status* status)
      identifier: TF_FinishOperation
      parameter_list: (TF_OperationDescription* desc,
                                 TF_Status* status)
       (: (
       parameter_declaration: TF_OperationDescription* desc
        type_identifier: TF_OperationDescription
        pointer_declarator: * desc
         *: *
         identifier: desc
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  mutex_lock l(desc->graph->mu);
  return TF_FinishOperationLocked(desc, status);
}
     {: {
     declaration: mutex_lock l(desc->graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(desc->graph->mu)
       identifier: l
       argument_list: (desc->graph->mu)
        (: (
        field_expression: desc->graph->mu
         field_expression: desc->graph
          identifier: desc
          ->: ->
          field_identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     return_statement: return TF_FinishOperationLocked(desc, status);
      return: return
      call_expression: TF_FinishOperationLocked(desc, status)
       identifier: TF_FinishOperationLocked
       argument_list: (desc, status)
        (: (
        identifier: desc
        ,: ,
        identifier: status
        ): )
      ;: ;
     }: }
   comment: // TF_Operation functions
   comment: // ----------------------------------------------------------
   function_definition: const char* TF_OperationName(TF_Operation* oper) {
  return oper->node.name().c_str();
}
    type_qualifier: const
     const: const
    primitive_type: char
    pointer_declarator: * TF_OperationName(TF_Operation* oper)
     *: *
     function_declarator: TF_OperationName(TF_Operation* oper)
      identifier: TF_OperationName
      parameter_list: (TF_Operation* oper)
       (: (
       parameter_declaration: TF_Operation* oper
        type_identifier: TF_Operation
        pointer_declarator: * oper
         *: *
         identifier: oper
       ): )
    compound_statement: {
  return oper->node.name().c_str();
}
     {: {
     return_statement: return oper->node.name().c_str();
      return: return
      call_expression: oper->node.name().c_str()
       field_expression: oper->node.name().c_str
        call_expression: oper->node.name()
         field_expression: oper->node.name
          field_expression: oper->node
           identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: name
         argument_list: ()
          (: (
          ): )
        .: .
        field_identifier: c_str
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: const char* TF_OperationOpType(TF_Operation* oper) {
  return oper->node.type_string().c_str();
}
    type_qualifier: const
     const: const
    primitive_type: char
    pointer_declarator: * TF_OperationOpType(TF_Operation* oper)
     *: *
     function_declarator: TF_OperationOpType(TF_Operation* oper)
      identifier: TF_OperationOpType
      parameter_list: (TF_Operation* oper)
       (: (
       parameter_declaration: TF_Operation* oper
        type_identifier: TF_Operation
        pointer_declarator: * oper
         *: *
         identifier: oper
       ): )
    compound_statement: {
  return oper->node.type_string().c_str();
}
     {: {
     return_statement: return oper->node.type_string().c_str();
      return: return
      call_expression: oper->node.type_string().c_str()
       field_expression: oper->node.type_string().c_str
        call_expression: oper->node.type_string()
         field_expression: oper->node.type_string
          field_expression: oper->node
           identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: type_string
         argument_list: ()
          (: (
          ): )
        .: .
        field_identifier: c_str
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: const char* TF_OperationDevice(TF_Operation* oper) {
  return oper->node.requested_device().c_str();
}
    type_qualifier: const
     const: const
    primitive_type: char
    pointer_declarator: * TF_OperationDevice(TF_Operation* oper)
     *: *
     function_declarator: TF_OperationDevice(TF_Operation* oper)
      identifier: TF_OperationDevice
      parameter_list: (TF_Operation* oper)
       (: (
       parameter_declaration: TF_Operation* oper
        type_identifier: TF_Operation
        pointer_declarator: * oper
         *: *
         identifier: oper
       ): )
    compound_statement: {
  return oper->node.requested_device().c_str();
}
     {: {
     return_statement: return oper->node.requested_device().c_str();
      return: return
      call_expression: oper->node.requested_device().c_str()
       field_expression: oper->node.requested_device().c_str
        call_expression: oper->node.requested_device()
         field_expression: oper->node.requested_device
          field_expression: oper->node
           identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: requested_device
         argument_list: ()
          (: (
          ): )
        .: .
        field_identifier: c_str
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: int TF_OperationNumOutputs(TF_Operation* oper) {
  return oper->node.num_outputs();
}
    primitive_type: int
    function_declarator: TF_OperationNumOutputs(TF_Operation* oper)
     identifier: TF_OperationNumOutputs
     parameter_list: (TF_Operation* oper)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ): )
    compound_statement: {
  return oper->node.num_outputs();
}
     {: {
     return_statement: return oper->node.num_outputs();
      return: return
      call_expression: oper->node.num_outputs()
       field_expression: oper->node.num_outputs
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: num_outputs
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: TF_DataType TF_OperationOutputType(TF_Output oper_out) {
  return static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index));
}
    type_identifier: TF_DataType
    function_declarator: TF_OperationOutputType(TF_Output oper_out)
     identifier: TF_OperationOutputType
     parameter_list: (TF_Output oper_out)
      (: (
      parameter_declaration: TF_Output oper_out
       type_identifier: TF_Output
       identifier: oper_out
      ): )
    compound_statement: {
  return static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index));
}
     {: {
     return_statement: return static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index));
      return: return
      call_expression: static_cast<TF_DataType>(
      oper_out.oper->node.output_type(oper_out.index))
       template_function: static_cast<TF_DataType>
        identifier: static_cast
        template_argument_list: <TF_DataType>
         <: <
         type_descriptor: TF_DataType
          type_identifier: TF_DataType
         >: >
       argument_list: (
      oper_out.oper->node.output_type(oper_out.index))
        (: (
        call_expression: oper_out.oper->node.output_type(oper_out.index)
         field_expression: oper_out.oper->node.output_type
          field_expression: oper_out.oper->node
           field_expression: oper_out.oper
            identifier: oper_out
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: output_type
         argument_list: (oper_out.index)
          (: (
          field_expression: oper_out.index
           identifier: oper_out
           .: .
           field_identifier: index
          ): )
        ): )
      ;: ;
     }: }
   function_definition: int TF_OperationOutputListLength(TF_Operation* oper, const char* arg_name,
                                 TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}
    primitive_type: int
    function_declarator: TF_OperationOutputListLength(TF_Operation* oper, const char* arg_name,
                                 TF_Status* status)
     identifier: TF_OperationOutputListLength
     parameter_list: (TF_Operation* oper, const char* arg_name,
                                 TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* arg_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * arg_name
        *: *
        identifier: arg_name
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}
     {: {
     declaration: NameRangeMap name_ranges;
      type_identifier: NameRangeMap
      identifier: name_ranges
      ;: ;
     expression_statement: status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges);
      assignment_expression: status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: NameRangesForNode(oper->node, oper->node.op_def(), nullptr, &name_ranges)
        identifier: NameRangesForNode
        argument_list: (oper->node, oper->node.op_def(), nullptr, &name_ranges)
         (: (
         field_expression: oper->node
          identifier: oper
          ->: ->
          field_identifier: node
         ,: ,
         call_expression: oper->node.op_def()
          field_expression: oper->node.op_def
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: op_def
          argument_list: ()
           (: (
           ): )
         ,: ,
         null: nullptr
          nullptr: nullptr
         ,: ,
         pointer_expression: &name_ranges
          &: &
          identifier: name_ranges
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return -1;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return -1;
       return: return
       number_literal: -1
       ;: ;
     declaration: auto iter = name_ranges.find(arg_name);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: iter = name_ranges.find(arg_name)
       identifier: iter
       =: =
       call_expression: name_ranges.find(arg_name)
        field_expression: name_ranges.find
         identifier: name_ranges
         .: .
         field_identifier: find
        argument_list: (arg_name)
         (: (
         identifier: arg_name
         ): )
      ;: ;
     if_statement: if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
      if: if
      condition_clause: (iter == name_ranges.end())
       (: (
       binary_expression: iter == name_ranges.end()
        identifier: iter
        ==: ==
        call_expression: name_ranges.end()
         field_expression: name_ranges.end
          identifier: name_ranges
          .: .
          field_identifier: end
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Output arg '", arg_name, "' not found");
    return -1;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Output arg '", arg_name, "' not found");
        assignment_expression: status->status = InvalidArgument("Output arg '", arg_name, "' not found")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Output arg '", arg_name, "' not found")
          identifier: InvalidArgument
          argument_list: ("Output arg '", arg_name, "' not found")
           (: (
           string_literal: "Output arg '"
            ": "
            string_content: Output arg '
            ": "
           ,: ,
           identifier: arg_name
           ,: ,
           string_literal: "' not found"
            ": "
            string_content: ' not found
            ": "
           ): )
        ;: ;
       return_statement: return -1;
        return: return
        number_literal: -1
        ;: ;
       }: }
     return_statement: return iter->second.second - iter->second.first;
      return: return
      binary_expression: iter->second.second - iter->second.first
       field_expression: iter->second.second
        field_expression: iter->second
         identifier: iter
         ->: ->
         field_identifier: second
        .: .
        field_identifier: second
       -: -
       field_expression: iter->second.first
        field_expression: iter->second
         identifier: iter
         ->: ->
         field_identifier: second
        .: .
        field_identifier: first
      ;: ;
     }: }
   function_definition: int TF_OperationNumInputs(TF_Operation* oper) {
  return oper->node.num_inputs();
}
    primitive_type: int
    function_declarator: TF_OperationNumInputs(TF_Operation* oper)
     identifier: TF_OperationNumInputs
     parameter_list: (TF_Operation* oper)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ): )
    compound_statement: {
  return oper->node.num_inputs();
}
     {: {
     return_statement: return oper->node.num_inputs();
      return: return
      call_expression: oper->node.num_inputs()
       field_expression: oper->node.num_inputs
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: num_inputs
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: TF_DataType TF_OperationInputType(TF_Input oper_in) {
  return static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index));
}
    type_identifier: TF_DataType
    function_declarator: TF_OperationInputType(TF_Input oper_in)
     identifier: TF_OperationInputType
     parameter_list: (TF_Input oper_in)
      (: (
      parameter_declaration: TF_Input oper_in
       type_identifier: TF_Input
       identifier: oper_in
      ): )
    compound_statement: {
  return static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index));
}
     {: {
     return_statement: return static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index));
      return: return
      call_expression: static_cast<TF_DataType>(oper_in.oper->node.input_type(oper_in.index))
       template_function: static_cast<TF_DataType>
        identifier: static_cast
        template_argument_list: <TF_DataType>
         <: <
         type_descriptor: TF_DataType
          type_identifier: TF_DataType
         >: >
       argument_list: (oper_in.oper->node.input_type(oper_in.index))
        (: (
        call_expression: oper_in.oper->node.input_type(oper_in.index)
         field_expression: oper_in.oper->node.input_type
          field_expression: oper_in.oper->node
           field_expression: oper_in.oper
            identifier: oper_in
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: input_type
         argument_list: (oper_in.index)
          (: (
          field_expression: oper_in.index
           identifier: oper_in
           .: .
           field_identifier: index
          ): )
        ): )
      ;: ;
     }: }
   function_definition: int TF_OperationInputListLength(TF_Operation* oper, const char* arg_name,
                                TF_Status* status) {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}
    primitive_type: int
    function_declarator: TF_OperationInputListLength(TF_Operation* oper, const char* arg_name,
                                TF_Status* status)
     identifier: TF_OperationInputListLength
     parameter_list: (TF_Operation* oper, const char* arg_name,
                                TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* arg_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * arg_name
        *: *
        identifier: arg_name
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  NameRangeMap name_ranges;
  status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr);
  if (!status->status.ok()) return -1;
  auto iter = name_ranges.find(arg_name);
  if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
  return iter->second.second - iter->second.first;
}
     {: {
     declaration: NameRangeMap name_ranges;
      type_identifier: NameRangeMap
      identifier: name_ranges
      ;: ;
     expression_statement: status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr);
      assignment_expression: status->status =
      NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: NameRangesForNode(oper->node, oper->node.op_def(), &name_ranges, nullptr)
        identifier: NameRangesForNode
        argument_list: (oper->node, oper->node.op_def(), &name_ranges, nullptr)
         (: (
         field_expression: oper->node
          identifier: oper
          ->: ->
          field_identifier: node
         ,: ,
         call_expression: oper->node.op_def()
          field_expression: oper->node.op_def
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: op_def
          argument_list: ()
           (: (
           ): )
         ,: ,
         pointer_expression: &name_ranges
          &: &
          identifier: name_ranges
         ,: ,
         null: nullptr
          nullptr: nullptr
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return -1;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return -1;
       return: return
       number_literal: -1
       ;: ;
     declaration: auto iter = name_ranges.find(arg_name);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: iter = name_ranges.find(arg_name)
       identifier: iter
       =: =
       call_expression: name_ranges.find(arg_name)
        field_expression: name_ranges.find
         identifier: name_ranges
         .: .
         field_identifier: find
        argument_list: (arg_name)
         (: (
         identifier: arg_name
         ): )
      ;: ;
     if_statement: if (iter == name_ranges.end()) {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
      if: if
      condition_clause: (iter == name_ranges.end())
       (: (
       binary_expression: iter == name_ranges.end()
        identifier: iter
        ==: ==
        call_expression: name_ranges.end()
         field_expression: name_ranges.end
          identifier: name_ranges
          .: .
          field_identifier: end
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Input arg '", arg_name, "' not found");
    return -1;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Input arg '", arg_name, "' not found");
        assignment_expression: status->status = InvalidArgument("Input arg '", arg_name, "' not found")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Input arg '", arg_name, "' not found")
          identifier: InvalidArgument
          argument_list: ("Input arg '", arg_name, "' not found")
           (: (
           string_literal: "Input arg '"
            ": "
            string_content: Input arg '
            ": "
           ,: ,
           identifier: arg_name
           ,: ,
           string_literal: "' not found"
            ": "
            string_content: ' not found
            ": "
           ): )
        ;: ;
       return_statement: return -1;
        return: return
        number_literal: -1
        ;: ;
       }: }
     return_statement: return iter->second.second - iter->second.first;
      return: return
      binary_expression: iter->second.second - iter->second.first
       field_expression: iter->second.second
        field_expression: iter->second
         identifier: iter
         ->: ->
         field_identifier: second
        .: .
        field_identifier: second
       -: -
       field_expression: iter->second.first
        field_expression: iter->second
         identifier: iter
         ->: ->
         field_identifier: second
        .: .
        field_identifier: first
      ;: ;
     }: }
   function_definition: TF_Output TF_OperationInput(TF_Input oper_in) {
  const tensorflow::Edge* edge;
  Status s = oper_in.oper->node.input_edge(oper_in.index, &edge);
  if (!s.ok()) {
    return {nullptr, -1};
  }

  return {ToOperation(edge->src()), edge->src_output()};
}
    type_identifier: TF_Output
    function_declarator: TF_OperationInput(TF_Input oper_in)
     identifier: TF_OperationInput
     parameter_list: (TF_Input oper_in)
      (: (
      parameter_declaration: TF_Input oper_in
       type_identifier: TF_Input
       identifier: oper_in
      ): )
    compound_statement: {
  const tensorflow::Edge* edge;
  Status s = oper_in.oper->node.input_edge(oper_in.index, &edge);
  if (!s.ok()) {
    return {nullptr, -1};
  }

  return {ToOperation(edge->src()), edge->src_output()};
}
     {: {
     declaration: const tensorflow::Edge* edge;
      type_qualifier: const
       const: const
      qualified_identifier: tensorflow::Edge
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: Edge
      pointer_declarator: * edge
       *: *
       identifier: edge
      ;: ;
     declaration: Status s = oper_in.oper->node.input_edge(oper_in.index, &edge);
      type_identifier: Status
      init_declarator: s = oper_in.oper->node.input_edge(oper_in.index, &edge)
       identifier: s
       =: =
       call_expression: oper_in.oper->node.input_edge(oper_in.index, &edge)
        field_expression: oper_in.oper->node.input_edge
         field_expression: oper_in.oper->node
          field_expression: oper_in.oper
           identifier: oper_in
           .: .
           field_identifier: oper
          ->: ->
          field_identifier: node
         .: .
         field_identifier: input_edge
        argument_list: (oper_in.index, &edge)
         (: (
         field_expression: oper_in.index
          identifier: oper_in
          .: .
          field_identifier: index
         ,: ,
         pointer_expression: &edge
          &: &
          identifier: edge
         ): )
      ;: ;
     if_statement: if (!s.ok()) {
    return {nullptr, -1};
  }
      if: if
      condition_clause: (!s.ok())
       (: (
       unary_expression: !s.ok()
        !: !
        call_expression: s.ok()
         field_expression: s.ok
          identifier: s
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    return {nullptr, -1};
  }
       {: {
       return_statement: return {nullptr, -1};
        return: return
        initializer_list: {nullptr, -1}
         {: {
         null: nullptr
          nullptr: nullptr
         ,: ,
         number_literal: -1
         }: }
        ;: ;
       }: }
     return_statement: return {ToOperation(edge->src()), edge->src_output()};
      return: return
      initializer_list: {ToOperation(edge->src()), edge->src_output()}
       {: {
       call_expression: ToOperation(edge->src())
        identifier: ToOperation
        argument_list: (edge->src())
         (: (
         call_expression: edge->src()
          field_expression: edge->src
           identifier: edge
           ->: ->
           field_identifier: src
          argument_list: ()
           (: (
           ): )
         ): )
       ,: ,
       call_expression: edge->src_output()
        field_expression: edge->src_output
         identifier: edge
         ->: ->
         field_identifier: src_output
        argument_list: ()
         (: (
         ): )
       }: }
      ;: ;
     }: }
   function_definition: void TF_OperationAllInputs(TF_Operation* oper, TF_Output* inputs,
                           int max_inputs) {
  for (auto* edge : oper->node.in_edges()) {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
}
    primitive_type: void
    function_declarator: TF_OperationAllInputs(TF_Operation* oper, TF_Output* inputs,
                           int max_inputs)
     identifier: TF_OperationAllInputs
     parameter_list: (TF_Operation* oper, TF_Output* inputs,
                           int max_inputs)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: TF_Output* inputs
       type_identifier: TF_Output
       pointer_declarator: * inputs
        *: *
        identifier: inputs
      ,: ,
      parameter_declaration: int max_inputs
       primitive_type: int
       identifier: max_inputs
      ): )
    compound_statement: {
  for (auto* edge : oper->node.in_edges()) {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
}
     {: {
     for_range_loop: for (auto* edge : oper->node.in_edges()) {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
      for: for
      (: (
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper->node.in_edges()
       field_expression: oper->node.in_edges
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: in_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
  }
       {: {
       if_statement: if (edge->dst_input() >= 0 && edge->dst_input() < max_inputs) {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
        if: if
        condition_clause: (edge->dst_input() >= 0 && edge->dst_input() < max_inputs)
         (: (
         binary_expression: edge->dst_input() >= 0 && edge->dst_input() < max_inputs
          binary_expression: edge->dst_input() >= 0
           call_expression: edge->dst_input()
            field_expression: edge->dst_input
             identifier: edge
             ->: ->
             field_identifier: dst_input
            argument_list: ()
             (: (
             ): )
           >=: >=
           number_literal: 0
          &&: &&
          binary_expression: edge->dst_input() < max_inputs
           call_expression: edge->dst_input()
            field_expression: edge->dst_input
             identifier: edge
             ->: ->
             field_identifier: dst_input
            argument_list: ()
             (: (
             ): )
           <: <
           identifier: max_inputs
         ): )
        compound_statement: {
      inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
    }
         {: {
         expression_statement: inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()};
          assignment_expression: inputs[edge->dst_input()] = {ToOperation(edge->src()),
                                   edge->src_output()}
           subscript_expression: inputs[edge->dst_input()]
            identifier: inputs
            subscript_argument_list: [edge->dst_input()]
             [: [
             call_expression: edge->dst_input()
              field_expression: edge->dst_input
               identifier: edge
               ->: ->
               field_identifier: dst_input
              argument_list: ()
               (: (
               ): )
             ]: ]
           =: =
           initializer_list: {ToOperation(edge->src()),
                                   edge->src_output()}
            {: {
            call_expression: ToOperation(edge->src())
             identifier: ToOperation
             argument_list: (edge->src())
              (: (
              call_expression: edge->src()
               field_expression: edge->src
                identifier: edge
                ->: ->
                field_identifier: src
               argument_list: ()
                (: (
                ): )
              ): )
            ,: ,
            call_expression: edge->src_output()
             field_expression: edge->src_output
              identifier: edge
              ->: ->
              field_identifier: src_output
             argument_list: ()
              (: (
              ): )
            }: }
          ;: ;
         }: }
       }: }
     }: }
   function_definition: int TF_OperationOutputNumConsumers(TF_Output oper_out) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
  return count;
}
    primitive_type: int
    function_declarator: TF_OperationOutputNumConsumers(TF_Output oper_out)
     identifier: TF_OperationOutputNumConsumers
     parameter_list: (TF_Output oper_out)
      (: (
      parameter_declaration: TF_Output oper_out
       type_identifier: TF_Output
       identifier: oper_out
      ): )
    compound_statement: {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
  return count;
}
     {: {
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     for_range_loop: for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper_out.oper->node.out_edges()
       field_expression: oper_out.oper->node.out_edges
        field_expression: oper_out.oper->node
         field_expression: oper_out.oper
          identifier: oper_out
          .: .
          field_identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: out_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->src_output() == oper_out.index) {
      ++count;
    }
  }
       {: {
       if_statement: if (edge->src_output() == oper_out.index) {
      ++count;
    }
        if: if
        condition_clause: (edge->src_output() == oper_out.index)
         (: (
         binary_expression: edge->src_output() == oper_out.index
          call_expression: edge->src_output()
           field_expression: edge->src_output
            identifier: edge
            ->: ->
            field_identifier: src_output
           argument_list: ()
            (: (
            ): )
          ==: ==
          field_expression: oper_out.index
           identifier: oper_out
           .: .
           field_identifier: index
         ): )
        compound_statement: {
      ++count;
    }
         {: {
         expression_statement: ++count;
          update_expression: ++count
           ++: ++
           identifier: count
          ;: ;
         }: }
       }: }
     return_statement: return count;
      return: return
      identifier: count
      ;: ;
     }: }
   function_definition: int TF_OperationOutputConsumers(TF_Output oper_out, TF_Input* consumers,
                                int max_consumers) {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
  return count;
}
    primitive_type: int
    function_declarator: TF_OperationOutputConsumers(TF_Output oper_out, TF_Input* consumers,
                                int max_consumers)
     identifier: TF_OperationOutputConsumers
     parameter_list: (TF_Output oper_out, TF_Input* consumers,
                                int max_consumers)
      (: (
      parameter_declaration: TF_Output oper_out
       type_identifier: TF_Output
       identifier: oper_out
      ,: ,
      parameter_declaration: TF_Input* consumers
       type_identifier: TF_Input
       pointer_declarator: * consumers
        *: *
        identifier: consumers
      ,: ,
      parameter_declaration: int max_consumers
       primitive_type: int
       identifier: max_consumers
      ): )
    compound_statement: {
  int count = 0;
  for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
  return count;
}
     {: {
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     for_range_loop: for (const auto* edge : oper_out.oper->node.out_edges()) {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper_out.oper->node.out_edges()
       field_expression: oper_out.oper->node.out_edges
        field_expression: oper_out.oper->node
         field_expression: oper_out.oper
          identifier: oper_out
          .: .
          field_identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: out_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
  }
       {: {
       if_statement: if (edge->src_output() == oper_out.index) {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
        if: if
        condition_clause: (edge->src_output() == oper_out.index)
         (: (
         binary_expression: edge->src_output() == oper_out.index
          call_expression: edge->src_output()
           field_expression: edge->src_output
            identifier: edge
            ->: ->
            field_identifier: src_output
           argument_list: ()
            (: (
            ): )
          ==: ==
          field_expression: oper_out.index
           identifier: oper_out
           .: .
           field_identifier: index
         ): )
        compound_statement: {
      if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
      ++count;
    }
         {: {
         if_statement: if (count < max_consumers) {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
          if: if
          condition_clause: (count < max_consumers)
           (: (
           binary_expression: count < max_consumers
            identifier: count
            <: <
            identifier: max_consumers
           ): )
          compound_statement: {
        consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
      }
           {: {
           expression_statement: consumers[count] = {ToOperation(edge->dst()), edge->dst_input()};
            assignment_expression: consumers[count] = {ToOperation(edge->dst()), edge->dst_input()}
             subscript_expression: consumers[count]
              identifier: consumers
              subscript_argument_list: [count]
               [: [
               identifier: count
               ]: ]
             =: =
             initializer_list: {ToOperation(edge->dst()), edge->dst_input()}
              {: {
              call_expression: ToOperation(edge->dst())
               identifier: ToOperation
               argument_list: (edge->dst())
                (: (
                call_expression: edge->dst()
                 field_expression: edge->dst
                  identifier: edge
                  ->: ->
                  field_identifier: dst
                 argument_list: ()
                  (: (
                  ): )
                ): )
              ,: ,
              call_expression: edge->dst_input()
               field_expression: edge->dst_input
                identifier: edge
                ->: ->
                field_identifier: dst_input
               argument_list: ()
                (: (
                ): )
              }: }
            ;: ;
           }: }
         expression_statement: ++count;
          update_expression: ++count
           ++: ++
           identifier: count
          ;: ;
         }: }
       }: }
     return_statement: return count;
      return: return
      identifier: count
      ;: ;
     }: }
   function_definition: int TF_OperationNumControlInputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
  return count;
}
    primitive_type: int
    function_declarator: TF_OperationNumControlInputs(TF_Operation* oper)
     identifier: TF_OperationNumControlInputs
     parameter_list: (TF_Operation* oper)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ): )
    compound_statement: {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
  return count;
}
     {: {
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     for_range_loop: for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper->node.in_edges()
       field_expression: oper->node.in_edges
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: in_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
  }
       {: {
       if_statement: if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      ++count;
    }
        if: if
        condition_clause: (edge->IsControlEdge() && !edge->src()->IsSource())
         (: (
         binary_expression: edge->IsControlEdge() && !edge->src()->IsSource()
          call_expression: edge->IsControlEdge()
           field_expression: edge->IsControlEdge
            identifier: edge
            ->: ->
            field_identifier: IsControlEdge
           argument_list: ()
            (: (
            ): )
          &&: &&
          unary_expression: !edge->src()->IsSource()
           !: !
           call_expression: edge->src()->IsSource()
            field_expression: edge->src()->IsSource
             call_expression: edge->src()
              field_expression: edge->src
               identifier: edge
               ->: ->
               field_identifier: src
              argument_list: ()
               (: (
               ): )
             ->: ->
             field_identifier: IsSource
            argument_list: ()
             (: (
             ): )
         ): )
        compound_statement: {
      ++count;
    }
         {: {
         expression_statement: ++count;
          update_expression: ++count
           ++: ++
           identifier: count
          ;: ;
         }: }
       }: }
     return_statement: return count;
      return: return
      identifier: count
      ;: ;
     }: }
   function_definition: int TF_OperationGetControlInputs(TF_Operation* oper,
                                 TF_Operation** control_inputs,
                                 int max_control_inputs) {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
  return count;
}
    primitive_type: int
    function_declarator: TF_OperationGetControlInputs(TF_Operation* oper,
                                 TF_Operation** control_inputs,
                                 int max_control_inputs)
     identifier: TF_OperationGetControlInputs
     parameter_list: (TF_Operation* oper,
                                 TF_Operation** control_inputs,
                                 int max_control_inputs)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: TF_Operation** control_inputs
       type_identifier: TF_Operation
       pointer_declarator: ** control_inputs
        *: *
        pointer_declarator: * control_inputs
         *: *
         identifier: control_inputs
      ,: ,
      parameter_declaration: int max_control_inputs
       primitive_type: int
       identifier: max_control_inputs
      ): )
    compound_statement: {
  int count = 0;
  for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
  return count;
}
     {: {
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     for_range_loop: for (const auto* edge : oper->node.in_edges()) {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper->node.in_edges()
       field_expression: oper->node.in_edges
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: in_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
  }
       {: {
       if_statement: if (edge->IsControlEdge() && !edge->src()->IsSource()) {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
        if: if
        condition_clause: (edge->IsControlEdge() && !edge->src()->IsSource())
         (: (
         binary_expression: edge->IsControlEdge() && !edge->src()->IsSource()
          call_expression: edge->IsControlEdge()
           field_expression: edge->IsControlEdge
            identifier: edge
            ->: ->
            field_identifier: IsControlEdge
           argument_list: ()
            (: (
            ): )
          &&: &&
          unary_expression: !edge->src()->IsSource()
           !: !
           call_expression: edge->src()->IsSource()
            field_expression: edge->src()->IsSource
             call_expression: edge->src()
              field_expression: edge->src
               identifier: edge
               ->: ->
               field_identifier: src
              argument_list: ()
               (: (
               ): )
             ->: ->
             field_identifier: IsSource
            argument_list: ()
             (: (
             ): )
         ): )
        compound_statement: {
      if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
      ++count;
    }
         {: {
         if_statement: if (count < max_control_inputs) {
        control_inputs[count] = ToOperation(edge->src());
      }
          if: if
          condition_clause: (count < max_control_inputs)
           (: (
           binary_expression: count < max_control_inputs
            identifier: count
            <: <
            identifier: max_control_inputs
           ): )
          compound_statement: {
        control_inputs[count] = ToOperation(edge->src());
      }
           {: {
           expression_statement: control_inputs[count] = ToOperation(edge->src());
            assignment_expression: control_inputs[count] = ToOperation(edge->src())
             subscript_expression: control_inputs[count]
              identifier: control_inputs
              subscript_argument_list: [count]
               [: [
               identifier: count
               ]: ]
             =: =
             call_expression: ToOperation(edge->src())
              identifier: ToOperation
              argument_list: (edge->src())
               (: (
               call_expression: edge->src()
                field_expression: edge->src
                 identifier: edge
                 ->: ->
                 field_identifier: src
                argument_list: ()
                 (: (
                 ): )
               ): )
            ;: ;
           }: }
         expression_statement: ++count;
          update_expression: ++count
           ++: ++
           identifier: count
          ;: ;
         }: }
       }: }
     return_statement: return count;
      return: return
      identifier: count
      ;: ;
     }: }
   function_definition: int TF_OperationNumControlOutputs(TF_Operation* oper) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
  return count;
}
    primitive_type: int
    function_declarator: TF_OperationNumControlOutputs(TF_Operation* oper)
     identifier: TF_OperationNumControlOutputs
     parameter_list: (TF_Operation* oper)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ): )
    compound_statement: {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
  return count;
}
     {: {
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     for_range_loop: for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper->node.out_edges()
       field_expression: oper->node.out_edges
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: out_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
  }
       {: {
       if_statement: if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      ++count;
    }
        if: if
        condition_clause: (edge->IsControlEdge() && !edge->dst()->IsSink())
         (: (
         binary_expression: edge->IsControlEdge() && !edge->dst()->IsSink()
          call_expression: edge->IsControlEdge()
           field_expression: edge->IsControlEdge
            identifier: edge
            ->: ->
            field_identifier: IsControlEdge
           argument_list: ()
            (: (
            ): )
          &&: &&
          unary_expression: !edge->dst()->IsSink()
           !: !
           call_expression: edge->dst()->IsSink()
            field_expression: edge->dst()->IsSink
             call_expression: edge->dst()
              field_expression: edge->dst
               identifier: edge
               ->: ->
               field_identifier: dst
              argument_list: ()
               (: (
               ): )
             ->: ->
             field_identifier: IsSink
            argument_list: ()
             (: (
             ): )
         ): )
        compound_statement: {
      ++count;
    }
         {: {
         expression_statement: ++count;
          update_expression: ++count
           ++: ++
           identifier: count
          ;: ;
         }: }
       }: }
     return_statement: return count;
      return: return
      identifier: count
      ;: ;
     }: }
   function_definition: int TF_OperationGetControlOutputs(TF_Operation* oper,
                                  TF_Operation** control_outputs,
                                  int max_control_outputs) {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
  return count;
}
    primitive_type: int
    function_declarator: TF_OperationGetControlOutputs(TF_Operation* oper,
                                  TF_Operation** control_outputs,
                                  int max_control_outputs)
     identifier: TF_OperationGetControlOutputs
     parameter_list: (TF_Operation* oper,
                                  TF_Operation** control_outputs,
                                  int max_control_outputs)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: TF_Operation** control_outputs
       type_identifier: TF_Operation
       pointer_declarator: ** control_outputs
        *: *
        pointer_declarator: * control_outputs
         *: *
         identifier: control_outputs
      ,: ,
      parameter_declaration: int max_control_outputs
       primitive_type: int
       identifier: max_control_outputs
      ): )
    compound_statement: {
  int count = 0;
  for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
  return count;
}
     {: {
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     for_range_loop: for (const auto* edge : oper->node.out_edges()) {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: oper->node.out_edges()
       field_expression: oper->node.out_edges
        field_expression: oper->node
         identifier: oper
         ->: ->
         field_identifier: node
        .: .
        field_identifier: out_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
  }
       {: {
       if_statement: if (edge->IsControlEdge() && !edge->dst()->IsSink()) {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
        if: if
        condition_clause: (edge->IsControlEdge() && !edge->dst()->IsSink())
         (: (
         binary_expression: edge->IsControlEdge() && !edge->dst()->IsSink()
          call_expression: edge->IsControlEdge()
           field_expression: edge->IsControlEdge
            identifier: edge
            ->: ->
            field_identifier: IsControlEdge
           argument_list: ()
            (: (
            ): )
          &&: &&
          unary_expression: !edge->dst()->IsSink()
           !: !
           call_expression: edge->dst()->IsSink()
            field_expression: edge->dst()->IsSink
             call_expression: edge->dst()
              field_expression: edge->dst
               identifier: edge
               ->: ->
               field_identifier: dst
              argument_list: ()
               (: (
               ): )
             ->: ->
             field_identifier: IsSink
            argument_list: ()
             (: (
             ): )
         ): )
        compound_statement: {
      if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
      ++count;
    }
         {: {
         if_statement: if (count < max_control_outputs) {
        control_outputs[count] = ToOperation(edge->dst());
      }
          if: if
          condition_clause: (count < max_control_outputs)
           (: (
           binary_expression: count < max_control_outputs
            identifier: count
            <: <
            identifier: max_control_outputs
           ): )
          compound_statement: {
        control_outputs[count] = ToOperation(edge->dst());
      }
           {: {
           expression_statement: control_outputs[count] = ToOperation(edge->dst());
            assignment_expression: control_outputs[count] = ToOperation(edge->dst())
             subscript_expression: control_outputs[count]
              identifier: control_outputs
              subscript_argument_list: [count]
               [: [
               identifier: count
               ]: ]
             =: =
             call_expression: ToOperation(edge->dst())
              identifier: ToOperation
              argument_list: (edge->dst())
               (: (
               call_expression: edge->dst()
                field_expression: edge->dst
                 identifier: edge
                 ->: ->
                 field_identifier: dst
                argument_list: ()
                 (: (
                 ): )
               ): )
            ;: ;
           }: }
         expression_statement: ++count;
          update_expression: ++count
           ++: ++
           identifier: count
          ;: ;
         }: }
       }: }
     return_statement: return count;
      return: return
      identifier: count
      ;: ;
     }: }
   function_definition: TF_AttrMetadata TF_OperationGetAttrMetadata(TF_Operation* oper,
                                            const char* attr_name,
                                            TF_Status* status) {
  TF_AttrMetadata metadata;
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return metadata;
  switch (attr->value_case()) {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
  return metadata;
}
    type_identifier: TF_AttrMetadata
    function_declarator: TF_OperationGetAttrMetadata(TF_Operation* oper,
                                            const char* attr_name,
                                            TF_Status* status)
     identifier: TF_OperationGetAttrMetadata
     parameter_list: (TF_Operation* oper,
                                            const char* attr_name,
                                            TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  TF_AttrMetadata metadata;
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return metadata;
  switch (attr->value_case()) {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
  return metadata;
}
     {: {
     declaration: TF_AttrMetadata metadata;
      type_identifier: TF_AttrMetadata
      identifier: metadata
      ;: ;
     declaration: const auto* attr = GetAttrValue(oper, attr_name, status);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: * attr = GetAttrValue(oper, attr_name, status)
       pointer_declarator: * attr
        *: *
        identifier: attr
       =: =
       call_expression: GetAttrValue(oper, attr_name, status)
        identifier: GetAttrValue
        argument_list: (oper, attr_name, status)
         (: (
         identifier: oper
         ,: ,
         identifier: attr_name
         ,: ,
         identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return metadata;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return metadata;
       return: return
       identifier: metadata
       ;: ;
     switch_statement: switch (attr->value_case()) {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
      switch: switch
      condition_clause: (attr->value_case())
       (: (
       call_expression: attr->value_case()
        field_expression: attr->value_case
         identifier: attr
         ->: ->
         field_identifier: value_case
        argument_list: ()
         (: (
         ): )
       ): )
      compound_statement: {
#define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

    SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
    SINGLE_CASE(kI, TF_ATTR_INT, -1);
    SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
    SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
    SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
    SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
    SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
#undef SINGLE_CASE

    case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
#define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

      LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
          for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); });
      LIST_CASE(i, TF_ATTR_INT);
      LIST_CASE(f, TF_ATTR_FLOAT);
      LIST_CASE(b, TF_ATTR_BOOL);
      LIST_CASE(type, TF_ATTR_TYPE);
      LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
          for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          });
      LIST_CASE(tensor, TF_ATTR_TENSOR);
      LIST_CASE(tensor, TF_ATTR_FUNC);
#undef LIST_CASE
      // All lists empty, determine the type from the OpDef.
      if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
      break;

    case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;

    case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
  }
       {: {
       preproc_function_def: #define SINGLE_CASE(kK, attr_type, size_expr) \
  case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;

        #define: #define
        identifier: SINGLE_CASE
        preproc_params: (kK, attr_type, size_expr)
         (: (
         identifier: kK
         ,: ,
         identifier: attr_type
         ,: ,
         identifier: size_expr
         ): )
        preproc_arg: case tensorflow::AttrValue::kK:             \
    metadata.is_list = 0;                     \
    metadata.list_size = -1;                  \
    metadata.type = attr_type;                \
    metadata.total_size = size_expr;          \
    break;
       expression_statement: SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length());
        call_expression: SINGLE_CASE(kS, TF_ATTR_STRING, attr->s().length())
         identifier: SINGLE_CASE
         argument_list: (kS, TF_ATTR_STRING, attr->s().length())
          (: (
          identifier: kS
          ,: ,
          identifier: TF_ATTR_STRING
          ,: ,
          call_expression: attr->s().length()
           field_expression: attr->s().length
            call_expression: attr->s()
             field_expression: attr->s
              identifier: attr
              ->: ->
              field_identifier: s
             argument_list: ()
              (: (
              ): )
            .: .
            field_identifier: length
           argument_list: ()
            (: (
            ): )
          ): )
        ;: ;
       expression_statement: SINGLE_CASE(kI, TF_ATTR_INT, -1);
        call_expression: SINGLE_CASE(kI, TF_ATTR_INT, -1)
         identifier: SINGLE_CASE
         argument_list: (kI, TF_ATTR_INT, -1)
          (: (
          identifier: kI
          ,: ,
          identifier: TF_ATTR_INT
          ,: ,
          number_literal: -1
          ): )
        ;: ;
       expression_statement: SINGLE_CASE(kF, TF_ATTR_FLOAT, -1);
        call_expression: SINGLE_CASE(kF, TF_ATTR_FLOAT, -1)
         identifier: SINGLE_CASE
         argument_list: (kF, TF_ATTR_FLOAT, -1)
          (: (
          identifier: kF
          ,: ,
          identifier: TF_ATTR_FLOAT
          ,: ,
          number_literal: -1
          ): )
        ;: ;
       expression_statement: SINGLE_CASE(kB, TF_ATTR_BOOL, -1);
        call_expression: SINGLE_CASE(kB, TF_ATTR_BOOL, -1)
         identifier: SINGLE_CASE
         argument_list: (kB, TF_ATTR_BOOL, -1)
          (: (
          identifier: kB
          ,: ,
          identifier: TF_ATTR_BOOL
          ,: ,
          number_literal: -1
          ): )
        ;: ;
       expression_statement: SINGLE_CASE(kType, TF_ATTR_TYPE, -1);
        call_expression: SINGLE_CASE(kType, TF_ATTR_TYPE, -1)
         identifier: SINGLE_CASE
         argument_list: (kType, TF_ATTR_TYPE, -1)
          (: (
          identifier: kType
          ,: ,
          identifier: TF_ATTR_TYPE
          ,: ,
          number_literal: -1
          ): )
        ;: ;
       expression_statement: SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size());
        call_expression: SINGLE_CASE(kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size())
         identifier: SINGLE_CASE
         argument_list: (kShape, TF_ATTR_SHAPE,
                attr->shape().unknown_rank() ? -1 : attr->shape().dim_size())
          (: (
          identifier: kShape
          ,: ,
          identifier: TF_ATTR_SHAPE
          ,: ,
          conditional_expression: attr->shape().unknown_rank() ? -1 : attr->shape().dim_size()
           call_expression: attr->shape().unknown_rank()
            field_expression: attr->shape().unknown_rank
             call_expression: attr->shape()
              field_expression: attr->shape
               identifier: attr
               ->: ->
               field_identifier: shape
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: unknown_rank
            argument_list: ()
             (: (
             ): )
           ?: ?
           number_literal: -1
           :: :
           call_expression: attr->shape().dim_size()
            field_expression: attr->shape().dim_size
             call_expression: attr->shape()
              field_expression: attr->shape
               identifier: attr
               ->: ->
               field_identifier: shape
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: dim_size
            argument_list: ()
             (: (
             ): )
          ): )
        ;: ;
       expression_statement: SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1);
        call_expression: SINGLE_CASE(kTensor, TF_ATTR_TENSOR, -1)
         identifier: SINGLE_CASE
         argument_list: (kTensor, TF_ATTR_TENSOR, -1)
          (: (
          identifier: kTensor
          ,: ,
          identifier: TF_ATTR_TENSOR
          ,: ,
          number_literal: -1
          ): )
        ;: ;
       preproc_call: #undef SINGLE_CASE

        preproc_directive: #undef
        preproc_arg: SINGLE_CASE
       case_statement: case tensorflow::AttrValue::kList:
      metadata.is_list = 1;
      metadata.list_size = 0;
      metadata.total_size = -1;
        case: case
        qualified_identifier: tensorflow::AttrValue::kList
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kList
          namespace_identifier: AttrValue
          ::: ::
          identifier: kList
        :: :
        expression_statement: metadata.is_list = 1;
         assignment_expression: metadata.is_list = 1
          field_expression: metadata.is_list
           identifier: metadata
           .: .
           field_identifier: is_list
          =: =
          number_literal: 1
         ;: ;
        expression_statement: metadata.list_size = 0;
         assignment_expression: metadata.list_size = 0
          field_expression: metadata.list_size
           identifier: metadata
           .: .
           field_identifier: list_size
          =: =
          number_literal: 0
         ;: ;
        expression_statement: metadata.total_size = -1;
         assignment_expression: metadata.total_size = -1
          field_expression: metadata.total_size
           identifier: metadata
           .: .
           field_identifier: total_size
          =: =
          number_literal: -1
         ;: ;
       preproc_function_def: #define LIST_CASE(field, attr_type, ...)              \
  if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }

        #define: #define
        identifier: LIST_CASE
        preproc_params: (field, attr_type, ...)
         (: (
         identifier: field
         ,: ,
         identifier: attr_type
         ,: ,
         ...: ...
         ): )
        preproc_arg: if (attr->list().field##_size() > 0) {              \
    metadata.type = attr_type;                        \
    metadata.list_size = attr->list().field##_size(); \
    __VA_ARGS__;                                      \
    break;                                            \
  }
       expression_statement: LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0;
        call_expression: LIST_CASE(
          s, TF_ATTR_STRING, metadata.total_size = 0
         identifier: LIST_CASE
         argument_list: (
          s, TF_ATTR_STRING, metadata.total_size = 0
          (: (
          identifier: s
          ,: ,
          identifier: TF_ATTR_STRING
          ,: ,
          assignment_expression: metadata.total_size = 0
           field_expression: metadata.total_size
            identifier: metadata
            .: .
            field_identifier: total_size
           =: =
           number_literal: 0
          ): 
        ;: ;
       for_statement: for (int i = 0; i < attr->list().s_size();
               ++i) { metadata.total_size += attr->list().s(i).size(); }
        for: for
        (: (
        declaration: int i = 0;
         primitive_type: int
         init_declarator: i = 0
          identifier: i
          =: =
          number_literal: 0
         ;: ;
        binary_expression: i < attr->list().s_size()
         identifier: i
         <: <
         call_expression: attr->list().s_size()
          field_expression: attr->list().s_size
           call_expression: attr->list()
            field_expression: attr->list
             identifier: attr
             ->: ->
             field_identifier: list
            argument_list: ()
             (: (
             ): )
           .: .
           field_identifier: s_size
          argument_list: ()
           (: (
           ): )
        ;: ;
        update_expression: ++i
         ++: ++
         identifier: i
        ): )
        compound_statement: { metadata.total_size += attr->list().s(i).size(); }
         {: {
         expression_statement: metadata.total_size += attr->list().s(i).size();
          assignment_expression: metadata.total_size += attr->list().s(i).size()
           field_expression: metadata.total_size
            identifier: metadata
            .: .
            field_identifier: total_size
           +=: +=
           call_expression: attr->list().s(i).size()
            field_expression: attr->list().s(i).size
             call_expression: attr->list().s(i)
              field_expression: attr->list().s
               call_expression: attr->list()
                field_expression: attr->list
                 identifier: attr
                 ->: ->
                 field_identifier: list
                argument_list: ()
                 (: (
                 ): )
               .: .
               field_identifier: s
              argument_list: (i)
               (: (
               identifier: i
               ): )
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
          ;: ;
         }: }
       ERROR: )
        ): )
       expression_statement: ;
        ;: ;
       expression_statement: LIST_CASE(i, TF_ATTR_INT);
        call_expression: LIST_CASE(i, TF_ATTR_INT)
         identifier: LIST_CASE
         argument_list: (i, TF_ATTR_INT)
          (: (
          identifier: i
          ,: ,
          identifier: TF_ATTR_INT
          ): )
        ;: ;
       expression_statement: LIST_CASE(f, TF_ATTR_FLOAT);
        call_expression: LIST_CASE(f, TF_ATTR_FLOAT)
         identifier: LIST_CASE
         argument_list: (f, TF_ATTR_FLOAT)
          (: (
          identifier: f
          ,: ,
          identifier: TF_ATTR_FLOAT
          ): )
        ;: ;
       expression_statement: LIST_CASE(b, TF_ATTR_BOOL);
        call_expression: LIST_CASE(b, TF_ATTR_BOOL)
         identifier: LIST_CASE
         argument_list: (b, TF_ATTR_BOOL)
          (: (
          identifier: b
          ,: ,
          identifier: TF_ATTR_BOOL
          ): )
        ;: ;
       expression_statement: LIST_CASE(type, TF_ATTR_TYPE);
        call_expression: LIST_CASE(type, TF_ATTR_TYPE)
         identifier: LIST_CASE
         argument_list: (type, TF_ATTR_TYPE)
          (: (
          identifier: type
          ,: ,
          identifier: TF_ATTR_TYPE
          ): )
        ;: ;
       expression_statement: LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0;
        call_expression: LIST_CASE(
          shape, TF_ATTR_SHAPE, metadata.total_size = 0
         identifier: LIST_CASE
         argument_list: (
          shape, TF_ATTR_SHAPE, metadata.total_size = 0
          (: (
          identifier: shape
          ,: ,
          identifier: TF_ATTR_SHAPE
          ,: ,
          assignment_expression: metadata.total_size = 0
           field_expression: metadata.total_size
            identifier: metadata
            .: .
            field_identifier: total_size
           =: =
           number_literal: 0
          ): 
        ;: ;
       for_statement: for (int i = 0; i < attr->list().shape_size(); ++i) {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          }
        for: for
        (: (
        declaration: int i = 0;
         primitive_type: int
         init_declarator: i = 0
          identifier: i
          =: =
          number_literal: 0
         ;: ;
        binary_expression: i < attr->list().shape_size()
         identifier: i
         <: <
         call_expression: attr->list().shape_size()
          field_expression: attr->list().shape_size
           call_expression: attr->list()
            field_expression: attr->list
             identifier: attr
             ->: ->
             field_identifier: list
            argument_list: ()
             (: (
             ): )
           .: .
           field_identifier: shape_size
          argument_list: ()
           (: (
           ): )
        ;: ;
        update_expression: ++i
         ++: ++
         identifier: i
        ): )
        compound_statement: {
            const auto& s = attr->list().shape(i);
            metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          }
         {: {
         declaration: const auto& s = attr->list().shape(i);
          type_qualifier: const
           const: const
          placeholder_type_specifier: auto
           auto: auto
          init_declarator: & s = attr->list().shape(i)
           reference_declarator: & s
            &: &
            identifier: s
           =: =
           call_expression: attr->list().shape(i)
            field_expression: attr->list().shape
             call_expression: attr->list()
              field_expression: attr->list
               identifier: attr
               ->: ->
               field_identifier: list
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: shape
            argument_list: (i)
             (: (
             identifier: i
             ): )
          ;: ;
         expression_statement: metadata.total_size += s.unknown_rank() ? 0 : s.dim_size();
          assignment_expression: metadata.total_size += s.unknown_rank() ? 0 : s.dim_size()
           field_expression: metadata.total_size
            identifier: metadata
            .: .
            field_identifier: total_size
           +=: +=
           conditional_expression: s.unknown_rank() ? 0 : s.dim_size()
            call_expression: s.unknown_rank()
             field_expression: s.unknown_rank
              identifier: s
              .: .
              field_identifier: unknown_rank
             argument_list: ()
              (: (
              ): )
            ?: ?
            number_literal: 0
            :: :
            call_expression: s.dim_size()
             field_expression: s.dim_size
              identifier: s
              .: .
              field_identifier: dim_size
             argument_list: ()
              (: (
              ): )
          ;: ;
         }: }
       ERROR: )
        ): )
       expression_statement: ;
        ;: ;
       expression_statement: LIST_CASE(tensor, TF_ATTR_TENSOR);
        call_expression: LIST_CASE(tensor, TF_ATTR_TENSOR)
         identifier: LIST_CASE
         argument_list: (tensor, TF_ATTR_TENSOR)
          (: (
          identifier: tensor
          ,: ,
          identifier: TF_ATTR_TENSOR
          ): )
        ;: ;
       expression_statement: LIST_CASE(tensor, TF_ATTR_FUNC);
        call_expression: LIST_CASE(tensor, TF_ATTR_FUNC)
         identifier: LIST_CASE
         argument_list: (tensor, TF_ATTR_FUNC)
          (: (
          identifier: tensor
          ,: ,
          identifier: TF_ATTR_FUNC
          ): )
        ;: ;
       preproc_call: #undef LIST_CASE

        preproc_directive: #undef
        preproc_arg: LIST_CASE
       comment: // All lists empty, determine the type from the OpDef.
       if_statement: if (metadata.list_size == 0) {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
        if: if
        condition_clause: (metadata.list_size == 0)
         (: (
         binary_expression: metadata.list_size == 0
          field_expression: metadata.list_size
           identifier: metadata
           .: .
           field_identifier: list_size
          ==: ==
          number_literal: 0
         ): )
        compound_statement: {
        for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
      }
         {: {
         for_statement: for (int i = 0; i < oper->node.op_def().attr_size(); ++i) {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
          for: for
          (: (
          declaration: int i = 0;
           primitive_type: int
           init_declarator: i = 0
            identifier: i
            =: =
            number_literal: 0
           ;: ;
          binary_expression: i < oper->node.op_def().attr_size()
           identifier: i
           <: <
           call_expression: oper->node.op_def().attr_size()
            field_expression: oper->node.op_def().attr_size
             call_expression: oper->node.op_def()
              field_expression: oper->node.op_def
               field_expression: oper->node
                identifier: oper
                ->: ->
                field_identifier: node
               .: .
               field_identifier: op_def
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: attr_size
            argument_list: ()
             (: (
             ): )
          ;: ;
          update_expression: ++i
           ++: ++
           identifier: i
          ): )
          compound_statement: {
          const auto& a = oper->node.op_def().attr(i);
          if (a.name() != attr_name) continue;
          const string& typestr = a.type();
          if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
        }
           {: {
           declaration: const auto& a = oper->node.op_def().attr(i);
            type_qualifier: const
             const: const
            placeholder_type_specifier: auto
             auto: auto
            init_declarator: & a = oper->node.op_def().attr(i)
             reference_declarator: & a
              &: &
              identifier: a
             =: =
             call_expression: oper->node.op_def().attr(i)
              field_expression: oper->node.op_def().attr
               call_expression: oper->node.op_def()
                field_expression: oper->node.op_def
                 field_expression: oper->node
                  identifier: oper
                  ->: ->
                  field_identifier: node
                 .: .
                 field_identifier: op_def
                argument_list: ()
                 (: (
                 ): )
               .: .
               field_identifier: attr
              argument_list: (i)
               (: (
               identifier: i
               ): )
            ;: ;
           if_statement: if (a.name() != attr_name) continue;
            if: if
            condition_clause: (a.name() != attr_name)
             (: (
             binary_expression: a.name() != attr_name
              call_expression: a.name()
               field_expression: a.name
                identifier: a
                .: .
                field_identifier: name
               argument_list: ()
                (: (
                ): )
              !=: !=
              identifier: attr_name
             ): )
            continue_statement: continue;
             continue: continue
             ;: ;
           declaration: const string& typestr = a.type();
            type_qualifier: const
             const: const
            type_identifier: string
            init_declarator: & typestr = a.type()
             reference_declarator: & typestr
              &: &
              identifier: typestr
             =: =
             call_expression: a.type()
              field_expression: a.type
               identifier: a
               .: .
               field_identifier: type
              argument_list: ()
               (: (
               ): )
            ;: ;
           if_statement: if (typestr == "list(string)") {
            metadata.type = TF_ATTR_STRING;
          } else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
            if: if
            condition_clause: (typestr == "list(string)")
             (: (
             binary_expression: typestr == "list(string)"
              identifier: typestr
              ==: ==
              string_literal: "list(string)"
               ": "
               string_content: list(string)
               ": "
             ): )
            compound_statement: {
            metadata.type = TF_ATTR_STRING;
          }
             {: {
             expression_statement: metadata.type = TF_ATTR_STRING;
              assignment_expression: metadata.type = TF_ATTR_STRING
               field_expression: metadata.type
                identifier: metadata
                .: .
                field_identifier: type
               =: =
               identifier: TF_ATTR_STRING
              ;: ;
             }: }
            else_clause: else if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
             else: else
             if_statement: if (typestr == "list(int)") {
            metadata.type = TF_ATTR_INT;
          } else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
              if: if
              condition_clause: (typestr == "list(int)")
               (: (
               binary_expression: typestr == "list(int)"
                identifier: typestr
                ==: ==
                string_literal: "list(int)"
                 ": "
                 string_content: list(int)
                 ": "
               ): )
              compound_statement: {
            metadata.type = TF_ATTR_INT;
          }
               {: {
               expression_statement: metadata.type = TF_ATTR_INT;
                assignment_expression: metadata.type = TF_ATTR_INT
                 field_expression: metadata.type
                  identifier: metadata
                  .: .
                  field_identifier: type
                 =: =
                 identifier: TF_ATTR_INT
                ;: ;
               }: }
              else_clause: else if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
               else: else
               if_statement: if (typestr == "list(float)") {
            metadata.type = TF_ATTR_FLOAT;
          } else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                if: if
                condition_clause: (typestr == "list(float)")
                 (: (
                 binary_expression: typestr == "list(float)"
                  identifier: typestr
                  ==: ==
                  string_literal: "list(float)"
                   ": "
                   string_content: list(float)
                   ": "
                 ): )
                compound_statement: {
            metadata.type = TF_ATTR_FLOAT;
          }
                 {: {
                 expression_statement: metadata.type = TF_ATTR_FLOAT;
                  assignment_expression: metadata.type = TF_ATTR_FLOAT
                   field_expression: metadata.type
                    identifier: metadata
                    .: .
                    field_identifier: type
                   =: =
                   identifier: TF_ATTR_FLOAT
                  ;: ;
                 }: }
                else_clause: else if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                 else: else
                 if_statement: if (typestr == "list(bool)") {
            metadata.type = TF_ATTR_BOOL;
          } else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                  if: if
                  condition_clause: (typestr == "list(bool)")
                   (: (
                   binary_expression: typestr == "list(bool)"
                    identifier: typestr
                    ==: ==
                    string_literal: "list(bool)"
                     ": "
                     string_content: list(bool)
                     ": "
                   ): )
                  compound_statement: {
            metadata.type = TF_ATTR_BOOL;
          }
                   {: {
                   expression_statement: metadata.type = TF_ATTR_BOOL;
                    assignment_expression: metadata.type = TF_ATTR_BOOL
                     field_expression: metadata.type
                      identifier: metadata
                      .: .
                      field_identifier: type
                     =: =
                     identifier: TF_ATTR_BOOL
                    ;: ;
                   }: }
                  else_clause: else if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                   else: else
                   if_statement: if (typestr == "list(type)") {
            metadata.type = TF_ATTR_TYPE;
          } else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                    if: if
                    condition_clause: (typestr == "list(type)")
                     (: (
                     binary_expression: typestr == "list(type)"
                      identifier: typestr
                      ==: ==
                      string_literal: "list(type)"
                       ": "
                       string_content: list(type)
                       ": "
                     ): )
                    compound_statement: {
            metadata.type = TF_ATTR_TYPE;
          }
                     {: {
                     expression_statement: metadata.type = TF_ATTR_TYPE;
                      assignment_expression: metadata.type = TF_ATTR_TYPE
                       field_expression: metadata.type
                        identifier: metadata
                        .: .
                        field_identifier: type
                       =: =
                       identifier: TF_ATTR_TYPE
                      ;: ;
                     }: }
                    else_clause: else if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                     else: else
                     if_statement: if (typestr == "list(shape)") {
            metadata.type = TF_ATTR_SHAPE;
          } else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                      if: if
                      condition_clause: (typestr == "list(shape)")
                       (: (
                       binary_expression: typestr == "list(shape)"
                        identifier: typestr
                        ==: ==
                        string_literal: "list(shape)"
                         ": "
                         string_content: list(shape)
                         ": "
                       ): )
                      compound_statement: {
            metadata.type = TF_ATTR_SHAPE;
          }
                       {: {
                       expression_statement: metadata.type = TF_ATTR_SHAPE;
                        assignment_expression: metadata.type = TF_ATTR_SHAPE
                         field_expression: metadata.type
                          identifier: metadata
                          .: .
                          field_identifier: type
                         =: =
                         identifier: TF_ATTR_SHAPE
                        ;: ;
                       }: }
                      else_clause: else if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                       else: else
                       if_statement: if (typestr == "list(tensor)") {
            metadata.type = TF_ATTR_TENSOR;
          } else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                        if: if
                        condition_clause: (typestr == "list(tensor)")
                         (: (
                         binary_expression: typestr == "list(tensor)"
                          identifier: typestr
                          ==: ==
                          string_literal: "list(tensor)"
                           ": "
                           string_content: list(tensor)
                           ": "
                         ): )
                        compound_statement: {
            metadata.type = TF_ATTR_TENSOR;
          }
                         {: {
                         expression_statement: metadata.type = TF_ATTR_TENSOR;
                          assignment_expression: metadata.type = TF_ATTR_TENSOR
                           field_expression: metadata.type
                            identifier: metadata
                            .: .
                            field_identifier: type
                           =: =
                           identifier: TF_ATTR_TENSOR
                          ;: ;
                         }: }
                        else_clause: else if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                         else: else
                         if_statement: if (typestr == "list(func)") {
            metadata.type = TF_ATTR_FUNC;
          } else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                          if: if
                          condition_clause: (typestr == "list(func)")
                           (: (
                           binary_expression: typestr == "list(func)"
                            identifier: typestr
                            ==: ==
                            string_literal: "list(func)"
                             ": "
                             string_content: list(func)
                             ": "
                           ): )
                          compound_statement: {
            metadata.type = TF_ATTR_FUNC;
          }
                           {: {
                           expression_statement: metadata.type = TF_ATTR_FUNC;
                            assignment_expression: metadata.type = TF_ATTR_FUNC
                             field_expression: metadata.type
                              identifier: metadata
                              .: .
                              field_identifier: type
                             =: =
                             identifier: TF_ATTR_FUNC
                            ;: ;
                           }: }
                          else_clause: else {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                           else: else
                           compound_statement: {
            status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
            return metadata;
          }
                            {: {
                            expression_statement: status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'");
                             assignment_expression: status->status = InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'")
                              field_expression: status->status
                               identifier: status
                               ->: ->
                               field_identifier: status
                              =: =
                              call_expression: InvalidArgument(
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'")
                               identifier: InvalidArgument
                               argument_list: (
                "Attribute '", attr_name,
                "' has an empty value of an unrecognized type '", typestr, "'")
                                (: (
                                string_literal: "Attribute '"
                                 ": "
                                 string_content: Attribute '
                                 ": "
                                ,: ,
                                identifier: attr_name
                                ,: ,
                                string_literal: "' has an empty value of an unrecognized type '"
                                 ": "
                                 string_content: ' has an empty value of an unrecognized type '
                                 ": "
                                ,: ,
                                identifier: typestr
                                ,: ,
                                string_literal: "'"
                                 ": "
                                 string_content: '
                                 ": "
                                ): )
                             ;: ;
                            return_statement: return metadata;
                             return: return
                             identifier: metadata
                             ;: ;
                            }: }
           }: }
         }: }
       break_statement: break;
        break: break
        ;: ;
       case_statement: case tensorflow::AttrValue::kPlaceholder:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_PLACEHOLDER;
      metadata.total_size = -1;
      break;
        case: case
        qualified_identifier: tensorflow::AttrValue::kPlaceholder
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kPlaceholder
          namespace_identifier: AttrValue
          ::: ::
          identifier: kPlaceholder
        :: :
        expression_statement: metadata.is_list = 0;
         assignment_expression: metadata.is_list = 0
          field_expression: metadata.is_list
           identifier: metadata
           .: .
           field_identifier: is_list
          =: =
          number_literal: 0
         ;: ;
        expression_statement: metadata.list_size = -1;
         assignment_expression: metadata.list_size = -1
          field_expression: metadata.list_size
           identifier: metadata
           .: .
           field_identifier: list_size
          =: =
          number_literal: -1
         ;: ;
        expression_statement: metadata.type = TF_ATTR_PLACEHOLDER;
         assignment_expression: metadata.type = TF_ATTR_PLACEHOLDER
          field_expression: metadata.type
           identifier: metadata
           .: .
           field_identifier: type
          =: =
          identifier: TF_ATTR_PLACEHOLDER
         ;: ;
        expression_statement: metadata.total_size = -1;
         assignment_expression: metadata.total_size = -1
          field_expression: metadata.total_size
           identifier: metadata
           .: .
           field_identifier: total_size
          =: =
          number_literal: -1
         ;: ;
        break_statement: break;
         break: break
         ;: ;
       case_statement: case tensorflow::AttrValue::kFunc:
      metadata.is_list = 0;
      metadata.list_size = -1;
      metadata.type = TF_ATTR_FUNC;
      metadata.total_size = -1;
      break;
        case: case
        qualified_identifier: tensorflow::AttrValue::kFunc
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kFunc
          namespace_identifier: AttrValue
          ::: ::
          identifier: kFunc
        :: :
        expression_statement: metadata.is_list = 0;
         assignment_expression: metadata.is_list = 0
          field_expression: metadata.is_list
           identifier: metadata
           .: .
           field_identifier: is_list
          =: =
          number_literal: 0
         ;: ;
        expression_statement: metadata.list_size = -1;
         assignment_expression: metadata.list_size = -1
          field_expression: metadata.list_size
           identifier: metadata
           .: .
           field_identifier: list_size
          =: =
          number_literal: -1
         ;: ;
        expression_statement: metadata.type = TF_ATTR_FUNC;
         assignment_expression: metadata.type = TF_ATTR_FUNC
          field_expression: metadata.type
           identifier: metadata
           .: .
           field_identifier: type
          =: =
          identifier: TF_ATTR_FUNC
         ;: ;
        expression_statement: metadata.total_size = -1;
         assignment_expression: metadata.total_size = -1
          field_expression: metadata.total_size
           identifier: metadata
           .: .
           field_identifier: total_size
          =: =
          number_literal: -1
         ;: ;
        break_statement: break;
         break: break
         ;: ;
       case_statement: case tensorflow::AttrValue::VALUE_NOT_SET:
      status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
      break;
        case: case
        qualified_identifier: tensorflow::AttrValue::VALUE_NOT_SET
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::VALUE_NOT_SET
          namespace_identifier: AttrValue
          ::: ::
          identifier: VALUE_NOT_SET
        :: :
        expression_statement: status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set");
         assignment_expression: status->status =
          InvalidArgument("Attribute '", attr_name, "' has no value set")
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          =: =
          call_expression: InvalidArgument("Attribute '", attr_name, "' has no value set")
           identifier: InvalidArgument
           argument_list: ("Attribute '", attr_name, "' has no value set")
            (: (
            string_literal: "Attribute '"
             ": "
             string_content: Attribute '
             ": "
            ,: ,
            identifier: attr_name
            ,: ,
            string_literal: "' has no value set"
             ": "
             string_content: ' has no value set
             ": "
            ): )
         ;: ;
        break_statement: break;
         break: break
         ;: ;
       }: }
     return_statement: return metadata;
      return: return
      identifier: metadata
      ;: ;
     }: }
   function_definition: void TF_OperationGetAttrString(TF_Operation* oper, const char* attr_name,
                               void* value, size_t max_length,
                               TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kS) {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
  if (max_length <= 0) {
    return;
  }
  const auto& s = attr->s();
  std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length));
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrString(TF_Operation* oper, const char* attr_name,
                               void* value, size_t max_length,
                               TF_Status* status)
     identifier: TF_OperationGetAttrString
     parameter_list: (TF_Operation* oper, const char* attr_name,
                               void* value, size_t max_length,
                               TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: void* value
       primitive_type: void
       pointer_declarator: * value
        *: *
        identifier: value
      ,: ,
      parameter_declaration: size_t max_length
       primitive_type: size_t
       identifier: max_length
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kS) {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
  if (max_length <= 0) {
    return;
  }
  const auto& s = attr->s();
  std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length));
}
     {: {
     declaration: const auto* attr = GetAttrValue(oper, attr_name, status);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: * attr = GetAttrValue(oper, attr_name, status)
       pointer_declarator: * attr
        *: *
        identifier: attr
       =: =
       call_expression: GetAttrValue(oper, attr_name, status)
        identifier: GetAttrValue
        argument_list: (oper, attr_name, status)
         (: (
         identifier: oper
         ,: ,
         identifier: attr_name
         ,: ,
         identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     if_statement: if (attr->value_case() != tensorflow::AttrValue::kS) {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
      if: if
      condition_clause: (attr->value_case() != tensorflow::AttrValue::kS)
       (: (
       binary_expression: attr->value_case() != tensorflow::AttrValue::kS
        call_expression: attr->value_case()
         field_expression: attr->value_case
          identifier: attr
          ->: ->
          field_identifier: value_case
         argument_list: ()
          (: (
          ): )
        !=: !=
        qualified_identifier: tensorflow::AttrValue::kS
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kS
          namespace_identifier: AttrValue
          ::: ::
          identifier: kS
       ): )
      compound_statement: {
    status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
    return;
  }
       {: {
       expression_statement: status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string");
        assignment_expression: status->status =
        InvalidArgument("Attribute '", attr_name, "' is not a string")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Attribute '", attr_name, "' is not a string")
          identifier: InvalidArgument
          argument_list: ("Attribute '", attr_name, "' is not a string")
           (: (
           string_literal: "Attribute '"
            ": "
            string_content: Attribute '
            ": "
           ,: ,
           identifier: attr_name
           ,: ,
           string_literal: "' is not a string"
            ": "
            string_content: ' is not a string
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     if_statement: if (max_length <= 0) {
    return;
  }
      if: if
      condition_clause: (max_length <= 0)
       (: (
       binary_expression: max_length <= 0
        identifier: max_length
        <=: <=
        number_literal: 0
       ): )
      compound_statement: {
    return;
  }
       {: {
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: const auto& s = attr->s();
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: & s = attr->s()
       reference_declarator: & s
        &: &
        identifier: s
       =: =
       call_expression: attr->s()
        field_expression: attr->s
         identifier: attr
         ->: ->
         field_identifier: s
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length));
      call_expression: std::memcpy(value, s.data(), std::min<size_t>(s.length(), max_length))
       qualified_identifier: std::memcpy
        namespace_identifier: std
        ::: ::
        identifier: memcpy
       argument_list: (value, s.data(), std::min<size_t>(s.length(), max_length))
        (: (
        identifier: value
        ,: ,
        call_expression: s.data()
         field_expression: s.data
          identifier: s
          .: .
          field_identifier: data
         argument_list: ()
          (: (
          ): )
        ,: ,
        call_expression: std::min<size_t>(s.length(), max_length)
         qualified_identifier: std::min<size_t>
          namespace_identifier: std
          ::: ::
          template_function: min<size_t>
           identifier: min
           template_argument_list: <size_t>
            <: <
            type_descriptor: size_t
             primitive_type: size_t
            >: >
         argument_list: (s.length(), max_length)
          (: (
          call_expression: s.length()
           field_expression: s.length
            identifier: s
            .: .
            field_identifier: length
           argument_list: ()
            (: (
            ): )
          ,: ,
          identifier: max_length
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_OperationGetAttrStringList(TF_Operation* oper, const char* attr_name,
                                   void** values, size_t* lengths,
                                   int max_values, void* storage,
                                   size_t storage_size, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().s_size());
  char* p = static_cast<char*>(storage);
  for (int i = 0; i < len; ++i) {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrStringList(TF_Operation* oper, const char* attr_name,
                                   void** values, size_t* lengths,
                                   int max_values, void* storage,
                                   size_t storage_size, TF_Status* status)
     identifier: TF_OperationGetAttrStringList
     parameter_list: (TF_Operation* oper, const char* attr_name,
                                   void** values, size_t* lengths,
                                   int max_values, void* storage,
                                   size_t storage_size, TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: void** values
       primitive_type: void
       pointer_declarator: ** values
        *: *
        pointer_declarator: * values
         *: *
         identifier: values
      ,: ,
      parameter_declaration: size_t* lengths
       primitive_type: size_t
       pointer_declarator: * lengths
        *: *
        identifier: lengths
      ,: ,
      parameter_declaration: int max_values
       primitive_type: int
       identifier: max_values
      ,: ,
      parameter_declaration: void* storage
       primitive_type: void
       pointer_declarator: * storage
        *: *
        identifier: storage
      ,: ,
      parameter_declaration: size_t storage_size
       primitive_type: size_t
       identifier: storage_size
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().s_size());
  char* p = static_cast<char*>(storage);
  for (int i = 0; i < len; ++i) {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
}
     {: {
     declaration: const auto* attr = GetAttrValue(oper, attr_name, status);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: * attr = GetAttrValue(oper, attr_name, status)
       pointer_declarator: * attr
        *: *
        identifier: attr
       =: =
       call_expression: GetAttrValue(oper, attr_name, status)
        identifier: GetAttrValue
        argument_list: (oper, attr_name, status)
         (: (
         identifier: oper
         ,: ,
         identifier: attr_name
         ,: ,
         identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     if_statement: if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
      if: if
      condition_clause: (attr->value_case() != tensorflow::AttrValue::kList)
       (: (
       binary_expression: attr->value_case() != tensorflow::AttrValue::kList
        call_expression: attr->value_case()
         field_expression: attr->value_case
          identifier: attr
          ->: ->
          field_identifier: value_case
         argument_list: ()
          (: (
          ): )
        !=: !=
        qualified_identifier: tensorflow::AttrValue::kList
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kList
          namespace_identifier: AttrValue
          ::: ::
          identifier: kList
       ): )
      compound_statement: {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
       {: {
       expression_statement: status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
        assignment_expression: status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Value for '", attr_name, "' is not a list")
          identifier: InvalidArgument
          argument_list: ("Value for '", attr_name, "' is not a list")
           (: (
           string_literal: "Value for '"
            ": "
            string_content: Value for '
            ": "
           ,: ,
           identifier: attr_name
           ,: ,
           string_literal: "' is not a list"
            ": "
            string_content: ' is not a list
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: const auto len = std::min(max_values, attr->list().s_size());
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: len = std::min(max_values, attr->list().s_size())
       identifier: len
       =: =
       call_expression: std::min(max_values, attr->list().s_size())
        qualified_identifier: std::min
         namespace_identifier: std
         ::: ::
         identifier: min
        argument_list: (max_values, attr->list().s_size())
         (: (
         identifier: max_values
         ,: ,
         call_expression: attr->list().s_size()
          field_expression: attr->list().s_size
           call_expression: attr->list()
            field_expression: attr->list
             identifier: attr
             ->: ->
             field_identifier: list
            argument_list: ()
             (: (
             ): )
           .: .
           field_identifier: s_size
          argument_list: ()
           (: (
           ): )
         ): )
      ;: ;
     declaration: char* p = static_cast<char*>(storage);
      primitive_type: char
      init_declarator: * p = static_cast<char*>(storage)
       pointer_declarator: * p
        *: *
        identifier: p
       =: =
       call_expression: static_cast<char*>(storage)
        template_function: static_cast<char*>
         identifier: static_cast
         template_argument_list: <char*>
          <: <
          type_descriptor: char*
           primitive_type: char
           abstract_pointer_declarator: *
            *: *
          >: >
        argument_list: (storage)
         (: (
         identifier: storage
         ): )
      ;: ;
     for_statement: for (int i = 0; i < len; ++i) {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < len
       identifier: i
       <: <
       identifier: len
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    const string& s = attr->list().s(i);
    values[i] = p;
    lengths[i] = s.size();
    if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
    memcpy(values[i], s.data(), s.size());
    p += s.size();
  }
       {: {
       declaration: const string& s = attr->list().s(i);
        type_qualifier: const
         const: const
        type_identifier: string
        init_declarator: & s = attr->list().s(i)
         reference_declarator: & s
          &: &
          identifier: s
         =: =
         call_expression: attr->list().s(i)
          field_expression: attr->list().s
           call_expression: attr->list()
            field_expression: attr->list
             identifier: attr
             ->: ->
             field_identifier: list
            argument_list: ()
             (: (
             ): )
           .: .
           field_identifier: s
          argument_list: (i)
           (: (
           identifier: i
           ): )
        ;: ;
       expression_statement: values[i] = p;
        assignment_expression: values[i] = p
         subscript_expression: values[i]
          identifier: values
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         identifier: p
        ;: ;
       expression_statement: lengths[i] = s.size();
        assignment_expression: lengths[i] = s.size()
         subscript_expression: lengths[i]
          identifier: lengths
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: s.size()
          field_expression: s.size
           identifier: s
           .: .
           field_identifier: size
          argument_list: ()
           (: (
           ): )
        ;: ;
       if_statement: if ((p + s.size()) > (static_cast<char*>(storage) + storage_size)) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
        if: if
        condition_clause: ((p + s.size()) > (static_cast<char*>(storage) + storage_size))
         (: (
         binary_expression: (p + s.size()) > (static_cast<char*>(storage) + storage_size)
          parenthesized_expression: (p + s.size())
           (: (
           binary_expression: p + s.size()
            identifier: p
            +: +
            call_expression: s.size()
             field_expression: s.size
              identifier: s
              .: .
              field_identifier: size
             argument_list: ()
              (: (
              ): )
           ): )
          >: >
          parenthesized_expression: (static_cast<char*>(storage) + storage_size)
           (: (
           binary_expression: static_cast<char*>(storage) + storage_size
            call_expression: static_cast<char*>(storage)
             template_function: static_cast<char*>
              identifier: static_cast
              template_argument_list: <char*>
               <: <
               type_descriptor: char*
                primitive_type: char
                abstract_pointer_declarator: *
                 *: *
               >: >
             argument_list: (storage)
              (: (
              identifier: storage
              ): )
            +: +
            identifier: storage_size
           ): )
         ): )
        compound_statement: {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
      return;
    }
         {: {
         expression_statement: status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings");
          assignment_expression: status->status = InvalidArgument(
          "Not enough storage to hold the requested list of strings")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument(
          "Not enough storage to hold the requested list of strings")
            identifier: InvalidArgument
            argument_list: (
          "Not enough storage to hold the requested list of strings")
             (: (
             string_literal: "Not enough storage to hold the requested list of strings"
              ": "
              string_content: Not enough storage to hold the requested list of strings
              ": "
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       expression_statement: memcpy(values[i], s.data(), s.size());
        call_expression: memcpy(values[i], s.data(), s.size())
         identifier: memcpy
         argument_list: (values[i], s.data(), s.size())
          (: (
          subscript_expression: values[i]
           identifier: values
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          ,: ,
          call_expression: s.data()
           field_expression: s.data
            identifier: s
            .: .
            field_identifier: data
           argument_list: ()
            (: (
            ): )
          ,: ,
          call_expression: s.size()
           field_expression: s.size
            identifier: s
            .: .
            field_identifier: size
           argument_list: ()
            (: (
            ): )
          ): )
        ;: ;
       expression_statement: p += s.size();
        assignment_expression: p += s.size()
         identifier: p
         +=: +=
         call_expression: s.size()
          field_expression: s.size
           identifier: s
           .: .
           field_identifier: size
          argument_list: ()
           (: (
           ): )
        ;: ;
       }: }
     }: }
   preproc_function_def: #define DEFINE_GETATTR(func, c_type, cpp_type, list_field)                   \
  void func(TF_Operation* oper, const char* attr_name, c_type* value,        \
            TF_Status* status) {                                             \
    cpp_type v;                                                              \
    status->status =                                                         \
        tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &v);          \
    if (!status->status.ok()) return;                                        \
    *value = static_cast<c_type>(v);                                         \
  }                                                                          \
  void func##List(TF_Operation* oper, const char* attr_name, c_type* values, \
                  int max_values, TF_Status* status) {                       \
    const auto* attr = GetAttrValue(oper, attr_name, status);                \
    if (!status->status.ok()) return;                                        \
    if (attr->value_case() != tensorflow::AttrValue::kList) {                \
      status->status =                                                       \
          InvalidArgument("Value for '", attr_name, "' is not a list.");     \
      return;                                                                \
    }                                                                        \
    const auto len = std::min(max_values, attr->list().list_field##_size()); \
    for (int i = 0; i < len; ++i) {                                          \
      values[i] = static_cast<c_type>(attr->list().list_field(i));           \
    }                                                                        \
  }

    #define: #define
    identifier: DEFINE_GETATTR
    preproc_params: (func, c_type, cpp_type, list_field)
     (: (
     identifier: func
     ,: ,
     identifier: c_type
     ,: ,
     identifier: cpp_type
     ,: ,
     identifier: list_field
     ): )
    preproc_arg: void func(TF_Operation* oper, const char* attr_name, c_type* value,        \
            TF_Status* status) {                                             \
    cpp_type v;                                                              \
    status->status =                                                         \
        tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &v);          \
    if (!status->status.ok()) return;                                        \
    *value = static_cast<c_type>(v);                                         \
  }                                                                          \
  void func##List(TF_Operation* oper, const char* attr_name, c_type* values, \
                  int max_values, TF_Status* status) {                       \
    const auto* attr = GetAttrValue(oper, attr_name, status);                \
    if (!status->status.ok()) return;                                        \
    if (attr->value_case() != tensorflow::AttrValue::kList) {                \
      status->status =                                                       \
          InvalidArgument("Value for '", attr_name, "' is not a list.");     \
      return;                                                                \
    }                                                                        \
    const auto len = std::min(max_values, attr->list().list_field##_size()); \
    for (int i = 0; i < len; ++i) {                                          \
      values[i] = static_cast<c_type>(attr->list().list_field(i));           \
    }                                                                        \
  }
   ERROR: DEFINE_GETATTR(TF_OperationGetAttrInt, int64_t, int64_t, i)
    function_declarator: DEFINE_GETATTR(TF_OperationGetAttrInt, int64_t, int64_t, i)
     identifier: DEFINE_GETATTR
     parameter_list: (TF_OperationGetAttrInt, int64_t, int64_t, i)
      (: (
      parameter_declaration: TF_OperationGetAttrInt
       type_identifier: TF_OperationGetAttrInt
      ,: ,
      parameter_declaration: int64_t
       primitive_type: int64_t
      ,: ,
      parameter_declaration: int64_t
       primitive_type: int64_t
      ,: ,
      parameter_declaration: i
       type_identifier: i
      ): )
   expression_statement: ;
    ;: ;
   ERROR: DEFINE_GETATTR(TF_OperationGetAttrFloat, float, float, f)
    function_declarator: DEFINE_GETATTR(TF_OperationGetAttrFloat, float, float, f)
     identifier: DEFINE_GETATTR
     parameter_list: (TF_OperationGetAttrFloat, float, float, f)
      (: (
      parameter_declaration: TF_OperationGetAttrFloat
       type_identifier: TF_OperationGetAttrFloat
      ,: ,
      parameter_declaration: float
       primitive_type: float
      ,: ,
      parameter_declaration: float
       primitive_type: float
      ,: ,
      parameter_declaration: f
       type_identifier: f
      ): )
   expression_statement: ;
    ;: ;
   ERROR: DEFINE_GETATTR(TF_OperationGetAttrBool, unsigned char, bool, b)
    function_declarator: DEFINE_GETATTR(TF_OperationGetAttrBool, unsigned char, bool, b)
     identifier: DEFINE_GETATTR
     parameter_list: (TF_OperationGetAttrBool, unsigned char, bool, b)
      (: (
      parameter_declaration: TF_OperationGetAttrBool
       type_identifier: TF_OperationGetAttrBool
      ,: ,
      parameter_declaration: unsigned char
       sized_type_specifier: unsigned char
        unsigned: unsigned
        primitive_type: char
      ,: ,
      parameter_declaration: bool
       primitive_type: bool
      ,: ,
      parameter_declaration: b
       type_identifier: b
      ): )
   expression_statement: ;
    ;: ;
   expression_statement: DEFINE_GETATTR(TF_OperationGetAttrType, TF_DataType, DataType, type);
    call_expression: DEFINE_GETATTR(TF_OperationGetAttrType, TF_DataType, DataType, type)
     identifier: DEFINE_GETATTR
     argument_list: (TF_OperationGetAttrType, TF_DataType, DataType, type)
      (: (
      identifier: TF_OperationGetAttrType
      ,: ,
      identifier: TF_DataType
      ,: ,
      identifier: DataType
      ,: ,
      identifier: type
      ): )
    ;: ;
   preproc_call: #undef DEFINE_GETATTR

    preproc_directive: #undef
    preproc_arg: DEFINE_GETATTR
   function_definition: void TF_OperationGetAttrShape(TF_Operation* oper, const char* attr_name,
                              int64_t* value, int num_dims, TF_Status* status) {
  PartialTensorShape shape;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape);
  if (!status->status.ok()) return;
  auto len = std::min(shape.dims(), num_dims);
  for (int i = 0; i < len; ++i) {
    value[i] = shape.dim_size(i);
  }
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrShape(TF_Operation* oper, const char* attr_name,
                              int64_t* value, int num_dims, TF_Status* status)
     identifier: TF_OperationGetAttrShape
     parameter_list: (TF_Operation* oper, const char* attr_name,
                              int64_t* value, int num_dims, TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: int64_t* value
       primitive_type: int64_t
       pointer_declarator: * value
        *: *
        identifier: value
      ,: ,
      parameter_declaration: int num_dims
       primitive_type: int
       identifier: num_dims
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  PartialTensorShape shape;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape);
  if (!status->status.ok()) return;
  auto len = std::min(shape.dims(), num_dims);
  for (int i = 0; i < len; ++i) {
    value[i] = shape.dim_size(i);
  }
}
     {: {
     declaration: PartialTensorShape shape;
      type_identifier: PartialTensorShape
      identifier: shape
      ;: ;
     expression_statement: status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape);
      assignment_expression: status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shape)
        qualified_identifier: tensorflow::GetNodeAttr
         namespace_identifier: tensorflow
         ::: ::
         identifier: GetNodeAttr
        argument_list: (oper->node.attrs(), attr_name, &shape)
         (: (
         call_expression: oper->node.attrs()
          field_expression: oper->node.attrs
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: attrs
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: attr_name
         ,: ,
         pointer_expression: &shape
          &: &
          identifier: shape
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     declaration: auto len = std::min(shape.dims(), num_dims);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: len = std::min(shape.dims(), num_dims)
       identifier: len
       =: =
       call_expression: std::min(shape.dims(), num_dims)
        qualified_identifier: std::min
         namespace_identifier: std
         ::: ::
         identifier: min
        argument_list: (shape.dims(), num_dims)
         (: (
         call_expression: shape.dims()
          field_expression: shape.dims
           identifier: shape
           .: .
           field_identifier: dims
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: num_dims
         ): )
      ;: ;
     for_statement: for (int i = 0; i < len; ++i) {
    value[i] = shape.dim_size(i);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < len
       identifier: i
       <: <
       identifier: len
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    value[i] = shape.dim_size(i);
  }
       {: {
       expression_statement: value[i] = shape.dim_size(i);
        assignment_expression: value[i] = shape.dim_size(i)
         subscript_expression: value[i]
          identifier: value
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: shape.dim_size(i)
          field_expression: shape.dim_size
           identifier: shape
           .: .
           field_identifier: dim_size
          argument_list: (i)
           (: (
           identifier: i
           ): )
        ;: ;
       }: }
     }: }
   function_definition: void TF_OperationGetAttrShapeList(TF_Operation* oper, const char* attr_name,
                                  int64_t** dims, int* num_dims, int num_shapes,
                                  int64_t* storage, int storage_size,
                                  TF_Status* status) {
  std::vector<PartialTensorShape> shapes;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes);
  if (!status->status.ok()) return;
  auto len = std::min(static_cast<int>(shapes.size()), num_shapes);
  int64_t* p = storage;
  int storage_left = storage_size;
  for (int i = 0; i < len; ++i) {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrShapeList(TF_Operation* oper, const char* attr_name,
                                  int64_t** dims, int* num_dims, int num_shapes,
                                  int64_t* storage, int storage_size,
                                  TF_Status* status)
     identifier: TF_OperationGetAttrShapeList
     parameter_list: (TF_Operation* oper, const char* attr_name,
                                  int64_t** dims, int* num_dims, int num_shapes,
                                  int64_t* storage, int storage_size,
                                  TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: int64_t** dims
       primitive_type: int64_t
       pointer_declarator: ** dims
        *: *
        pointer_declarator: * dims
         *: *
         identifier: dims
      ,: ,
      parameter_declaration: int* num_dims
       primitive_type: int
       pointer_declarator: * num_dims
        *: *
        identifier: num_dims
      ,: ,
      parameter_declaration: int num_shapes
       primitive_type: int
       identifier: num_shapes
      ,: ,
      parameter_declaration: int64_t* storage
       primitive_type: int64_t
       pointer_declarator: * storage
        *: *
        identifier: storage
      ,: ,
      parameter_declaration: int storage_size
       primitive_type: int
       identifier: storage_size
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  std::vector<PartialTensorShape> shapes;
  status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes);
  if (!status->status.ok()) return;
  auto len = std::min(static_cast<int>(shapes.size()), num_shapes);
  int64_t* p = storage;
  int storage_left = storage_size;
  for (int i = 0; i < len; ++i) {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
}
     {: {
     declaration: std::vector<PartialTensorShape> shapes;
      qualified_identifier: std::vector<PartialTensorShape>
       namespace_identifier: std
       ::: ::
       template_type: vector<PartialTensorShape>
        type_identifier: vector
        template_argument_list: <PartialTensorShape>
         <: <
         type_descriptor: PartialTensorShape
          type_identifier: PartialTensorShape
         >: >
      identifier: shapes
      ;: ;
     expression_statement: status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes);
      assignment_expression: status->status =
      tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &shapes)
        qualified_identifier: tensorflow::GetNodeAttr
         namespace_identifier: tensorflow
         ::: ::
         identifier: GetNodeAttr
        argument_list: (oper->node.attrs(), attr_name, &shapes)
         (: (
         call_expression: oper->node.attrs()
          field_expression: oper->node.attrs
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: attrs
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: attr_name
         ,: ,
         pointer_expression: &shapes
          &: &
          identifier: shapes
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     declaration: auto len = std::min(static_cast<int>(shapes.size()), num_shapes);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: len = std::min(static_cast<int>(shapes.size()), num_shapes)
       identifier: len
       =: =
       call_expression: std::min(static_cast<int>(shapes.size()), num_shapes)
        qualified_identifier: std::min
         namespace_identifier: std
         ::: ::
         identifier: min
        argument_list: (static_cast<int>(shapes.size()), num_shapes)
         (: (
         call_expression: static_cast<int>(shapes.size())
          template_function: static_cast<int>
           identifier: static_cast
           template_argument_list: <int>
            <: <
            type_descriptor: int
             primitive_type: int
            >: >
          argument_list: (shapes.size())
           (: (
           call_expression: shapes.size()
            field_expression: shapes.size
             identifier: shapes
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
           ): )
         ,: ,
         identifier: num_shapes
         ): )
      ;: ;
     declaration: int64_t* p = storage;
      primitive_type: int64_t
      init_declarator: * p = storage
       pointer_declarator: * p
        *: *
        identifier: p
       =: =
       identifier: storage
      ;: ;
     declaration: int storage_left = storage_size;
      primitive_type: int
      init_declarator: storage_left = storage_size
       identifier: storage_left
       =: =
       identifier: storage_size
      ;: ;
     for_statement: for (int i = 0; i < len; ++i) {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < len
       identifier: i
       <: <
       identifier: len
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    // shapes[i].dims() == -1 for shapes with an unknown rank.
    int64_t n = shapes[i].dims();
    num_dims[i] = n;
    dims[i] = p;
    if (n < 0) {
      continue;
    }
    if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
    storage_left -= n;
    for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
  }
       {: {
       comment: // shapes[i].dims() == -1 for shapes with an unknown rank.
       declaration: int64_t n = shapes[i].dims();
        primitive_type: int64_t
        init_declarator: n = shapes[i].dims()
         identifier: n
         =: =
         call_expression: shapes[i].dims()
          field_expression: shapes[i].dims
           subscript_expression: shapes[i]
            identifier: shapes
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           .: .
           field_identifier: dims
          argument_list: ()
           (: (
           ): )
        ;: ;
       expression_statement: num_dims[i] = n;
        assignment_expression: num_dims[i] = n
         subscript_expression: num_dims[i]
          identifier: num_dims
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         identifier: n
        ;: ;
       expression_statement: dims[i] = p;
        assignment_expression: dims[i] = p
         subscript_expression: dims[i]
          identifier: dims
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         identifier: p
        ;: ;
       if_statement: if (n < 0) {
      continue;
    }
        if: if
        condition_clause: (n < 0)
         (: (
         binary_expression: n < 0
          identifier: n
          <: <
          number_literal: 0
         ): )
        compound_statement: {
      continue;
    }
         {: {
         continue_statement: continue;
          continue: continue
          ;: ;
         }: }
       if_statement: if (storage_left < n) {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
        if: if
        condition_clause: (storage_left < n)
         (: (
         binary_expression: storage_left < n
          identifier: storage_left
          <: <
          identifier: n
         ): )
        compound_statement: {
      status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
      return;
    }
         {: {
         expression_statement: status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes");
          assignment_expression: status->status = InvalidArgument(
          "Not enough storage to hold the requested list of shapes")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument(
          "Not enough storage to hold the requested list of shapes")
            identifier: InvalidArgument
            argument_list: (
          "Not enough storage to hold the requested list of shapes")
             (: (
             string_literal: "Not enough storage to hold the requested list of shapes"
              ": "
              string_content: Not enough storage to hold the requested list of shapes
              ": "
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       expression_statement: storage_left -= n;
        assignment_expression: storage_left -= n
         identifier: storage_left
         -=: -=
         identifier: n
        ;: ;
       for_statement: for (int j = 0; j < n; ++j, ++p) {
      *p = shapes[i].dim_size(j);
    }
        for: for
        (: (
        declaration: int j = 0;
         primitive_type: int
         init_declarator: j = 0
          identifier: j
          =: =
          number_literal: 0
         ;: ;
        binary_expression: j < n
         identifier: j
         <: <
         identifier: n
        ;: ;
        comma_expression: ++j, ++p
         update_expression: ++j
          ++: ++
          identifier: j
         ,: ,
         update_expression: ++p
          ++: ++
          identifier: p
        ): )
        compound_statement: {
      *p = shapes[i].dim_size(j);
    }
         {: {
         expression_statement: *p = shapes[i].dim_size(j);
          assignment_expression: *p = shapes[i].dim_size(j)
           pointer_expression: *p
            *: *
            identifier: p
           =: =
           call_expression: shapes[i].dim_size(j)
            field_expression: shapes[i].dim_size
             subscript_expression: shapes[i]
              identifier: shapes
              subscript_argument_list: [i]
               [: [
               identifier: i
               ]: ]
             .: .
             field_identifier: dim_size
            argument_list: (j)
             (: (
             identifier: j
             ): )
          ;: ;
         }: }
       }: }
     }: }
   function_definition: void TF_OperationGetAttrTensorShapeProto(TF_Operation* oper,
                                         const char* attr_name,
                                         TF_Buffer* value, TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kShape) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
  status->status = MessageToBuffer(attr->shape(), value);
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrTensorShapeProto(TF_Operation* oper,
                                         const char* attr_name,
                                         TF_Buffer* value, TF_Status* status)
     identifier: TF_OperationGetAttrTensorShapeProto
     parameter_list: (TF_Operation* oper,
                                         const char* attr_name,
                                         TF_Buffer* value, TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Buffer* value
       type_identifier: TF_Buffer
       pointer_declarator: * value
        *: *
        identifier: value
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kShape) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
  status->status = MessageToBuffer(attr->shape(), value);
}
     {: {
     declaration: const auto* attr = GetAttrValue(oper, attr_name, status);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: * attr = GetAttrValue(oper, attr_name, status)
       pointer_declarator: * attr
        *: *
        identifier: attr
       =: =
       call_expression: GetAttrValue(oper, attr_name, status)
        identifier: GetAttrValue
        argument_list: (oper, attr_name, status)
         (: (
         identifier: oper
         ,: ,
         identifier: attr_name
         ,: ,
         identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     if_statement: if (attr->value_case() != tensorflow::AttrValue::kShape) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
      if: if
      condition_clause: (attr->value_case() != tensorflow::AttrValue::kShape)
       (: (
       binary_expression: attr->value_case() != tensorflow::AttrValue::kShape
        call_expression: attr->value_case()
         field_expression: attr->value_case
          identifier: attr
          ->: ->
          field_identifier: value_case
         argument_list: ()
          (: (
          ): )
        !=: !=
        qualified_identifier: tensorflow::AttrValue::kShape
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kShape
          namespace_identifier: AttrValue
          ::: ::
          identifier: kShape
       ): )
      compound_statement: {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
    return;
  }
       {: {
       expression_statement: status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.");
        assignment_expression: status->status =
        InvalidArgument("Value for '", attr_name, "' is not a shape.")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Value for '", attr_name, "' is not a shape.")
          identifier: InvalidArgument
          argument_list: ("Value for '", attr_name, "' is not a shape.")
           (: (
           string_literal: "Value for '"
            ": "
            string_content: Value for '
            ": "
           ,: ,
           identifier: attr_name
           ,: ,
           string_literal: "' is not a shape."
            ": "
            string_content: ' is not a shape.
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: status->status = MessageToBuffer(attr->shape(), value);
      assignment_expression: status->status = MessageToBuffer(attr->shape(), value)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(attr->shape(), value)
        identifier: MessageToBuffer
        argument_list: (attr->shape(), value)
         (: (
         call_expression: attr->shape()
          field_expression: attr->shape
           identifier: attr
           ->: ->
           field_identifier: shape
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: value
         ): )
      ;: ;
     }: }
   function_definition: void TF_OperationGetAttrTensorShapeProtoList(TF_Operation* oper,
                                             const char* attr_name,
                                             TF_Buffer** values, int max_values,
                                             TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().shape_size());
  for (int i = 0; i < len; ++i) {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrTensorShapeProtoList(TF_Operation* oper,
                                             const char* attr_name,
                                             TF_Buffer** values, int max_values,
                                             TF_Status* status)
     identifier: TF_OperationGetAttrTensorShapeProtoList
     parameter_list: (TF_Operation* oper,
                                             const char* attr_name,
                                             TF_Buffer** values, int max_values,
                                             TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Buffer** values
       type_identifier: TF_Buffer
       pointer_declarator: ** values
        *: *
        pointer_declarator: * values
         *: *
         identifier: values
      ,: ,
      parameter_declaration: int max_values
       primitive_type: int
       identifier: max_values
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
  const auto len = std::min(max_values, attr->list().shape_size());
  for (int i = 0; i < len; ++i) {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
}
     {: {
     declaration: const auto* attr = GetAttrValue(oper, attr_name, status);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: * attr = GetAttrValue(oper, attr_name, status)
       pointer_declarator: * attr
        *: *
        identifier: attr
       =: =
       call_expression: GetAttrValue(oper, attr_name, status)
        identifier: GetAttrValue
        argument_list: (oper, attr_name, status)
         (: (
         identifier: oper
         ,: ,
         identifier: attr_name
         ,: ,
         identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     if_statement: if (attr->value_case() != tensorflow::AttrValue::kList) {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
      if: if
      condition_clause: (attr->value_case() != tensorflow::AttrValue::kList)
       (: (
       binary_expression: attr->value_case() != tensorflow::AttrValue::kList
        call_expression: attr->value_case()
         field_expression: attr->value_case
          identifier: attr
          ->: ->
          field_identifier: value_case
         argument_list: ()
          (: (
          ): )
        !=: !=
        qualified_identifier: tensorflow::AttrValue::kList
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: AttrValue::kList
          namespace_identifier: AttrValue
          ::: ::
          identifier: kList
       ): )
      compound_statement: {
    status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
    return;
  }
       {: {
       expression_statement: status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list");
        assignment_expression: status->status =
        InvalidArgument("Value for '", attr_name, "' is not a list")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Value for '", attr_name, "' is not a list")
          identifier: InvalidArgument
          argument_list: ("Value for '", attr_name, "' is not a list")
           (: (
           string_literal: "Value for '"
            ": "
            string_content: Value for '
            ": "
           ,: ,
           identifier: attr_name
           ,: ,
           string_literal: "' is not a list"
            ": "
            string_content: ' is not a list
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: const auto len = std::min(max_values, attr->list().shape_size());
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: len = std::min(max_values, attr->list().shape_size())
       identifier: len
       =: =
       call_expression: std::min(max_values, attr->list().shape_size())
        qualified_identifier: std::min
         namespace_identifier: std
         ::: ::
         identifier: min
        argument_list: (max_values, attr->list().shape_size())
         (: (
         identifier: max_values
         ,: ,
         call_expression: attr->list().shape_size()
          field_expression: attr->list().shape_size
           call_expression: attr->list()
            field_expression: attr->list
             identifier: attr
             ->: ->
             field_identifier: list
            argument_list: ()
             (: (
             ): )
           .: .
           field_identifier: shape_size
          argument_list: ()
           (: (
           ): )
         ): )
      ;: ;
     for_statement: for (int i = 0; i < len; ++i) {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < len
       identifier: i
       <: <
       identifier: len
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    values[i] = TF_NewBuffer();
    status->status = MessageToBuffer(attr->list().shape(i), values[i]);
    if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
  }
       {: {
       expression_statement: values[i] = TF_NewBuffer();
        assignment_expression: values[i] = TF_NewBuffer()
         subscript_expression: values[i]
          identifier: values
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: TF_NewBuffer()
          identifier: TF_NewBuffer
          argument_list: ()
           (: (
           ): )
        ;: ;
       expression_statement: status->status = MessageToBuffer(attr->list().shape(i), values[i]);
        assignment_expression: status->status = MessageToBuffer(attr->list().shape(i), values[i])
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: MessageToBuffer(attr->list().shape(i), values[i])
          identifier: MessageToBuffer
          argument_list: (attr->list().shape(i), values[i])
           (: (
           call_expression: attr->list().shape(i)
            field_expression: attr->list().shape
             call_expression: attr->list()
              field_expression: attr->list
               identifier: attr
               ->: ->
               field_identifier: list
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: shape
            argument_list: (i)
             (: (
             identifier: i
             ): )
           ,: ,
           subscript_expression: values[i]
            identifier: values
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       if_statement: if (!status->status.ok()) {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
      // Delete everything allocated to far, the operation has failed.
      for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
      return;
    }
         {: {
         comment: // Delete everything allocated to far, the operation has failed.
         for_statement: for (int j = 0; j <= i; ++j) {
        TF_DeleteBuffer(values[j]);
      }
          for: for
          (: (
          declaration: int j = 0;
           primitive_type: int
           init_declarator: j = 0
            identifier: j
            =: =
            number_literal: 0
           ;: ;
          binary_expression: j <= i
           identifier: j
           <=: <=
           identifier: i
          ;: ;
          update_expression: ++j
           ++: ++
           identifier: j
          ): )
          compound_statement: {
        TF_DeleteBuffer(values[j]);
      }
           {: {
           expression_statement: TF_DeleteBuffer(values[j]);
            call_expression: TF_DeleteBuffer(values[j])
             identifier: TF_DeleteBuffer
             argument_list: (values[j])
              (: (
              subscript_expression: values[j]
               identifier: values
               subscript_argument_list: [j]
                [: [
                identifier: j
                ]: ]
              ): )
            ;: ;
           }: }
         return_statement: return;
          return: return
          ;: ;
         }: }
       }: }
     }: }
   function_definition: void TF_OperationGetAttrTensor(TF_Operation* oper, const char* attr_name,
                               TF_Tensor** value, TF_Status* status) {
  *value = nullptr;
  Tensor t;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t);
  if (!status->status.ok()) return;
  *value = TF_TensorFromTensor(t, &status->status);
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrTensor(TF_Operation* oper, const char* attr_name,
                               TF_Tensor** value, TF_Status* status)
     identifier: TF_OperationGetAttrTensor
     parameter_list: (TF_Operation* oper, const char* attr_name,
                               TF_Tensor** value, TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Tensor** value
       type_identifier: TF_Tensor
       pointer_declarator: ** value
        *: *
        pointer_declarator: * value
         *: *
         identifier: value
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  *value = nullptr;
  Tensor t;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t);
  if (!status->status.ok()) return;
  *value = TF_TensorFromTensor(t, &status->status);
}
     {: {
     expression_statement: *value = nullptr;
      assignment_expression: *value = nullptr
       pointer_expression: *value
        *: *
        identifier: value
       =: =
       null: nullptr
        nullptr: nullptr
      ;: ;
     declaration: Tensor t;
      type_identifier: Tensor
      identifier: t
      ;: ;
     expression_statement: status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t);
      assignment_expression: status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &t)
        qualified_identifier: tensorflow::GetNodeAttr
         namespace_identifier: tensorflow
         ::: ::
         identifier: GetNodeAttr
        argument_list: (oper->node.attrs(), attr_name, &t)
         (: (
         call_expression: oper->node.attrs()
          field_expression: oper->node.attrs
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: attrs
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: attr_name
         ,: ,
         pointer_expression: &t
          &: &
          identifier: t
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     expression_statement: *value = TF_TensorFromTensor(t, &status->status);
      assignment_expression: *value = TF_TensorFromTensor(t, &status->status)
       pointer_expression: *value
        *: *
        identifier: value
       =: =
       call_expression: TF_TensorFromTensor(t, &status->status)
        identifier: TF_TensorFromTensor
        argument_list: (t, &status->status)
         (: (
         identifier: t
         ,: ,
         pointer_expression: &status->status
          &: &
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
         ): )
      ;: ;
     }: }
   function_definition: void TF_OperationGetAttrTensorList(TF_Operation* oper, const char* attr_name,
                                   TF_Tensor** values, int max_values,
                                   TF_Status* status) {
  std::vector<Tensor> ts;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts);
  if (!status->status.ok()) return;
  const auto len = std::min(max_values, static_cast<int>(ts.size()));
  for (int i = 0; i < len; ++i) {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrTensorList(TF_Operation* oper, const char* attr_name,
                                   TF_Tensor** values, int max_values,
                                   TF_Status* status)
     identifier: TF_OperationGetAttrTensorList
     parameter_list: (TF_Operation* oper, const char* attr_name,
                                   TF_Tensor** values, int max_values,
                                   TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Tensor** values
       type_identifier: TF_Tensor
       pointer_declarator: ** values
        *: *
        pointer_declarator: * values
         *: *
         identifier: values
      ,: ,
      parameter_declaration: int max_values
       primitive_type: int
       identifier: max_values
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  std::vector<Tensor> ts;
  status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts);
  if (!status->status.ok()) return;
  const auto len = std::min(max_values, static_cast<int>(ts.size()));
  for (int i = 0; i < len; ++i) {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
}
     {: {
     declaration: std::vector<Tensor> ts;
      qualified_identifier: std::vector<Tensor>
       namespace_identifier: std
       ::: ::
       template_type: vector<Tensor>
        type_identifier: vector
        template_argument_list: <Tensor>
         <: <
         type_descriptor: Tensor
          type_identifier: Tensor
         >: >
      identifier: ts
      ;: ;
     expression_statement: status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts);
      assignment_expression: status->status = tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: tensorflow::GetNodeAttr(oper->node.attrs(), attr_name, &ts)
        qualified_identifier: tensorflow::GetNodeAttr
         namespace_identifier: tensorflow
         ::: ::
         identifier: GetNodeAttr
        argument_list: (oper->node.attrs(), attr_name, &ts)
         (: (
         call_expression: oper->node.attrs()
          field_expression: oper->node.attrs
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: attrs
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: attr_name
         ,: ,
         pointer_expression: &ts
          &: &
          identifier: ts
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     declaration: const auto len = std::min(max_values, static_cast<int>(ts.size()));
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: len = std::min(max_values, static_cast<int>(ts.size()))
       identifier: len
       =: =
       call_expression: std::min(max_values, static_cast<int>(ts.size()))
        qualified_identifier: std::min
         namespace_identifier: std
         ::: ::
         identifier: min
        argument_list: (max_values, static_cast<int>(ts.size()))
         (: (
         identifier: max_values
         ,: ,
         call_expression: static_cast<int>(ts.size())
          template_function: static_cast<int>
           identifier: static_cast
           template_argument_list: <int>
            <: <
            type_descriptor: int
             primitive_type: int
            >: >
          argument_list: (ts.size())
           (: (
           call_expression: ts.size()
            field_expression: ts.size
             identifier: ts
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
           ): )
         ): )
      ;: ;
     for_statement: for (int i = 0; i < len; ++i) {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < len
       identifier: i
       <: <
       identifier: len
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    values[i] = TF_TensorFromTensor(ts[i], &status->status);
  }
       {: {
       expression_statement: values[i] = TF_TensorFromTensor(ts[i], &status->status);
        assignment_expression: values[i] = TF_TensorFromTensor(ts[i], &status->status)
         subscript_expression: values[i]
          identifier: values
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: TF_TensorFromTensor(ts[i], &status->status)
          identifier: TF_TensorFromTensor
          argument_list: (ts[i], &status->status)
           (: (
           subscript_expression: ts[i]
            identifier: ts
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ,: ,
           pointer_expression: &status->status
            &: &
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
           ): )
        ;: ;
       }: }
     }: }
   function_definition: void TF_OperationGetAttrValueProto(TF_Operation* oper, const char* attr_name,
                                   TF_Buffer* output_attr_value,
                                   TF_Status* status) {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  status->status = MessageToBuffer(*attr, output_attr_value);
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrValueProto(TF_Operation* oper, const char* attr_name,
                                   TF_Buffer* output_attr_value,
                                   TF_Status* status)
     identifier: TF_OperationGetAttrValueProto
     parameter_list: (TF_Operation* oper, const char* attr_name,
                                   TF_Buffer* output_attr_value,
                                   TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Buffer* output_attr_value
       type_identifier: TF_Buffer
       pointer_declarator: * output_attr_value
        *: *
        identifier: output_attr_value
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  const auto* attr = GetAttrValue(oper, attr_name, status);
  if (!status->status.ok()) return;
  status->status = MessageToBuffer(*attr, output_attr_value);
}
     {: {
     declaration: const auto* attr = GetAttrValue(oper, attr_name, status);
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: * attr = GetAttrValue(oper, attr_name, status)
       pointer_declarator: * attr
        *: *
        identifier: attr
       =: =
       call_expression: GetAttrValue(oper, attr_name, status)
        identifier: GetAttrValue
        argument_list: (oper, attr_name, status)
         (: (
         identifier: oper
         ,: ,
         identifier: attr_name
         ,: ,
         identifier: status
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) return;
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     expression_statement: status->status = MessageToBuffer(*attr, output_attr_value);
      assignment_expression: status->status = MessageToBuffer(*attr, output_attr_value)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(*attr, output_attr_value)
        identifier: MessageToBuffer
        argument_list: (*attr, output_attr_value)
         (: (
         pointer_expression: *attr
          *: *
          identifier: attr
         ,: ,
         identifier: output_attr_value
         ): )
      ;: ;
     }: }
   function_definition: int TF_OperationGetNumAttrs(TF_Operation* oper) {
  return oper->node.attrs().size();
}
    primitive_type: int
    function_declarator: TF_OperationGetNumAttrs(TF_Operation* oper)
     identifier: TF_OperationGetNumAttrs
     parameter_list: (TF_Operation* oper)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ): )
    compound_statement: {
  return oper->node.attrs().size();
}
     {: {
     return_statement: return oper->node.attrs().size();
      return: return
      call_expression: oper->node.attrs().size()
       field_expression: oper->node.attrs().size
        call_expression: oper->node.attrs()
         field_expression: oper->node.attrs
          field_expression: oper->node
           identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: attrs
         argument_list: ()
          (: (
          ): )
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: int TF_OperationGetAttrNameLength(TF_Operation* oper, int i) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
  return -1;
}
    primitive_type: int
    function_declarator: TF_OperationGetAttrNameLength(TF_Operation* oper, int i)
     identifier: TF_OperationGetAttrNameLength
     parameter_list: (TF_Operation* oper, int i)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: int i
       primitive_type: int
       identifier: i
      ): )
    compound_statement: {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
  return -1;
}
     {: {
     declaration: auto attrs = oper->node.attrs();
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: attrs = oper->node.attrs()
       identifier: attrs
       =: =
       call_expression: oper->node.attrs()
        field_expression: oper->node.attrs
         field_expression: oper->node
          identifier: oper
          ->: ->
          field_identifier: node
         .: .
         field_identifier: attrs
        argument_list: ()
         (: (
         ): )
      ;: ;
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     declaration: AttrValueMap::const_iterator it;
      qualified_identifier: AttrValueMap::const_iterator
       namespace_identifier: AttrValueMap
       ::: ::
       type_identifier: const_iterator
      identifier: it
      ;: ;
     for_statement: for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
      for: for
      (: (
      assignment_expression: it = attrs.begin()
       identifier: it
       =: =
       call_expression: attrs.begin()
        field_expression: attrs.begin
         identifier: attrs
         .: .
         field_identifier: begin
        argument_list: ()
         (: (
         ): )
      ;: ;
      binary_expression: it != attrs.end()
       identifier: it
       !=: !=
       call_expression: attrs.end()
        field_expression: attrs.end
         identifier: attrs
         .: .
         field_identifier: end
        argument_list: ()
         (: (
         ): )
      ;: ;
      update_expression: it++
       identifier: it
       ++: ++
      ): )
      compound_statement: {
    if (count == i) {
      return it->first.length();
    }
    count++;
  }
       {: {
       if_statement: if (count == i) {
      return it->first.length();
    }
        if: if
        condition_clause: (count == i)
         (: (
         binary_expression: count == i
          identifier: count
          ==: ==
          identifier: i
         ): )
        compound_statement: {
      return it->first.length();
    }
         {: {
         return_statement: return it->first.length();
          return: return
          call_expression: it->first.length()
           field_expression: it->first.length
            field_expression: it->first
             identifier: it
             ->: ->
             field_identifier: first
            .: .
            field_identifier: length
           argument_list: ()
            (: (
            ): )
          ;: ;
         }: }
       expression_statement: count++;
        update_expression: count++
         identifier: count
         ++: ++
        ;: ;
       }: }
     return_statement: return -1;
      return: return
      number_literal: -1
      ;: ;
     }: }
   function_definition: void TF_OperationGetAttrName(TF_Operation* oper, int i, char* output,
                             TF_Status* status) {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
  status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th");
}
    primitive_type: void
    function_declarator: TF_OperationGetAttrName(TF_Operation* oper, int i, char* output,
                             TF_Status* status)
     identifier: TF_OperationGetAttrName
     parameter_list: (TF_Operation* oper, int i, char* output,
                             TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: int i
       primitive_type: int
       identifier: i
      ,: ,
      parameter_declaration: char* output
       primitive_type: char
       pointer_declarator: * output
        *: *
        identifier: output
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  auto attrs = oper->node.attrs();
  int count = 0;
  AttrValueMap::const_iterator it;
  for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
  status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th");
}
     {: {
     declaration: auto attrs = oper->node.attrs();
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: attrs = oper->node.attrs()
       identifier: attrs
       =: =
       call_expression: oper->node.attrs()
        field_expression: oper->node.attrs
         field_expression: oper->node
          identifier: oper
          ->: ->
          field_identifier: node
         .: .
         field_identifier: attrs
        argument_list: ()
         (: (
         ): )
      ;: ;
     declaration: int count = 0;
      primitive_type: int
      init_declarator: count = 0
       identifier: count
       =: =
       number_literal: 0
      ;: ;
     declaration: AttrValueMap::const_iterator it;
      qualified_identifier: AttrValueMap::const_iterator
       namespace_identifier: AttrValueMap
       ::: ::
       type_identifier: const_iterator
      identifier: it
      ;: ;
     for_statement: for (it = attrs.begin(); it != attrs.end(); it++) {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
      for: for
      (: (
      assignment_expression: it = attrs.begin()
       identifier: it
       =: =
       call_expression: attrs.begin()
        field_expression: attrs.begin
         identifier: attrs
         .: .
         field_identifier: begin
        argument_list: ()
         (: (
         ): )
      ;: ;
      binary_expression: it != attrs.end()
       identifier: it
       !=: !=
       call_expression: attrs.end()
        field_expression: attrs.end
         identifier: attrs
         .: .
         field_identifier: end
        argument_list: ()
         (: (
         ): )
      ;: ;
      update_expression: it++
       identifier: it
       ++: ++
      ): )
      compound_statement: {
    if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
    count++;
  }
       {: {
       if_statement: if (count == i) {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
        if: if
        condition_clause: (count == i)
         (: (
         binary_expression: count == i
          identifier: count
          ==: ==
          identifier: i
         ): )
        compound_statement: {
      strncpy(output, it->first.c_str(), it->first.length());
      status->status = absl::OkStatus();
      return;
    }
         {: {
         expression_statement: strncpy(output, it->first.c_str(), it->first.length());
          call_expression: strncpy(output, it->first.c_str(), it->first.length())
           identifier: strncpy
           argument_list: (output, it->first.c_str(), it->first.length())
            (: (
            identifier: output
            ,: ,
            call_expression: it->first.c_str()
             field_expression: it->first.c_str
              field_expression: it->first
               identifier: it
               ->: ->
               field_identifier: first
              .: .
              field_identifier: c_str
             argument_list: ()
              (: (
              ): )
            ,: ,
            call_expression: it->first.length()
             field_expression: it->first.length
              field_expression: it->first
               identifier: it
               ->: ->
               field_identifier: first
              .: .
              field_identifier: length
             argument_list: ()
              (: (
              ): )
            ): )
          ;: ;
         expression_statement: status->status = absl::OkStatus();
          assignment_expression: status->status = absl::OkStatus()
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: absl::OkStatus()
            qualified_identifier: absl::OkStatus
             namespace_identifier: absl
             ::: ::
             identifier: OkStatus
            argument_list: ()
             (: (
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       expression_statement: count++;
        update_expression: count++
         identifier: count
         ++: ++
        ;: ;
       }: }
     expression_statement: status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th");
      assignment_expression: status->status = OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th")
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: OutOfRange("Operation only has ", count,
                              " attributes, can't get the ", i, "th")
        identifier: OutOfRange
        argument_list: ("Operation only has ", count,
                              " attributes, can't get the ", i, "th")
         (: (
         string_literal: "Operation only has "
          ": "
          string_content: Operation only has 
          ": "
         ,: ,
         identifier: count
         ,: ,
         string_literal: " attributes, can't get the "
          ": "
          string_content:  attributes, can't get the 
          ": "
         ,: ,
         identifier: i
         ,: ,
         string_literal: "th"
          ": "
          string_content: th
          ": "
         ): )
      ;: ;
     }: }
   function_definition: void TF_OperationToNodeDef(TF_Operation* oper, TF_Buffer* output_node_def,
                           TF_Status* status) {
  status->status = MessageToBuffer(oper->node.def(), output_node_def);
}
    primitive_type: void
    function_declarator: TF_OperationToNodeDef(TF_Operation* oper, TF_Buffer* output_node_def,
                           TF_Status* status)
     identifier: TF_OperationToNodeDef
     parameter_list: (TF_Operation* oper, TF_Buffer* output_node_def,
                           TF_Status* status)
      (: (
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ,: ,
      parameter_declaration: TF_Buffer* output_node_def
       type_identifier: TF_Buffer
       pointer_declarator: * output_node_def
        *: *
        identifier: output_node_def
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  status->status = MessageToBuffer(oper->node.def(), output_node_def);
}
     {: {
     expression_statement: status->status = MessageToBuffer(oper->node.def(), output_node_def);
      assignment_expression: status->status = MessageToBuffer(oper->node.def(), output_node_def)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(oper->node.def(), output_node_def)
        identifier: MessageToBuffer
        argument_list: (oper->node.def(), output_node_def)
         (: (
         call_expression: oper->node.def()
          field_expression: oper->node.def
           field_expression: oper->node
            identifier: oper
            ->: ->
            field_identifier: node
           .: .
           field_identifier: def
          argument_list: ()
           (: (
           ): )
         ,: ,
         identifier: output_node_def
         ): )
      ;: ;
     }: }
   comment: // TF_Graph functions ---------------------------------------------------------
   function_definition: TF_Graph::TF_Graph()
    : graph(tensorflow::OpRegistry::Global()),
      refiner(graph.versions().producer(), graph.op_registry()),
      delete_requested(false),
      parent(nullptr),
      parent_inputs(nullptr) {
  // Tell the shape refiner to also run shape inference on functions.
  refiner.set_function_library_for_shape_inference(&graph.flib_def());
}
    function_declarator: TF_Graph::TF_Graph()
     qualified_identifier: TF_Graph::TF_Graph
      namespace_identifier: TF_Graph
      ::: ::
      identifier: TF_Graph
     parameter_list: ()
      (: (
      ): )
    field_initializer_list: : graph(tensorflow::OpRegistry::Global()),
      refiner(graph.versions().producer(), graph.op_registry()),
      delete_requested(false),
      parent(nullptr),
      parent_inputs(nullptr)
     :: :
     field_initializer: graph(tensorflow::OpRegistry::Global())
      field_identifier: graph
      argument_list: (tensorflow::OpRegistry::Global())
       (: (
       call_expression: tensorflow::OpRegistry::Global()
        qualified_identifier: tensorflow::OpRegistry::Global
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: OpRegistry::Global
          namespace_identifier: OpRegistry
          ::: ::
          identifier: Global
        argument_list: ()
         (: (
         ): )
       ): )
     ,: ,
     field_initializer: refiner(graph.versions().producer(), graph.op_registry())
      field_identifier: refiner
      argument_list: (graph.versions().producer(), graph.op_registry())
       (: (
       call_expression: graph.versions().producer()
        field_expression: graph.versions().producer
         call_expression: graph.versions()
          field_expression: graph.versions
           identifier: graph
           .: .
           field_identifier: versions
          argument_list: ()
           (: (
           ): )
         .: .
         field_identifier: producer
        argument_list: ()
         (: (
         ): )
       ,: ,
       call_expression: graph.op_registry()
        field_expression: graph.op_registry
         identifier: graph
         .: .
         field_identifier: op_registry
        argument_list: ()
         (: (
         ): )
       ): )
     ,: ,
     field_initializer: delete_requested(false)
      field_identifier: delete_requested
      argument_list: (false)
       (: (
       false: false
       ): )
     ,: ,
     field_initializer: parent(nullptr)
      field_identifier: parent
      argument_list: (nullptr)
       (: (
       null: nullptr
        nullptr: nullptr
       ): )
     ,: ,
     field_initializer: parent_inputs(nullptr)
      field_identifier: parent_inputs
      argument_list: (nullptr)
       (: (
       null: nullptr
        nullptr: nullptr
       ): )
    compound_statement: {
  // Tell the shape refiner to also run shape inference on functions.
  refiner.set_function_library_for_shape_inference(&graph.flib_def());
}
     {: {
     comment: // Tell the shape refiner to also run shape inference on functions.
     expression_statement: refiner.set_function_library_for_shape_inference(&graph.flib_def());
      call_expression: refiner.set_function_library_for_shape_inference(&graph.flib_def())
       field_expression: refiner.set_function_library_for_shape_inference
        identifier: refiner
        .: .
        field_identifier: set_function_library_for_shape_inference
       argument_list: (&graph.flib_def())
        (: (
        pointer_expression: &graph.flib_def()
         &: &
         call_expression: graph.flib_def()
          field_expression: graph.flib_def
           identifier: graph
           .: .
           field_identifier: flib_def
          argument_list: ()
           (: (
           ): )
        ): )
      ;: ;
     }: }
   function_definition: TF_Graph* TF_NewGraph() { return new TF_Graph; }
    type_identifier: TF_Graph
    pointer_declarator: * TF_NewGraph()
     *: *
     function_declarator: TF_NewGraph()
      identifier: TF_NewGraph
      parameter_list: ()
       (: (
       ): )
    compound_statement: { return new TF_Graph; }
     {: {
     return_statement: return new TF_Graph;
      return: return
      new_expression: new TF_Graph
       new: new
       type_identifier: TF_Graph
      ;: ;
     }: }
   function_definition: void TF_DeleteGraph(TF_Graph* g) {
  if (g == nullptr) return;
  g->mu.lock();
  g->delete_requested = true;
  const bool del = g->sessions.empty();
  g->mu.unlock();
  if (del) delete g;
}
    primitive_type: void
    function_declarator: TF_DeleteGraph(TF_Graph* g)
     identifier: TF_DeleteGraph
     parameter_list: (TF_Graph* g)
      (: (
      parameter_declaration: TF_Graph* g
       type_identifier: TF_Graph
       pointer_declarator: * g
        *: *
        identifier: g
      ): )
    compound_statement: {
  if (g == nullptr) return;
  g->mu.lock();
  g->delete_requested = true;
  const bool del = g->sessions.empty();
  g->mu.unlock();
  if (del) delete g;
}
     {: {
     if_statement: if (g == nullptr) return;
      if: if
      condition_clause: (g == nullptr)
       (: (
       binary_expression: g == nullptr
        identifier: g
        ==: ==
        null: nullptr
         nullptr: nullptr
       ): )
      return_statement: return;
       return: return
       ;: ;
     expression_statement: g->mu.lock();
      call_expression: g->mu.lock()
       field_expression: g->mu.lock
        field_expression: g->mu
         identifier: g
         ->: ->
         field_identifier: mu
        .: .
        field_identifier: lock
       argument_list: ()
        (: (
        ): )
      ;: ;
     expression_statement: g->delete_requested = true;
      assignment_expression: g->delete_requested = true
       field_expression: g->delete_requested
        identifier: g
        ->: ->
        field_identifier: delete_requested
       =: =
       true: true
      ;: ;
     declaration: const bool del = g->sessions.empty();
      type_qualifier: const
       const: const
      primitive_type: bool
      init_declarator: del = g->sessions.empty()
       identifier: del
       =: =
       call_expression: g->sessions.empty()
        field_expression: g->sessions.empty
         field_expression: g->sessions
          identifier: g
          ->: ->
          field_identifier: sessions
         .: .
         field_identifier: empty
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: g->mu.unlock();
      call_expression: g->mu.unlock()
       field_expression: g->mu.unlock
        field_expression: g->mu
         identifier: g
         ->: ->
         field_identifier: mu
        .: .
        field_identifier: unlock
       argument_list: ()
        (: (
        ): )
      ;: ;
     if_statement: if (del) delete g;
      if: if
      condition_clause: (del)
       (: (
       identifier: del
       ): )
      expression_statement: delete g;
       delete_expression: delete g
        delete: delete
        identifier: g
       ;: ;
     }: }
   function_definition: TF_Operation* TF_GraphOperationByName(TF_Graph* graph, const char* oper_name) {
  mutex_lock l(graph->mu);
  auto iter = graph->name_map.find(oper_name);
  if (iter == graph->name_map.end()) {
    return nullptr;
  } else {
    return ToOperation(iter->second);
  }
}
    type_identifier: TF_Operation
    pointer_declarator: * TF_GraphOperationByName(TF_Graph* graph, const char* oper_name)
     *: *
     function_declarator: TF_GraphOperationByName(TF_Graph* graph, const char* oper_name)
      identifier: TF_GraphOperationByName
      parameter_list: (TF_Graph* graph, const char* oper_name)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: const char* oper_name
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * oper_name
         *: *
         identifier: oper_name
       ): )
    compound_statement: {
  mutex_lock l(graph->mu);
  auto iter = graph->name_map.find(oper_name);
  if (iter == graph->name_map.end()) {
    return nullptr;
  } else {
    return ToOperation(iter->second);
  }
}
     {: {
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: auto iter = graph->name_map.find(oper_name);
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: iter = graph->name_map.find(oper_name)
       identifier: iter
       =: =
       call_expression: graph->name_map.find(oper_name)
        field_expression: graph->name_map.find
         field_expression: graph->name_map
          identifier: graph
          ->: ->
          field_identifier: name_map
         .: .
         field_identifier: find
        argument_list: (oper_name)
         (: (
         identifier: oper_name
         ): )
      ;: ;
     if_statement: if (iter == graph->name_map.end()) {
    return nullptr;
  } else {
    return ToOperation(iter->second);
  }
      if: if
      condition_clause: (iter == graph->name_map.end())
       (: (
       binary_expression: iter == graph->name_map.end()
        identifier: iter
        ==: ==
        call_expression: graph->name_map.end()
         field_expression: graph->name_map.end
          field_expression: graph->name_map
           identifier: graph
           ->: ->
           field_identifier: name_map
          .: .
          field_identifier: end
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    return nullptr;
  }
       {: {
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
      else_clause: else {
    return ToOperation(iter->second);
  }
       else: else
       compound_statement: {
    return ToOperation(iter->second);
  }
        {: {
        return_statement: return ToOperation(iter->second);
         return: return
         call_expression: ToOperation(iter->second)
          identifier: ToOperation
          argument_list: (iter->second)
           (: (
           field_expression: iter->second
            identifier: iter
            ->: ->
            field_identifier: second
           ): )
         ;: ;
        }: }
     }: }
   function_definition: TF_Operation* TF_GraphNextOperation(TF_Graph* graph, size_t* pos) {
  if (*pos == 0) {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  } else {
    // Advance to the next node.
    *pos += 1;
  }

  mutex_lock l(graph->mu);
  while (*pos < static_cast<size_t>(graph->graph.num_node_ids())) {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }

  // No more nodes.
  return nullptr;
}
    type_identifier: TF_Operation
    pointer_declarator: * TF_GraphNextOperation(TF_Graph* graph, size_t* pos)
     *: *
     function_declarator: TF_GraphNextOperation(TF_Graph* graph, size_t* pos)
      identifier: TF_GraphNextOperation
      parameter_list: (TF_Graph* graph, size_t* pos)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: size_t* pos
        primitive_type: size_t
        pointer_declarator: * pos
         *: *
         identifier: pos
       ): )
    compound_statement: {
  if (*pos == 0) {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  } else {
    // Advance to the next node.
    *pos += 1;
  }

  mutex_lock l(graph->mu);
  while (*pos < static_cast<size_t>(graph->graph.num_node_ids())) {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }

  // No more nodes.
  return nullptr;
}
     {: {
     if_statement: if (*pos == 0) {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  } else {
    // Advance to the next node.
    *pos += 1;
  }
      if: if
      condition_clause: (*pos == 0)
       (: (
       binary_expression: *pos == 0
        pointer_expression: *pos
         *: *
         identifier: pos
        ==: ==
        number_literal: 0
       ): )
      compound_statement: {
    // Advance past the first sentinel nodes in every graph (the source & sink).
    *pos += 2;
  }
       {: {
       comment: // Advance past the first sentinel nodes in every graph (the source & sink).
       expression_statement: *pos += 2;
        assignment_expression: *pos += 2
         pointer_expression: *pos
          *: *
          identifier: pos
         +=: +=
         number_literal: 2
        ;: ;
       }: }
      else_clause: else {
    // Advance to the next node.
    *pos += 1;
  }
       else: else
       compound_statement: {
    // Advance to the next node.
    *pos += 1;
  }
        {: {
        comment: // Advance to the next node.
        expression_statement: *pos += 1;
         assignment_expression: *pos += 1
          pointer_expression: *pos
           *: *
           identifier: pos
          +=: +=
          number_literal: 1
         ;: ;
        }: }
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     while_statement: while (*pos < static_cast<size_t>(graph->graph.num_node_ids())) {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }
      while: while
      condition_clause: (*pos < static_cast<size_t>(graph->graph.num_node_ids()))
       (: (
       binary_expression: *pos < static_cast<size_t>(graph->graph.num_node_ids())
        pointer_expression: *pos
         *: *
         identifier: pos
        <: <
        call_expression: static_cast<size_t>(graph->graph.num_node_ids())
         template_function: static_cast<size_t>
          identifier: static_cast
          template_argument_list: <size_t>
           <: <
           type_descriptor: size_t
            primitive_type: size_t
           >: >
         argument_list: (graph->graph.num_node_ids())
          (: (
          call_expression: graph->graph.num_node_ids()
           field_expression: graph->graph.num_node_ids
            field_expression: graph->graph
             identifier: graph
             ->: ->
             field_identifier: graph
            .: .
            field_identifier: num_node_ids
           argument_list: ()
            (: (
            ): )
          ): )
       ): )
      compound_statement: {
    Node* node = graph->graph.FindNodeId(*pos);
    // FindNodeId() returns nullptr for nodes that have been deleted.
    // We aren't currently allowing nodes to be deleted, but it is safer
    // to still check.
    if (node != nullptr) return ToOperation(node);
    *pos += 1;
  }
       {: {
       declaration: Node* node = graph->graph.FindNodeId(*pos);
        type_identifier: Node
        init_declarator: * node = graph->graph.FindNodeId(*pos)
         pointer_declarator: * node
          *: *
          identifier: node
         =: =
         call_expression: graph->graph.FindNodeId(*pos)
          field_expression: graph->graph.FindNodeId
           field_expression: graph->graph
            identifier: graph
            ->: ->
            field_identifier: graph
           .: .
           field_identifier: FindNodeId
          argument_list: (*pos)
           (: (
           pointer_expression: *pos
            *: *
            identifier: pos
           ): )
        ;: ;
       comment: // FindNodeId() returns nullptr for nodes that have been deleted.
       comment: // We aren't currently allowing nodes to be deleted, but it is safer
       comment: // to still check.
       if_statement: if (node != nullptr) return ToOperation(node);
        if: if
        condition_clause: (node != nullptr)
         (: (
         binary_expression: node != nullptr
          identifier: node
          !=: !=
          null: nullptr
           nullptr: nullptr
         ): )
        return_statement: return ToOperation(node);
         return: return
         call_expression: ToOperation(node)
          identifier: ToOperation
          argument_list: (node)
           (: (
           identifier: node
           ): )
         ;: ;
       expression_statement: *pos += 1;
        assignment_expression: *pos += 1
         pointer_expression: *pos
          *: *
          identifier: pos
         +=: +=
         number_literal: 1
        ;: ;
       }: }
     comment: // No more nodes.
     return_statement: return nullptr;
      return: return
      null: nullptr
       nullptr: nullptr
      ;: ;
     }: }
   function_definition: void TF_GraphToGraphDef(TF_Graph* graph, TF_Buffer* output_graph_def,
                        TF_Status* status) {
  GraphDef def;
  {
    mutex_lock l(graph->mu);
    graph->graph.ToGraphDef(&def);
  }
  status->status = MessageToBuffer(def, output_graph_def);
}
    primitive_type: void
    function_declarator: TF_GraphToGraphDef(TF_Graph* graph, TF_Buffer* output_graph_def,
                        TF_Status* status)
     identifier: TF_GraphToGraphDef
     parameter_list: (TF_Graph* graph, TF_Buffer* output_graph_def,
                        TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Buffer* output_graph_def
       type_identifier: TF_Buffer
       pointer_declarator: * output_graph_def
        *: *
        identifier: output_graph_def
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  GraphDef def;
  {
    mutex_lock l(graph->mu);
    graph->graph.ToGraphDef(&def);
  }
  status->status = MessageToBuffer(def, output_graph_def);
}
     {: {
     declaration: GraphDef def;
      type_identifier: GraphDef
      identifier: def
      ;: ;
     compound_statement: {
    mutex_lock l(graph->mu);
    graph->graph.ToGraphDef(&def);
  }
      {: {
      declaration: mutex_lock l(graph->mu);
       type_identifier: mutex_lock
       init_declarator: l(graph->mu)
        identifier: l
        argument_list: (graph->mu)
         (: (
         field_expression: graph->mu
          identifier: graph
          ->: ->
          field_identifier: mu
         ): )
       ;: ;
      expression_statement: graph->graph.ToGraphDef(&def);
       call_expression: graph->graph.ToGraphDef(&def)
        field_expression: graph->graph.ToGraphDef
         field_expression: graph->graph
          identifier: graph
          ->: ->
          field_identifier: graph
         .: .
         field_identifier: ToGraphDef
        argument_list: (&def)
         (: (
         pointer_expression: &def
          &: &
          identifier: def
         ): )
       ;: ;
      }: }
     expression_statement: status->status = MessageToBuffer(def, output_graph_def);
      assignment_expression: status->status = MessageToBuffer(def, output_graph_def)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(def, output_graph_def)
        identifier: MessageToBuffer
        argument_list: (def, output_graph_def)
         (: (
         identifier: def
         ,: ,
         identifier: output_graph_def
         ): )
      ;: ;
     }: }
   function_definition: void TF_GraphGetOpDef(TF_Graph* graph, const char* op_name,
                      TF_Buffer* output_op_def, TF_Status* status) {
  const OpDef* op_def;
  {
    mutex_lock l(graph->mu);
    status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
    if (!status->status.ok()) return;
  }
  status->status = MessageToBuffer(*op_def, output_op_def);
}
    primitive_type: void
    function_declarator: TF_GraphGetOpDef(TF_Graph* graph, const char* op_name,
                      TF_Buffer* output_op_def, TF_Status* status)
     identifier: TF_GraphGetOpDef
     parameter_list: (TF_Graph* graph, const char* op_name,
                      TF_Buffer* output_op_def, TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: const char* op_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * op_name
        *: *
        identifier: op_name
      ,: ,
      parameter_declaration: TF_Buffer* output_op_def
       type_identifier: TF_Buffer
       pointer_declarator: * output_op_def
        *: *
        identifier: output_op_def
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  const OpDef* op_def;
  {
    mutex_lock l(graph->mu);
    status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
    if (!status->status.ok()) return;
  }
  status->status = MessageToBuffer(*op_def, output_op_def);
}
     {: {
     declaration: const OpDef* op_def;
      type_qualifier: const
       const: const
      type_identifier: OpDef
      pointer_declarator: * op_def
       *: *
       identifier: op_def
      ;: ;
     compound_statement: {
    mutex_lock l(graph->mu);
    status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
    if (!status->status.ok()) return;
  }
      {: {
      declaration: mutex_lock l(graph->mu);
       type_identifier: mutex_lock
       init_declarator: l(graph->mu)
        identifier: l
        argument_list: (graph->mu)
         (: (
         field_expression: graph->mu
          identifier: graph
          ->: ->
          field_identifier: mu
         ): )
       ;: ;
      expression_statement: status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def);
       assignment_expression: status->status = graph->graph.op_registry()->LookUpOpDef(op_name, &op_def)
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: graph->graph.op_registry()->LookUpOpDef(op_name, &op_def)
         field_expression: graph->graph.op_registry()->LookUpOpDef
          call_expression: graph->graph.op_registry()
           field_expression: graph->graph.op_registry
            field_expression: graph->graph
             identifier: graph
             ->: ->
             field_identifier: graph
            .: .
            field_identifier: op_registry
           argument_list: ()
            (: (
            ): )
          ->: ->
          field_identifier: LookUpOpDef
         argument_list: (op_name, &op_def)
          (: (
          identifier: op_name
          ,: ,
          pointer_expression: &op_def
           &: &
           identifier: op_def
          ): )
       ;: ;
      if_statement: if (!status->status.ok()) return;
       if: if
       condition_clause: (!status->status.ok())
        (: (
        unary_expression: !status->status.ok()
         !: !
         call_expression: status->status.ok()
          field_expression: status->status.ok
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           .: .
           field_identifier: ok
          argument_list: ()
           (: (
           ): )
        ): )
       return_statement: return;
        return: return
        ;: ;
      }: }
     expression_statement: status->status = MessageToBuffer(*op_def, output_op_def);
      assignment_expression: status->status = MessageToBuffer(*op_def, output_op_def)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(*op_def, output_op_def)
        identifier: MessageToBuffer
        argument_list: (*op_def, output_op_def)
         (: (
         pointer_expression: *op_def
          *: *
          identifier: op_def
         ,: ,
         identifier: output_op_def
         ): )
      ;: ;
     }: }
   function_definition: void TF_GraphVersions(TF_Graph* graph, TF_Buffer* output_version_def,
                      TF_Status* status) {
  VersionDef versions;
  {
    mutex_lock l(graph->mu);
    versions = graph->graph.versions();
  }
  status->status = MessageToBuffer(versions, output_version_def);
}
    primitive_type: void
    function_declarator: TF_GraphVersions(TF_Graph* graph, TF_Buffer* output_version_def,
                      TF_Status* status)
     identifier: TF_GraphVersions
     parameter_list: (TF_Graph* graph, TF_Buffer* output_version_def,
                      TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Buffer* output_version_def
       type_identifier: TF_Buffer
       pointer_declarator: * output_version_def
        *: *
        identifier: output_version_def
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  VersionDef versions;
  {
    mutex_lock l(graph->mu);
    versions = graph->graph.versions();
  }
  status->status = MessageToBuffer(versions, output_version_def);
}
     {: {
     declaration: VersionDef versions;
      type_identifier: VersionDef
      identifier: versions
      ;: ;
     compound_statement: {
    mutex_lock l(graph->mu);
    versions = graph->graph.versions();
  }
      {: {
      declaration: mutex_lock l(graph->mu);
       type_identifier: mutex_lock
       init_declarator: l(graph->mu)
        identifier: l
        argument_list: (graph->mu)
         (: (
         field_expression: graph->mu
          identifier: graph
          ->: ->
          field_identifier: mu
         ): )
       ;: ;
      expression_statement: versions = graph->graph.versions();
       assignment_expression: versions = graph->graph.versions()
        identifier: versions
        =: =
        call_expression: graph->graph.versions()
         field_expression: graph->graph.versions
          field_expression: graph->graph
           identifier: graph
           ->: ->
           field_identifier: graph
          .: .
          field_identifier: versions
         argument_list: ()
          (: (
          ): )
       ;: ;
      }: }
     expression_statement: status->status = MessageToBuffer(versions, output_version_def);
      assignment_expression: status->status = MessageToBuffer(versions, output_version_def)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(versions, output_version_def)
        identifier: MessageToBuffer
        argument_list: (versions, output_version_def)
         (: (
         identifier: versions
         ,: ,
         identifier: output_version_def
         ): )
      ;: ;
     }: }
   function_definition: TF_ImportGraphDefOptions* TF_NewImportGraphDefOptions() {
  return new TF_ImportGraphDefOptions;
}
    type_identifier: TF_ImportGraphDefOptions
    pointer_declarator: * TF_NewImportGraphDefOptions()
     *: *
     function_declarator: TF_NewImportGraphDefOptions()
      identifier: TF_NewImportGraphDefOptions
      parameter_list: ()
       (: (
       ): )
    compound_statement: {
  return new TF_ImportGraphDefOptions;
}
     {: {
     return_statement: return new TF_ImportGraphDefOptions;
      return: return
      new_expression: new TF_ImportGraphDefOptions
       new: new
       type_identifier: TF_ImportGraphDefOptions
      ;: ;
     }: }
   function_definition: void TF_DeleteImportGraphDefOptions(TF_ImportGraphDefOptions* opts) {
  delete opts;
}
    primitive_type: void
    function_declarator: TF_DeleteImportGraphDefOptions(TF_ImportGraphDefOptions* opts)
     identifier: TF_DeleteImportGraphDefOptions
     parameter_list: (TF_ImportGraphDefOptions* opts)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ): )
    compound_statement: {
  delete opts;
}
     {: {
     expression_statement: delete opts;
      delete_expression: delete opts
       delete: delete
       identifier: opts
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsSetPrefix(TF_ImportGraphDefOptions* opts,
                                       const char* prefix) {
  opts->opts.prefix = prefix;
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsSetPrefix(TF_ImportGraphDefOptions* opts,
                                       const char* prefix)
     identifier: TF_ImportGraphDefOptionsSetPrefix
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                       const char* prefix)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: const char* prefix
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * prefix
        *: *
        identifier: prefix
      ): )
    compound_statement: {
  opts->opts.prefix = prefix;
}
     {: {
     expression_statement: opts->opts.prefix = prefix;
      assignment_expression: opts->opts.prefix = prefix
       field_expression: opts->opts.prefix
        field_expression: opts->opts
         identifier: opts
         ->: ->
         field_identifier: opts
        .: .
        field_identifier: prefix
       =: =
       identifier: prefix
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsSetDefaultDevice(TF_ImportGraphDefOptions* opts,
                                              const char* device) {
  opts->opts.default_device = device;
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsSetDefaultDevice(TF_ImportGraphDefOptions* opts,
                                              const char* device)
     identifier: TF_ImportGraphDefOptionsSetDefaultDevice
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                              const char* device)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: const char* device
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * device
        *: *
        identifier: device
      ): )
    compound_statement: {
  opts->opts.default_device = device;
}
     {: {
     expression_statement: opts->opts.default_device = device;
      assignment_expression: opts->opts.default_device = device
       field_expression: opts->opts.default_device
        field_expression: opts->opts
         identifier: opts
         ->: ->
         field_identifier: opts
        .: .
        field_identifier: default_device
       =: =
       identifier: device
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsSetUniquifyNames(TF_ImportGraphDefOptions* opts,
                                              unsigned char uniquify_names) {
  opts->opts.uniquify_names = uniquify_names;
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsSetUniquifyNames(TF_ImportGraphDefOptions* opts,
                                              unsigned char uniquify_names)
     identifier: TF_ImportGraphDefOptionsSetUniquifyNames
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                              unsigned char uniquify_names)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: unsigned char uniquify_names
       sized_type_specifier: unsigned char
        unsigned: unsigned
        primitive_type: char
       identifier: uniquify_names
      ): )
    compound_statement: {
  opts->opts.uniquify_names = uniquify_names;
}
     {: {
     expression_statement: opts->opts.uniquify_names = uniquify_names;
      assignment_expression: opts->opts.uniquify_names = uniquify_names
       field_expression: opts->opts.uniquify_names
        field_expression: opts->opts
         identifier: opts
         ->: ->
         field_identifier: opts
        .: .
        field_identifier: uniquify_names
       =: =
       identifier: uniquify_names
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsSetUniquifyPrefix(TF_ImportGraphDefOptions* opts,
                                               unsigned char uniquify_prefix) {
  opts->opts.uniquify_prefix = uniquify_prefix;
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsSetUniquifyPrefix(TF_ImportGraphDefOptions* opts,
                                               unsigned char uniquify_prefix)
     identifier: TF_ImportGraphDefOptionsSetUniquifyPrefix
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                               unsigned char uniquify_prefix)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: unsigned char uniquify_prefix
       sized_type_specifier: unsigned char
        unsigned: unsigned
        primitive_type: char
       identifier: uniquify_prefix
      ): )
    compound_statement: {
  opts->opts.uniquify_prefix = uniquify_prefix;
}
     {: {
     expression_statement: opts->opts.uniquify_prefix = uniquify_prefix;
      assignment_expression: opts->opts.uniquify_prefix = uniquify_prefix
       field_expression: opts->opts.uniquify_prefix
        field_expression: opts->opts
         identifier: opts
         ->: ->
         field_identifier: opts
        .: .
        field_identifier: uniquify_prefix
       =: =
       identifier: uniquify_prefix
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsAddInputMapping(TF_ImportGraphDefOptions* opts,
                                             const char* src_name,
                                             int src_index, TF_Output dst) {
  opts->tensor_id_data.push_back(src_name);
  const string& src_name_str = opts->tensor_id_data.back();
  // We don't need to store dst's name in tensor_id_data, since `dst` must
  // outlive the ImportGraphDef call.
  opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst);
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsAddInputMapping(TF_ImportGraphDefOptions* opts,
                                             const char* src_name,
                                             int src_index, TF_Output dst)
     identifier: TF_ImportGraphDefOptionsAddInputMapping
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                             const char* src_name,
                                             int src_index, TF_Output dst)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: const char* src_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * src_name
        *: *
        identifier: src_name
      ,: ,
      parameter_declaration: int src_index
       primitive_type: int
       identifier: src_index
      ,: ,
      parameter_declaration: TF_Output dst
       type_identifier: TF_Output
       identifier: dst
      ): )
    compound_statement: {
  opts->tensor_id_data.push_back(src_name);
  const string& src_name_str = opts->tensor_id_data.back();
  // We don't need to store dst's name in tensor_id_data, since `dst` must
  // outlive the ImportGraphDef call.
  opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst);
}
     {: {
     expression_statement: opts->tensor_id_data.push_back(src_name);
      call_expression: opts->tensor_id_data.push_back(src_name)
       field_expression: opts->tensor_id_data.push_back
        field_expression: opts->tensor_id_data
         identifier: opts
         ->: ->
         field_identifier: tensor_id_data
        .: .
        field_identifier: push_back
       argument_list: (src_name)
        (: (
        identifier: src_name
        ): )
      ;: ;
     declaration: const string& src_name_str = opts->tensor_id_data.back();
      type_qualifier: const
       const: const
      type_identifier: string
      init_declarator: & src_name_str = opts->tensor_id_data.back()
       reference_declarator: & src_name_str
        &: &
        identifier: src_name_str
       =: =
       call_expression: opts->tensor_id_data.back()
        field_expression: opts->tensor_id_data.back
         field_expression: opts->tensor_id_data
          identifier: opts
          ->: ->
          field_identifier: tensor_id_data
         .: .
         field_identifier: back
        argument_list: ()
         (: (
         ): )
      ;: ;
     comment: // We don't need to store dst's name in tensor_id_data, since `dst` must
     comment: // outlive the ImportGraphDef call.
     expression_statement: opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst);
      assignment_expression: opts->opts.input_map[TensorId(src_name_str, src_index)] = ToTensorId(dst)
       subscript_expression: opts->opts.input_map[TensorId(src_name_str, src_index)]
        field_expression: opts->opts.input_map
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: input_map
        subscript_argument_list: [TensorId(src_name_str, src_index)]
         [: [
         call_expression: TensorId(src_name_str, src_index)
          identifier: TensorId
          argument_list: (src_name_str, src_index)
           (: (
           identifier: src_name_str
           ,: ,
           identifier: src_index
           ): )
         ]: ]
       =: =
       call_expression: ToTensorId(dst)
        identifier: ToTensorId
        argument_list: (dst)
         (: (
         identifier: dst
         ): )
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsRemapControlDependency(
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst) {
  opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot);
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsRemapControlDependency(
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst)
     identifier: TF_ImportGraphDefOptionsRemapControlDependency
     parameter_list: (
    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: const char* src_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * src_name
        *: *
        identifier: src_name
      ,: ,
      parameter_declaration: TF_Operation* dst
       type_identifier: TF_Operation
       pointer_declarator: * dst
        *: *
        identifier: dst
      ): )
    compound_statement: {
  opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot);
}
     {: {
     expression_statement: opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot);
      assignment_expression: opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)] =
      TensorId(dst->node.name(), tensorflow::Graph::kControlSlot)
       subscript_expression: opts->opts.input_map[TensorId(src_name, tensorflow::Graph::kControlSlot)]
        field_expression: opts->opts.input_map
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: input_map
        subscript_argument_list: [TensorId(src_name, tensorflow::Graph::kControlSlot)]
         [: [
         call_expression: TensorId(src_name, tensorflow::Graph::kControlSlot)
          identifier: TensorId
          argument_list: (src_name, tensorflow::Graph::kControlSlot)
           (: (
           identifier: src_name
           ,: ,
           qualified_identifier: tensorflow::Graph::kControlSlot
            namespace_identifier: tensorflow
            ::: ::
            qualified_identifier: Graph::kControlSlot
             namespace_identifier: Graph
             ::: ::
             identifier: kControlSlot
           ): )
         ]: ]
       =: =
       call_expression: TensorId(dst->node.name(), tensorflow::Graph::kControlSlot)
        identifier: TensorId
        argument_list: (dst->node.name(), tensorflow::Graph::kControlSlot)
         (: (
         call_expression: dst->node.name()
          field_expression: dst->node.name
           field_expression: dst->node
            identifier: dst
            ->: ->
            field_identifier: node
           .: .
           field_identifier: name
          argument_list: ()
           (: (
           ): )
         ,: ,
         qualified_identifier: tensorflow::Graph::kControlSlot
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: Graph::kControlSlot
           namespace_identifier: Graph
           ::: ::
           identifier: kControlSlot
         ): )
      ;: ;
     }: }
   function_definition: extern void TF_ImportGraphDefOptionsAddControlDependency(
    TF_ImportGraphDefOptions* opts, TF_Operation* oper) {
  opts->opts.control_dependencies.push_back(oper->node.name());
}
    storage_class_specifier: extern
     extern: extern
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsAddControlDependency(
    TF_ImportGraphDefOptions* opts, TF_Operation* oper)
     identifier: TF_ImportGraphDefOptionsAddControlDependency
     parameter_list: (
    TF_ImportGraphDefOptions* opts, TF_Operation* oper)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: TF_Operation* oper
       type_identifier: TF_Operation
       pointer_declarator: * oper
        *: *
        identifier: oper
      ): )
    compound_statement: {
  opts->opts.control_dependencies.push_back(oper->node.name());
}
     {: {
     expression_statement: opts->opts.control_dependencies.push_back(oper->node.name());
      call_expression: opts->opts.control_dependencies.push_back(oper->node.name())
       field_expression: opts->opts.control_dependencies.push_back
        field_expression: opts->opts.control_dependencies
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: control_dependencies
        .: .
        field_identifier: push_back
       argument_list: (oper->node.name())
        (: (
        call_expression: oper->node.name()
         field_expression: oper->node.name
          field_expression: oper->node
           identifier: oper
           ->: ->
           field_identifier: node
          .: .
          field_identifier: name
         argument_list: ()
          (: (
          ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsAddReturnOutput(TF_ImportGraphDefOptions* opts,
                                             const char* oper_name, int index) {
  opts->tensor_id_data.push_back(oper_name);
  const string& oper_name_str = opts->tensor_id_data.back();
  opts->opts.return_tensors.emplace_back(oper_name_str, index);
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsAddReturnOutput(TF_ImportGraphDefOptions* opts,
                                             const char* oper_name, int index)
     identifier: TF_ImportGraphDefOptionsAddReturnOutput
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                             const char* oper_name, int index)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: const char* oper_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * oper_name
        *: *
        identifier: oper_name
      ,: ,
      parameter_declaration: int index
       primitive_type: int
       identifier: index
      ): )
    compound_statement: {
  opts->tensor_id_data.push_back(oper_name);
  const string& oper_name_str = opts->tensor_id_data.back();
  opts->opts.return_tensors.emplace_back(oper_name_str, index);
}
     {: {
     expression_statement: opts->tensor_id_data.push_back(oper_name);
      call_expression: opts->tensor_id_data.push_back(oper_name)
       field_expression: opts->tensor_id_data.push_back
        field_expression: opts->tensor_id_data
         identifier: opts
         ->: ->
         field_identifier: tensor_id_data
        .: .
        field_identifier: push_back
       argument_list: (oper_name)
        (: (
        identifier: oper_name
        ): )
      ;: ;
     declaration: const string& oper_name_str = opts->tensor_id_data.back();
      type_qualifier: const
       const: const
      type_identifier: string
      init_declarator: & oper_name_str = opts->tensor_id_data.back()
       reference_declarator: & oper_name_str
        &: &
        identifier: oper_name_str
       =: =
       call_expression: opts->tensor_id_data.back()
        field_expression: opts->tensor_id_data.back
         field_expression: opts->tensor_id_data
          identifier: opts
          ->: ->
          field_identifier: tensor_id_data
         .: .
         field_identifier: back
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: opts->opts.return_tensors.emplace_back(oper_name_str, index);
      call_expression: opts->opts.return_tensors.emplace_back(oper_name_str, index)
       field_expression: opts->opts.return_tensors.emplace_back
        field_expression: opts->opts.return_tensors
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: return_tensors
        .: .
        field_identifier: emplace_back
       argument_list: (oper_name_str, index)
        (: (
        identifier: oper_name_str
        ,: ,
        identifier: index
        ): )
      ;: ;
     }: }
   function_definition: int TF_ImportGraphDefOptionsNumReturnOutputs(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_tensors.size();
}
    primitive_type: int
    function_declarator: TF_ImportGraphDefOptionsNumReturnOutputs(
    const TF_ImportGraphDefOptions* opts)
     identifier: TF_ImportGraphDefOptionsNumReturnOutputs
     parameter_list: (
    const TF_ImportGraphDefOptions* opts)
      (: (
      parameter_declaration: const TF_ImportGraphDefOptions* opts
       type_qualifier: const
        const: const
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ): )
    compound_statement: {
  return opts->opts.return_tensors.size();
}
     {: {
     return_statement: return opts->opts.return_tensors.size();
      return: return
      call_expression: opts->opts.return_tensors.size()
       field_expression: opts->opts.return_tensors.size
        field_expression: opts->opts.return_tensors
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: return_tensors
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefOptionsAddReturnOperation(TF_ImportGraphDefOptions* opts,
                                                const char* oper_name) {
  opts->opts.return_nodes.push_back(oper_name);
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefOptionsAddReturnOperation(TF_ImportGraphDefOptions* opts,
                                                const char* oper_name)
     identifier: TF_ImportGraphDefOptionsAddReturnOperation
     parameter_list: (TF_ImportGraphDefOptions* opts,
                                                const char* oper_name)
      (: (
      parameter_declaration: TF_ImportGraphDefOptions* opts
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: const char* oper_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * oper_name
        *: *
        identifier: oper_name
      ): )
    compound_statement: {
  opts->opts.return_nodes.push_back(oper_name);
}
     {: {
     expression_statement: opts->opts.return_nodes.push_back(oper_name);
      call_expression: opts->opts.return_nodes.push_back(oper_name)
       field_expression: opts->opts.return_nodes.push_back
        field_expression: opts->opts.return_nodes
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: return_nodes
        .: .
        field_identifier: push_back
       argument_list: (oper_name)
        (: (
        identifier: oper_name
        ): )
      ;: ;
     }: }
   function_definition: int TF_ImportGraphDefOptionsNumReturnOperations(
    const TF_ImportGraphDefOptions* opts) {
  return opts->opts.return_nodes.size();
}
    primitive_type: int
    function_declarator: TF_ImportGraphDefOptionsNumReturnOperations(
    const TF_ImportGraphDefOptions* opts)
     identifier: TF_ImportGraphDefOptionsNumReturnOperations
     parameter_list: (
    const TF_ImportGraphDefOptions* opts)
      (: (
      parameter_declaration: const TF_ImportGraphDefOptions* opts
       type_qualifier: const
        const: const
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ): )
    compound_statement: {
  return opts->opts.return_nodes.size();
}
     {: {
     return_statement: return opts->opts.return_nodes.size();
      return: return
      call_expression: opts->opts.return_nodes.size()
       field_expression: opts->opts.return_nodes.size
        field_expression: opts->opts.return_nodes
         field_expression: opts->opts
          identifier: opts
          ->: ->
          field_identifier: opts
         .: .
         field_identifier: return_nodes
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefResultsReturnOutputs(TF_ImportGraphDefResults* results,
                                           int* num_outputs,
                                           TF_Output** outputs) {
  *num_outputs = results->return_tensors.size();
  *outputs = results->return_tensors.data();
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefResultsReturnOutputs(TF_ImportGraphDefResults* results,
                                           int* num_outputs,
                                           TF_Output** outputs)
     identifier: TF_ImportGraphDefResultsReturnOutputs
     parameter_list: (TF_ImportGraphDefResults* results,
                                           int* num_outputs,
                                           TF_Output** outputs)
      (: (
      parameter_declaration: TF_ImportGraphDefResults* results
       type_identifier: TF_ImportGraphDefResults
       pointer_declarator: * results
        *: *
        identifier: results
      ,: ,
      parameter_declaration: int* num_outputs
       primitive_type: int
       pointer_declarator: * num_outputs
        *: *
        identifier: num_outputs
      ,: ,
      parameter_declaration: TF_Output** outputs
       type_identifier: TF_Output
       pointer_declarator: ** outputs
        *: *
        pointer_declarator: * outputs
         *: *
         identifier: outputs
      ): )
    compound_statement: {
  *num_outputs = results->return_tensors.size();
  *outputs = results->return_tensors.data();
}
     {: {
     expression_statement: *num_outputs = results->return_tensors.size();
      assignment_expression: *num_outputs = results->return_tensors.size()
       pointer_expression: *num_outputs
        *: *
        identifier: num_outputs
       =: =
       call_expression: results->return_tensors.size()
        field_expression: results->return_tensors.size
         field_expression: results->return_tensors
          identifier: results
          ->: ->
          field_identifier: return_tensors
         .: .
         field_identifier: size
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: *outputs = results->return_tensors.data();
      assignment_expression: *outputs = results->return_tensors.data()
       pointer_expression: *outputs
        *: *
        identifier: outputs
       =: =
       call_expression: results->return_tensors.data()
        field_expression: results->return_tensors.data
         field_expression: results->return_tensors
          identifier: results
          ->: ->
          field_identifier: return_tensors
         .: .
         field_identifier: data
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefResultsReturnOperations(TF_ImportGraphDefResults* results,
                                              int* num_opers,
                                              TF_Operation*** opers) {
  *num_opers = results->return_nodes.size();
  *opers = results->return_nodes.data();
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefResultsReturnOperations(TF_ImportGraphDefResults* results,
                                              int* num_opers,
                                              TF_Operation*** opers)
     identifier: TF_ImportGraphDefResultsReturnOperations
     parameter_list: (TF_ImportGraphDefResults* results,
                                              int* num_opers,
                                              TF_Operation*** opers)
      (: (
      parameter_declaration: TF_ImportGraphDefResults* results
       type_identifier: TF_ImportGraphDefResults
       pointer_declarator: * results
        *: *
        identifier: results
      ,: ,
      parameter_declaration: int* num_opers
       primitive_type: int
       pointer_declarator: * num_opers
        *: *
        identifier: num_opers
      ,: ,
      parameter_declaration: TF_Operation*** opers
       type_identifier: TF_Operation
       pointer_declarator: *** opers
        *: *
        pointer_declarator: ** opers
         *: *
         pointer_declarator: * opers
          *: *
          identifier: opers
      ): )
    compound_statement: {
  *num_opers = results->return_nodes.size();
  *opers = results->return_nodes.data();
}
     {: {
     expression_statement: *num_opers = results->return_nodes.size();
      assignment_expression: *num_opers = results->return_nodes.size()
       pointer_expression: *num_opers
        *: *
        identifier: num_opers
       =: =
       call_expression: results->return_nodes.size()
        field_expression: results->return_nodes.size
         field_expression: results->return_nodes
          identifier: results
          ->: ->
          field_identifier: return_nodes
         .: .
         field_identifier: size
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: *opers = results->return_nodes.data();
      assignment_expression: *opers = results->return_nodes.data()
       pointer_expression: *opers
        *: *
        identifier: opers
       =: =
       call_expression: results->return_nodes.data()
        field_expression: results->return_nodes.data
         field_expression: results->return_nodes
          identifier: results
          ->: ->
          field_identifier: return_nodes
         .: .
         field_identifier: data
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   function_definition: void TF_ImportGraphDefResultsMissingUnusedInputMappings(
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes) {
  *num_missing_unused_input_mappings = results->missing_unused_key_names.size();
  *src_names = results->missing_unused_key_names.data();
  *src_indexes = results->missing_unused_key_indexes.data();
}
    primitive_type: void
    function_declarator: TF_ImportGraphDefResultsMissingUnusedInputMappings(
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes)
     identifier: TF_ImportGraphDefResultsMissingUnusedInputMappings
     parameter_list: (
    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,
    const char*** src_names, int** src_indexes)
      (: (
      parameter_declaration: TF_ImportGraphDefResults* results
       type_identifier: TF_ImportGraphDefResults
       pointer_declarator: * results
        *: *
        identifier: results
      ,: ,
      parameter_declaration: int* num_missing_unused_input_mappings
       primitive_type: int
       pointer_declarator: * num_missing_unused_input_mappings
        *: *
        identifier: num_missing_unused_input_mappings
      ,: ,
      parameter_declaration: const char*** src_names
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: *** src_names
        *: *
        pointer_declarator: ** src_names
         *: *
         pointer_declarator: * src_names
          *: *
          identifier: src_names
      ,: ,
      parameter_declaration: int** src_indexes
       primitive_type: int
       pointer_declarator: ** src_indexes
        *: *
        pointer_declarator: * src_indexes
         *: *
         identifier: src_indexes
      ): )
    compound_statement: {
  *num_missing_unused_input_mappings = results->missing_unused_key_names.size();
  *src_names = results->missing_unused_key_names.data();
  *src_indexes = results->missing_unused_key_indexes.data();
}
     {: {
     expression_statement: *num_missing_unused_input_mappings = results->missing_unused_key_names.size();
      assignment_expression: *num_missing_unused_input_mappings = results->missing_unused_key_names.size()
       pointer_expression: *num_missing_unused_input_mappings
        *: *
        identifier: num_missing_unused_input_mappings
       =: =
       call_expression: results->missing_unused_key_names.size()
        field_expression: results->missing_unused_key_names.size
         field_expression: results->missing_unused_key_names
          identifier: results
          ->: ->
          field_identifier: missing_unused_key_names
         .: .
         field_identifier: size
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: *src_names = results->missing_unused_key_names.data();
      assignment_expression: *src_names = results->missing_unused_key_names.data()
       pointer_expression: *src_names
        *: *
        identifier: src_names
       =: =
       call_expression: results->missing_unused_key_names.data()
        field_expression: results->missing_unused_key_names.data
         field_expression: results->missing_unused_key_names
          identifier: results
          ->: ->
          field_identifier: missing_unused_key_names
         .: .
         field_identifier: data
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: *src_indexes = results->missing_unused_key_indexes.data();
      assignment_expression: *src_indexes = results->missing_unused_key_indexes.data()
       pointer_expression: *src_indexes
        *: *
        identifier: src_indexes
       =: =
       call_expression: results->missing_unused_key_indexes.data()
        field_expression: results->missing_unused_key_indexes.data
         field_expression: results->missing_unused_key_indexes
          identifier: results
          ->: ->
          field_identifier: missing_unused_key_indexes
         .: .
         field_identifier: data
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   function_definition: void TF_DeleteImportGraphDefResults(TF_ImportGraphDefResults* results) {
  delete results;
}
    primitive_type: void
    function_declarator: TF_DeleteImportGraphDefResults(TF_ImportGraphDefResults* results)
     identifier: TF_DeleteImportGraphDefResults
     parameter_list: (TF_ImportGraphDefResults* results)
      (: (
      parameter_declaration: TF_ImportGraphDefResults* results
       type_identifier: TF_ImportGraphDefResults
       pointer_declarator: * results
        *: *
        identifier: results
      ): )
    compound_statement: {
  delete results;
}
     {: {
     expression_statement: delete results;
      delete_expression: delete results
       delete: delete
       identifier: results
      ;: ;
     }: }
   declaration: static void GraphImportGraphDefLocked(TF_Graph* graph, const GraphDef& def,
                                      const TF_ImportGraphDefOptions* opts,
                                      TF_ImportGraphDefResults* tf_results,
                                      TF_Status* status)
    storage_class_specifier: static
     static: static
    primitive_type: void
    function_declarator: GraphImportGraphDefLocked(TF_Graph* graph, const GraphDef& def,
                                      const TF_ImportGraphDefOptions* opts,
                                      TF_ImportGraphDefResults* tf_results,
                                      TF_Status* status)
     identifier: GraphImportGraphDefLocked
     parameter_list: (TF_Graph* graph, const GraphDef& def,
                                      const TF_ImportGraphDefOptions* opts,
                                      TF_ImportGraphDefResults* tf_results,
                                      TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: const GraphDef& def
       type_qualifier: const
        const: const
       type_identifier: GraphDef
       reference_declarator: & def
        &: &
        identifier: def
      ,: ,
      parameter_declaration: const TF_ImportGraphDefOptions* opts
       type_qualifier: const
        const: const
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * opts
        *: *
        identifier: opts
      ,: ,
      parameter_declaration: TF_ImportGraphDefResults* tf_results
       type_identifier: TF_ImportGraphDefResults
       pointer_declarator: * tf_results
        *: *
        identifier: tf_results
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    ;: 
   expression_statement: TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu)
    call_expression: TF_EXCLUSIVE_LOCKS_REQUIRED(graph->mu)
     identifier: TF_EXCLUSIVE_LOCKS_REQUIRED
     argument_list: (graph->mu)
      (: (
      field_expression: graph->mu
       identifier: graph
       ->: ->
       field_identifier: mu
      ): )
    ;: 
   compound_statement: {
  const int last_node_id = graph->graph.num_node_ids();
  tensorflow::ImportGraphDefResults results;
  status->status = tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results);
  if (!status->status.ok()) return;

  // Add new nodes to name_map
  for (int i = last_node_id; i < graph->graph.num_node_ids(); ++i) {
    auto* node = graph->graph.FindNodeId(i);
    if (node != nullptr) graph->name_map[node->name()] = node;
  }

  // Populate return_tensors
  DCHECK(tf_results->return_tensors.empty());
  tf_results->return_tensors.resize(results.return_tensors.size());
  for (int i = 0; i < results.return_tensors.size(); ++i) {
    tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
    tf_results->return_tensors[i].index = results.return_tensors[i].second;
  }

  // Populate return_nodes
  DCHECK(tf_results->return_nodes.empty());
  tf_results->return_nodes.resize(results.return_nodes.size());
  for (int i = 0; i < results.return_nodes.size(); ++i) {
    tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
  }

  // Populate missing unused map keys
  DCHECK(tf_results->missing_unused_key_names.empty());
  DCHECK(tf_results->missing_unused_key_indexes.empty());
  DCHECK(tf_results->missing_unused_key_names_data.empty());

  size_t size = results.missing_unused_input_map_keys.size();
  tf_results->missing_unused_key_names.resize(size);
  tf_results->missing_unused_key_indexes.resize(size);

  for (int i = 0; i < size; ++i) {
    TensorId id = results.missing_unused_input_map_keys[i];
    tf_results->missing_unused_key_names_data.emplace_back(id.first);
    tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
    tf_results->missing_unused_key_indexes[i] = id.second;
  }
}
    {: {
    declaration: const int last_node_id = graph->graph.num_node_ids();
     type_qualifier: const
      const: const
     primitive_type: int
     init_declarator: last_node_id = graph->graph.num_node_ids()
      identifier: last_node_id
      =: =
      call_expression: graph->graph.num_node_ids()
       field_expression: graph->graph.num_node_ids
        field_expression: graph->graph
         identifier: graph
         ->: ->
         field_identifier: graph
        .: .
        field_identifier: num_node_ids
       argument_list: ()
        (: (
        ): )
     ;: ;
    declaration: tensorflow::ImportGraphDefResults results;
     qualified_identifier: tensorflow::ImportGraphDefResults
      namespace_identifier: tensorflow
      ::: ::
      type_identifier: ImportGraphDefResults
     identifier: results
     ;: ;
    expression_statement: status->status = tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results);
     assignment_expression: status->status = tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results)
      field_expression: status->status
       identifier: status
       ->: ->
       field_identifier: status
      =: =
      call_expression: tensorflow::ImportGraphDef(opts->opts, def, &graph->graph,
                                              &graph->refiner, &results)
       qualified_identifier: tensorflow::ImportGraphDef
        namespace_identifier: tensorflow
        ::: ::
        identifier: ImportGraphDef
       argument_list: (opts->opts, def, &graph->graph,
                                              &graph->refiner, &results)
        (: (
        field_expression: opts->opts
         identifier: opts
         ->: ->
         field_identifier: opts
        ,: ,
        identifier: def
        ,: ,
        pointer_expression: &graph->graph
         &: &
         field_expression: graph->graph
          identifier: graph
          ->: ->
          field_identifier: graph
        ,: ,
        pointer_expression: &graph->refiner
         &: &
         field_expression: graph->refiner
          identifier: graph
          ->: ->
          field_identifier: refiner
        ,: ,
        pointer_expression: &results
         &: &
         identifier: results
        ): )
     ;: ;
    if_statement: if (!status->status.ok()) return;
     if: if
     condition_clause: (!status->status.ok())
      (: (
      unary_expression: !status->status.ok()
       !: !
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
      ): )
     return_statement: return;
      return: return
      ;: ;
    comment: // Add new nodes to name_map
    for_statement: for (int i = last_node_id; i < graph->graph.num_node_ids(); ++i) {
    auto* node = graph->graph.FindNodeId(i);
    if (node != nullptr) graph->name_map[node->name()] = node;
  }
     for: for
     (: (
     declaration: int i = last_node_id;
      primitive_type: int
      init_declarator: i = last_node_id
       identifier: i
       =: =
       identifier: last_node_id
      ;: ;
     binary_expression: i < graph->graph.num_node_ids()
      identifier: i
      <: <
      call_expression: graph->graph.num_node_ids()
       field_expression: graph->graph.num_node_ids
        field_expression: graph->graph
         identifier: graph
         ->: ->
         field_identifier: graph
        .: .
        field_identifier: num_node_ids
       argument_list: ()
        (: (
        ): )
     ;: ;
     update_expression: ++i
      ++: ++
      identifier: i
     ): )
     compound_statement: {
    auto* node = graph->graph.FindNodeId(i);
    if (node != nullptr) graph->name_map[node->name()] = node;
  }
      {: {
      declaration: auto* node = graph->graph.FindNodeId(i);
       placeholder_type_specifier: auto
        auto: auto
       init_declarator: * node = graph->graph.FindNodeId(i)
        pointer_declarator: * node
         *: *
         identifier: node
        =: =
        call_expression: graph->graph.FindNodeId(i)
         field_expression: graph->graph.FindNodeId
          field_expression: graph->graph
           identifier: graph
           ->: ->
           field_identifier: graph
          .: .
          field_identifier: FindNodeId
         argument_list: (i)
          (: (
          identifier: i
          ): )
       ;: ;
      if_statement: if (node != nullptr) graph->name_map[node->name()] = node;
       if: if
       condition_clause: (node != nullptr)
        (: (
        binary_expression: node != nullptr
         identifier: node
         !=: !=
         null: nullptr
          nullptr: nullptr
        ): )
       expression_statement: graph->name_map[node->name()] = node;
        assignment_expression: graph->name_map[node->name()] = node
         subscript_expression: graph->name_map[node->name()]
          field_expression: graph->name_map
           identifier: graph
           ->: ->
           field_identifier: name_map
          subscript_argument_list: [node->name()]
           [: [
           call_expression: node->name()
            field_expression: node->name
             identifier: node
             ->: ->
             field_identifier: name
            argument_list: ()
             (: (
             ): )
           ]: ]
         =: =
         identifier: node
        ;: ;
      }: }
    comment: // Populate return_tensors
    expression_statement: DCHECK(tf_results->return_tensors.empty());
     call_expression: DCHECK(tf_results->return_tensors.empty())
      identifier: DCHECK
      argument_list: (tf_results->return_tensors.empty())
       (: (
       call_expression: tf_results->return_tensors.empty()
        field_expression: tf_results->return_tensors.empty
         field_expression: tf_results->return_tensors
          identifier: tf_results
          ->: ->
          field_identifier: return_tensors
         .: .
         field_identifier: empty
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    expression_statement: tf_results->return_tensors.resize(results.return_tensors.size());
     call_expression: tf_results->return_tensors.resize(results.return_tensors.size())
      field_expression: tf_results->return_tensors.resize
       field_expression: tf_results->return_tensors
        identifier: tf_results
        ->: ->
        field_identifier: return_tensors
       .: .
       field_identifier: resize
      argument_list: (results.return_tensors.size())
       (: (
       call_expression: results.return_tensors.size()
        field_expression: results.return_tensors.size
         field_expression: results.return_tensors
          identifier: results
          .: .
          field_identifier: return_tensors
         .: .
         field_identifier: size
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    for_statement: for (int i = 0; i < results.return_tensors.size(); ++i) {
    tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
    tf_results->return_tensors[i].index = results.return_tensors[i].second;
  }
     for: for
     (: (
     declaration: int i = 0;
      primitive_type: int
      init_declarator: i = 0
       identifier: i
       =: =
       number_literal: 0
      ;: ;
     binary_expression: i < results.return_tensors.size()
      identifier: i
      <: <
      call_expression: results.return_tensors.size()
       field_expression: results.return_tensors.size
        field_expression: results.return_tensors
         identifier: results
         .: .
         field_identifier: return_tensors
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
     ;: ;
     update_expression: ++i
      ++: ++
      identifier: i
     ): )
     compound_statement: {
    tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
    tf_results->return_tensors[i].index = results.return_tensors[i].second;
  }
      {: {
      expression_statement: tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first);
       assignment_expression: tf_results->return_tensors[i].oper =
        ToOperation(results.return_tensors[i].first)
        field_expression: tf_results->return_tensors[i].oper
         subscript_expression: tf_results->return_tensors[i]
          field_expression: tf_results->return_tensors
           identifier: tf_results
           ->: ->
           field_identifier: return_tensors
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         .: .
         field_identifier: oper
        =: =
        call_expression: ToOperation(results.return_tensors[i].first)
         identifier: ToOperation
         argument_list: (results.return_tensors[i].first)
          (: (
          field_expression: results.return_tensors[i].first
           subscript_expression: results.return_tensors[i]
            field_expression: results.return_tensors
             identifier: results
             .: .
             field_identifier: return_tensors
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           .: .
           field_identifier: first
          ): )
       ;: ;
      expression_statement: tf_results->return_tensors[i].index = results.return_tensors[i].second;
       assignment_expression: tf_results->return_tensors[i].index = results.return_tensors[i].second
        field_expression: tf_results->return_tensors[i].index
         subscript_expression: tf_results->return_tensors[i]
          field_expression: tf_results->return_tensors
           identifier: tf_results
           ->: ->
           field_identifier: return_tensors
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         .: .
         field_identifier: index
        =: =
        field_expression: results.return_tensors[i].second
         subscript_expression: results.return_tensors[i]
          field_expression: results.return_tensors
           identifier: results
           .: .
           field_identifier: return_tensors
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         .: .
         field_identifier: second
       ;: ;
      }: }
    comment: // Populate return_nodes
    expression_statement: DCHECK(tf_results->return_nodes.empty());
     call_expression: DCHECK(tf_results->return_nodes.empty())
      identifier: DCHECK
      argument_list: (tf_results->return_nodes.empty())
       (: (
       call_expression: tf_results->return_nodes.empty()
        field_expression: tf_results->return_nodes.empty
         field_expression: tf_results->return_nodes
          identifier: tf_results
          ->: ->
          field_identifier: return_nodes
         .: .
         field_identifier: empty
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    expression_statement: tf_results->return_nodes.resize(results.return_nodes.size());
     call_expression: tf_results->return_nodes.resize(results.return_nodes.size())
      field_expression: tf_results->return_nodes.resize
       field_expression: tf_results->return_nodes
        identifier: tf_results
        ->: ->
        field_identifier: return_nodes
       .: .
       field_identifier: resize
      argument_list: (results.return_nodes.size())
       (: (
       call_expression: results.return_nodes.size()
        field_expression: results.return_nodes.size
         field_expression: results.return_nodes
          identifier: results
          .: .
          field_identifier: return_nodes
         .: .
         field_identifier: size
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    for_statement: for (int i = 0; i < results.return_nodes.size(); ++i) {
    tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
  }
     for: for
     (: (
     declaration: int i = 0;
      primitive_type: int
      init_declarator: i = 0
       identifier: i
       =: =
       number_literal: 0
      ;: ;
     binary_expression: i < results.return_nodes.size()
      identifier: i
      <: <
      call_expression: results.return_nodes.size()
       field_expression: results.return_nodes.size
        field_expression: results.return_nodes
         identifier: results
         .: .
         field_identifier: return_nodes
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
     ;: ;
     update_expression: ++i
      ++: ++
      identifier: i
     ): )
     compound_statement: {
    tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
  }
      {: {
      expression_statement: tf_results->return_nodes[i] = ToOperation(results.return_nodes[i]);
       assignment_expression: tf_results->return_nodes[i] = ToOperation(results.return_nodes[i])
        subscript_expression: tf_results->return_nodes[i]
         field_expression: tf_results->return_nodes
          identifier: tf_results
          ->: ->
          field_identifier: return_nodes
         subscript_argument_list: [i]
          [: [
          identifier: i
          ]: ]
        =: =
        call_expression: ToOperation(results.return_nodes[i])
         identifier: ToOperation
         argument_list: (results.return_nodes[i])
          (: (
          subscript_expression: results.return_nodes[i]
           field_expression: results.return_nodes
            identifier: results
            .: .
            field_identifier: return_nodes
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          ): )
       ;: ;
      }: }
    comment: // Populate missing unused map keys
    expression_statement: DCHECK(tf_results->missing_unused_key_names.empty());
     call_expression: DCHECK(tf_results->missing_unused_key_names.empty())
      identifier: DCHECK
      argument_list: (tf_results->missing_unused_key_names.empty())
       (: (
       call_expression: tf_results->missing_unused_key_names.empty()
        field_expression: tf_results->missing_unused_key_names.empty
         field_expression: tf_results->missing_unused_key_names
          identifier: tf_results
          ->: ->
          field_identifier: missing_unused_key_names
         .: .
         field_identifier: empty
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    expression_statement: DCHECK(tf_results->missing_unused_key_indexes.empty());
     call_expression: DCHECK(tf_results->missing_unused_key_indexes.empty())
      identifier: DCHECK
      argument_list: (tf_results->missing_unused_key_indexes.empty())
       (: (
       call_expression: tf_results->missing_unused_key_indexes.empty()
        field_expression: tf_results->missing_unused_key_indexes.empty
         field_expression: tf_results->missing_unused_key_indexes
          identifier: tf_results
          ->: ->
          field_identifier: missing_unused_key_indexes
         .: .
         field_identifier: empty
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    expression_statement: DCHECK(tf_results->missing_unused_key_names_data.empty());
     call_expression: DCHECK(tf_results->missing_unused_key_names_data.empty())
      identifier: DCHECK
      argument_list: (tf_results->missing_unused_key_names_data.empty())
       (: (
       call_expression: tf_results->missing_unused_key_names_data.empty()
        field_expression: tf_results->missing_unused_key_names_data.empty
         field_expression: tf_results->missing_unused_key_names_data
          identifier: tf_results
          ->: ->
          field_identifier: missing_unused_key_names_data
         .: .
         field_identifier: empty
        argument_list: ()
         (: (
         ): )
       ): )
     ;: ;
    declaration: size_t size = results.missing_unused_input_map_keys.size();
     primitive_type: size_t
     init_declarator: size = results.missing_unused_input_map_keys.size()
      identifier: size
      =: =
      call_expression: results.missing_unused_input_map_keys.size()
       field_expression: results.missing_unused_input_map_keys.size
        field_expression: results.missing_unused_input_map_keys
         identifier: results
         .: .
         field_identifier: missing_unused_input_map_keys
        .: .
        field_identifier: size
       argument_list: ()
        (: (
        ): )
     ;: ;
    expression_statement: tf_results->missing_unused_key_names.resize(size);
     call_expression: tf_results->missing_unused_key_names.resize(size)
      field_expression: tf_results->missing_unused_key_names.resize
       field_expression: tf_results->missing_unused_key_names
        identifier: tf_results
        ->: ->
        field_identifier: missing_unused_key_names
       .: .
       field_identifier: resize
      argument_list: (size)
       (: (
       identifier: size
       ): )
     ;: ;
    expression_statement: tf_results->missing_unused_key_indexes.resize(size);
     call_expression: tf_results->missing_unused_key_indexes.resize(size)
      field_expression: tf_results->missing_unused_key_indexes.resize
       field_expression: tf_results->missing_unused_key_indexes
        identifier: tf_results
        ->: ->
        field_identifier: missing_unused_key_indexes
       .: .
       field_identifier: resize
      argument_list: (size)
       (: (
       identifier: size
       ): )
     ;: ;
    for_statement: for (int i = 0; i < size; ++i) {
    TensorId id = results.missing_unused_input_map_keys[i];
    tf_results->missing_unused_key_names_data.emplace_back(id.first);
    tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
    tf_results->missing_unused_key_indexes[i] = id.second;
  }
     for: for
     (: (
     declaration: int i = 0;
      primitive_type: int
      init_declarator: i = 0
       identifier: i
       =: =
       number_literal: 0
      ;: ;
     binary_expression: i < size
      identifier: i
      <: <
      identifier: size
     ;: ;
     update_expression: ++i
      ++: ++
      identifier: i
     ): )
     compound_statement: {
    TensorId id = results.missing_unused_input_map_keys[i];
    tf_results->missing_unused_key_names_data.emplace_back(id.first);
    tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
    tf_results->missing_unused_key_indexes[i] = id.second;
  }
      {: {
      declaration: TensorId id = results.missing_unused_input_map_keys[i];
       type_identifier: TensorId
       init_declarator: id = results.missing_unused_input_map_keys[i]
        identifier: id
        =: =
        subscript_expression: results.missing_unused_input_map_keys[i]
         field_expression: results.missing_unused_input_map_keys
          identifier: results
          .: .
          field_identifier: missing_unused_input_map_keys
         subscript_argument_list: [i]
          [: [
          identifier: i
          ]: ]
       ;: ;
      expression_statement: tf_results->missing_unused_key_names_data.emplace_back(id.first);
       call_expression: tf_results->missing_unused_key_names_data.emplace_back(id.first)
        field_expression: tf_results->missing_unused_key_names_data.emplace_back
         field_expression: tf_results->missing_unused_key_names_data
          identifier: tf_results
          ->: ->
          field_identifier: missing_unused_key_names_data
         .: .
         field_identifier: emplace_back
        argument_list: (id.first)
         (: (
         field_expression: id.first
          identifier: id
          .: .
          field_identifier: first
         ): )
       ;: ;
      expression_statement: tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str();
       assignment_expression: tf_results->missing_unused_key_names[i] =
        tf_results->missing_unused_key_names_data.back().c_str()
        subscript_expression: tf_results->missing_unused_key_names[i]
         field_expression: tf_results->missing_unused_key_names
          identifier: tf_results
          ->: ->
          field_identifier: missing_unused_key_names
         subscript_argument_list: [i]
          [: [
          identifier: i
          ]: ]
        =: =
        call_expression: tf_results->missing_unused_key_names_data.back().c_str()
         field_expression: tf_results->missing_unused_key_names_data.back().c_str
          call_expression: tf_results->missing_unused_key_names_data.back()
           field_expression: tf_results->missing_unused_key_names_data.back
            field_expression: tf_results->missing_unused_key_names_data
             identifier: tf_results
             ->: ->
             field_identifier: missing_unused_key_names_data
            .: .
            field_identifier: back
           argument_list: ()
            (: (
            ): )
          .: .
          field_identifier: c_str
         argument_list: ()
          (: (
          ): )
       ;: ;
      expression_statement: tf_results->missing_unused_key_indexes[i] = id.second;
       assignment_expression: tf_results->missing_unused_key_indexes[i] = id.second
        subscript_expression: tf_results->missing_unused_key_indexes[i]
         field_expression: tf_results->missing_unused_key_indexes
          identifier: tf_results
          ->: ->
          field_identifier: missing_unused_key_indexes
         subscript_argument_list: [i]
          [: [
          identifier: i
          ]: ]
        =: =
        field_expression: id.second
         identifier: id
         .: .
         field_identifier: second
       ;: ;
      }: }
    }: }
   function_definition: TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResults(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}
    type_identifier: TF_ImportGraphDefResults
    pointer_declarator: * TF_GraphImportGraphDefWithResults(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status)
     *: *
     function_declarator: TF_GraphImportGraphDefWithResults(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status)
      identifier: TF_GraphImportGraphDefWithResults
      parameter_list: (
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: const TF_Buffer* graph_def
        type_qualifier: const
         const: const
        type_identifier: TF_Buffer
        pointer_declarator: * graph_def
         *: *
         identifier: graph_def
       ,: ,
       parameter_declaration: const TF_ImportGraphDefOptions* options
        type_qualifier: const
         const: const
        type_identifier: TF_ImportGraphDefOptions
        pointer_declarator: * options
         *: *
         identifier: options
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}
     {: {
     declaration: GraphDef def;
      type_identifier: GraphDef
      identifier: def
      ;: ;
     if_statement: if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
      if: if
      condition_clause: (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length))
       (: (
       unary_expression: !tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)
        !: !
        call_expression: tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)
         qualified_identifier: tensorflow::ParseProtoUnlimited
          namespace_identifier: tensorflow
          ::: ::
          identifier: ParseProtoUnlimited
         argument_list: (&def, graph_def->data,
                                       graph_def->length)
          (: (
          pointer_expression: &def
           &: &
           identifier: def
          ,: ,
          field_expression: graph_def->data
           identifier: graph_def
           ->: ->
           field_identifier: data
          ,: ,
          field_expression: graph_def->length
           identifier: graph_def
           ->: ->
           field_identifier: length
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Invalid GraphDef");
    return nullptr;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Invalid GraphDef");
        assignment_expression: status->status = InvalidArgument("Invalid GraphDef")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Invalid GraphDef")
          identifier: InvalidArgument
          argument_list: ("Invalid GraphDef")
           (: (
           string_literal: "Invalid GraphDef"
            ": "
            string_content: Invalid GraphDef
            ": "
           ): )
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     declaration: auto results = new TF_ImportGraphDefResults();
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: results = new TF_ImportGraphDefResults()
       identifier: results
       =: =
       new_expression: new TF_ImportGraphDefResults()
        new: new
        type_identifier: TF_ImportGraphDefResults
        argument_list: ()
         (: (
         ): )
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: GraphImportGraphDefLocked(graph, def, options, results, status);
      call_expression: GraphImportGraphDefLocked(graph, def, options, results, status)
       identifier: GraphImportGraphDefLocked
       argument_list: (graph, def, options, results, status)
        (: (
        identifier: graph
        ,: ,
        identifier: def
        ,: ,
        identifier: options
        ,: ,
        identifier: results
        ,: ,
        identifier: status
        ): )
      ;: ;
     if_statement: if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    delete results;
    return nullptr;
  }
       {: {
       expression_statement: delete results;
        delete_expression: delete results
         delete: delete
         identifier: results
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     return_statement: return results;
      return: return
      identifier: results
      ;: ;
     }: }
   function_definition: TF_ImportGraphDefResults* TF_GraphImportGraphDefWithResultsNoSerialization(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status) {
  const GraphDef* graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data);
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}
    type_identifier: TF_ImportGraphDefResults
    pointer_declarator: * TF_GraphImportGraphDefWithResultsNoSerialization(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status)
     *: *
     function_declarator: TF_GraphImportGraphDefWithResultsNoSerialization(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status)
      identifier: TF_GraphImportGraphDefWithResultsNoSerialization
      parameter_list: (
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Status* status)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: const TF_Buffer* graph_def
        type_qualifier: const
         const: const
        type_identifier: TF_Buffer
        pointer_declarator: * graph_def
         *: *
         identifier: graph_def
       ,: ,
       parameter_declaration: const TF_ImportGraphDefOptions* options
        type_qualifier: const
         const: const
        type_identifier: TF_ImportGraphDefOptions
        pointer_declarator: * options
         *: *
         identifier: options
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  const GraphDef* graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data);
  auto results = new TF_ImportGraphDefResults();
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status);
  if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
  return results;
}
     {: {
     declaration: const GraphDef* graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data);
      type_qualifier: const
       const: const
      type_identifier: GraphDef
      init_declarator: * graph_def_ptr =
      reinterpret_cast<const GraphDef*>(graph_def->data)
       pointer_declarator: * graph_def_ptr
        *: *
        identifier: graph_def_ptr
       =: =
       call_expression: reinterpret_cast<const GraphDef*>(graph_def->data)
        template_function: reinterpret_cast<const GraphDef*>
         identifier: reinterpret_cast
         template_argument_list: <const GraphDef*>
          <: <
          type_descriptor: const GraphDef*
           type_qualifier: const
            const: const
           type_identifier: GraphDef
           abstract_pointer_declarator: *
            *: *
          >: >
        argument_list: (graph_def->data)
         (: (
         field_expression: graph_def->data
          identifier: graph_def
          ->: ->
          field_identifier: data
         ): )
      ;: ;
     declaration: auto results = new TF_ImportGraphDefResults();
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: results = new TF_ImportGraphDefResults()
       identifier: results
       =: =
       new_expression: new TF_ImportGraphDefResults()
        new: new
        type_identifier: TF_ImportGraphDefResults
        argument_list: ()
         (: (
         ): )
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status);
      call_expression: GraphImportGraphDefLocked(graph, *graph_def_ptr, options, results, status)
       identifier: GraphImportGraphDefLocked
       argument_list: (graph, *graph_def_ptr, options, results, status)
        (: (
        identifier: graph
        ,: ,
        pointer_expression: *graph_def_ptr
         *: *
         identifier: graph_def_ptr
        ,: ,
        identifier: options
        ,: ,
        identifier: results
        ,: ,
        identifier: status
        ): )
      ;: ;
     if_statement: if (!status->status.ok()) {
    delete results;
    return nullptr;
  }
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    delete results;
    return nullptr;
  }
       {: {
       expression_statement: delete results;
        delete_expression: delete results
         delete: delete
         identifier: results
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     return_statement: return results;
      return: return
      identifier: results
      ;: ;
     }: }
   function_definition: void TF_GraphImportGraphDefWithReturnOutputs(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status) {
  if (num_return_outputs != options->opts.return_tensors.size()) {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
  if (num_return_outputs > 0 && return_outputs == nullptr) {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  TF_ImportGraphDefResults results;
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, &results, status);
  DCHECK_EQ(results.return_tensors.size(), num_return_outputs);
  memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output));
}
    primitive_type: void
    function_declarator: TF_GraphImportGraphDefWithReturnOutputs(
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status)
     identifier: TF_GraphImportGraphDefWithReturnOutputs
     parameter_list: (
    TF_Graph* graph, const TF_Buffer* graph_def,
    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,
    int num_return_outputs, TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: const TF_Buffer* graph_def
       type_qualifier: const
        const: const
       type_identifier: TF_Buffer
       pointer_declarator: * graph_def
        *: *
        identifier: graph_def
      ,: ,
      parameter_declaration: const TF_ImportGraphDefOptions* options
       type_qualifier: const
        const: const
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * options
        *: *
        identifier: options
      ,: ,
      parameter_declaration: TF_Output* return_outputs
       type_identifier: TF_Output
       pointer_declarator: * return_outputs
        *: *
        identifier: return_outputs
      ,: ,
      parameter_declaration: int num_return_outputs
       primitive_type: int
       identifier: num_return_outputs
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  if (num_return_outputs != options->opts.return_tensors.size()) {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
  if (num_return_outputs > 0 && return_outputs == nullptr) {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
  GraphDef def;
  if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
  TF_ImportGraphDefResults results;
  mutex_lock l(graph->mu);
  GraphImportGraphDefLocked(graph, def, options, &results, status);
  DCHECK_EQ(results.return_tensors.size(), num_return_outputs);
  memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output));
}
     {: {
     if_statement: if (num_return_outputs != options->opts.return_tensors.size()) {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
      if: if
      condition_clause: (num_return_outputs != options->opts.return_tensors.size())
       (: (
       binary_expression: num_return_outputs != options->opts.return_tensors.size()
        identifier: num_return_outputs
        !=: !=
        call_expression: options->opts.return_tensors.size()
         field_expression: options->opts.return_tensors.size
          field_expression: options->opts.return_tensors
           field_expression: options->opts
            identifier: options
            ->: ->
            field_identifier: opts
           .: .
           field_identifier: return_tensors
          .: .
          field_identifier: size
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
    return;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs);
        assignment_expression: status->status = InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs)
          identifier: InvalidArgument
          argument_list: ("Expected 'num_return_outputs' to be ",
                                     options->opts.return_tensors.size(),
                                     ", got ", num_return_outputs)
           (: (
           string_literal: "Expected 'num_return_outputs' to be "
            ": "
            string_content: Expected 'num_return_outputs' to be 
            ": "
           ,: ,
           call_expression: options->opts.return_tensors.size()
            field_expression: options->opts.return_tensors.size
             field_expression: options->opts.return_tensors
              field_expression: options->opts
               identifier: options
               ->: ->
               field_identifier: opts
              .: .
              field_identifier: return_tensors
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
           ,: ,
           string_literal: ", got "
            ": "
            string_content: , got 
            ": "
           ,: ,
           identifier: num_return_outputs
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     if_statement: if (num_return_outputs > 0 && return_outputs == nullptr) {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
      if: if
      condition_clause: (num_return_outputs > 0 && return_outputs == nullptr)
       (: (
       binary_expression: num_return_outputs > 0 && return_outputs == nullptr
        binary_expression: num_return_outputs > 0
         identifier: num_return_outputs
         >: >
         number_literal: 0
        &&: &&
        binary_expression: return_outputs == nullptr
         identifier: return_outputs
         ==: ==
         null: nullptr
          nullptr: nullptr
       ): )
      compound_statement: {
    status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
    return;
  }
       {: {
       expression_statement: status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs);
        assignment_expression: status->status = InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument(
        "'return_outputs' must be preallocated to length ", num_return_outputs)
          identifier: InvalidArgument
          argument_list: (
        "'return_outputs' must be preallocated to length ", num_return_outputs)
           (: (
           string_literal: "'return_outputs' must be preallocated to length "
            ": "
            string_content: 'return_outputs' must be preallocated to length 
            ": "
           ,: ,
           identifier: num_return_outputs
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: GraphDef def;
      type_identifier: GraphDef
      identifier: def
      ;: ;
     if_statement: if (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)) {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
      if: if
      condition_clause: (!tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length))
       (: (
       unary_expression: !tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)
        !: !
        call_expression: tensorflow::ParseProtoUnlimited(&def, graph_def->data,
                                       graph_def->length)
         qualified_identifier: tensorflow::ParseProtoUnlimited
          namespace_identifier: tensorflow
          ::: ::
          identifier: ParseProtoUnlimited
         argument_list: (&def, graph_def->data,
                                       graph_def->length)
          (: (
          pointer_expression: &def
           &: &
           identifier: def
          ,: ,
          field_expression: graph_def->data
           identifier: graph_def
           ->: ->
           field_identifier: data
          ,: ,
          field_expression: graph_def->length
           identifier: graph_def
           ->: ->
           field_identifier: length
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Invalid GraphDef");
    return;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Invalid GraphDef");
        assignment_expression: status->status = InvalidArgument("Invalid GraphDef")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Invalid GraphDef")
          identifier: InvalidArgument
          argument_list: ("Invalid GraphDef")
           (: (
           string_literal: "Invalid GraphDef"
            ": "
            string_content: Invalid GraphDef
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: TF_ImportGraphDefResults results;
      type_identifier: TF_ImportGraphDefResults
      identifier: results
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: GraphImportGraphDefLocked(graph, def, options, &results, status);
      call_expression: GraphImportGraphDefLocked(graph, def, options, &results, status)
       identifier: GraphImportGraphDefLocked
       argument_list: (graph, def, options, &results, status)
        (: (
        identifier: graph
        ,: ,
        identifier: def
        ,: ,
        identifier: options
        ,: ,
        pointer_expression: &results
         &: &
         identifier: results
        ,: ,
        identifier: status
        ): )
      ;: ;
     expression_statement: DCHECK_EQ(results.return_tensors.size(), num_return_outputs);
      call_expression: DCHECK_EQ(results.return_tensors.size(), num_return_outputs)
       identifier: DCHECK_EQ
       argument_list: (results.return_tensors.size(), num_return_outputs)
        (: (
        call_expression: results.return_tensors.size()
         field_expression: results.return_tensors.size
          field_expression: results.return_tensors
           identifier: results
           .: .
           field_identifier: return_tensors
          .: .
          field_identifier: size
         argument_list: ()
          (: (
          ): )
        ,: ,
        identifier: num_return_outputs
        ): )
      ;: ;
     expression_statement: memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output));
      call_expression: memcpy(return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output))
       identifier: memcpy
       argument_list: (return_outputs, results.return_tensors.data(),
         num_return_outputs * sizeof(TF_Output))
        (: (
        identifier: return_outputs
        ,: ,
        call_expression: results.return_tensors.data()
         field_expression: results.return_tensors.data
          field_expression: results.return_tensors
           identifier: results
           .: .
           field_identifier: return_tensors
          .: .
          field_identifier: data
         argument_list: ()
          (: (
          ): )
        ,: ,
        binary_expression: num_return_outputs * sizeof(TF_Output)
         identifier: num_return_outputs
         *: *
         sizeof_expression: sizeof(TF_Output)
          sizeof: sizeof
          parenthesized_expression: (TF_Output)
           (: (
           identifier: TF_Output
           ): )
        ): )
      ;: ;
     }: }
   function_definition: void TF_GraphImportGraphDef(TF_Graph* graph, const TF_Buffer* graph_def,
                            const TF_ImportGraphDefOptions* options,
                            TF_Status* status) {
  TF_ImportGraphDefResults* results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status);
  TF_DeleteImportGraphDefResults(results);
}
    primitive_type: void
    function_declarator: TF_GraphImportGraphDef(TF_Graph* graph, const TF_Buffer* graph_def,
                            const TF_ImportGraphDefOptions* options,
                            TF_Status* status)
     identifier: TF_GraphImportGraphDef
     parameter_list: (TF_Graph* graph, const TF_Buffer* graph_def,
                            const TF_ImportGraphDefOptions* options,
                            TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: const TF_Buffer* graph_def
       type_qualifier: const
        const: const
       type_identifier: TF_Buffer
       pointer_declarator: * graph_def
        *: *
        identifier: graph_def
      ,: ,
      parameter_declaration: const TF_ImportGraphDefOptions* options
       type_qualifier: const
        const: const
       type_identifier: TF_ImportGraphDefOptions
       pointer_declarator: * options
        *: *
        identifier: options
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  TF_ImportGraphDefResults* results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status);
  TF_DeleteImportGraphDefResults(results);
}
     {: {
     declaration: TF_ImportGraphDefResults* results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status);
      type_identifier: TF_ImportGraphDefResults
      init_declarator: * results =
      TF_GraphImportGraphDefWithResults(graph, graph_def, options, status)
       pointer_declarator: * results
        *: *
        identifier: results
       =: =
       call_expression: TF_GraphImportGraphDefWithResults(graph, graph_def, options, status)
        identifier: TF_GraphImportGraphDefWithResults
        argument_list: (graph, graph_def, options, status)
         (: (
         identifier: graph
         ,: ,
         identifier: graph_def
         ,: ,
         identifier: options
         ,: ,
         identifier: status
         ): )
      ;: ;
     expression_statement: TF_DeleteImportGraphDefResults(results);
      call_expression: TF_DeleteImportGraphDefResults(results)
       identifier: TF_DeleteImportGraphDefResults
       argument_list: (results)
        (: (
        identifier: results
        ): )
      ;: ;
     }: }
   comment: // While loop functions -------------------------------------------------------
   namespace_definition: namespace {

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

// Creates a placeholder representing an input to the cond or body graph.
// TODO(skyewm): remove these from final graph
bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}

// Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
// `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
// will be prepended to copied node names. `control_deps` are nodes in
// `dst_graph` that the copied `src_graph` nodes will have control dependencies
// on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
// in `dst_graph` will be returned. `return_nodes` must be non-null.
Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}

bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}

bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}

#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void FreeWhileResources(const TF_WhileParams* params) {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}

TF_WhileParams EmptyWhileParams() {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}

}
    namespace: namespace
    declaration_list: {

#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

// Creates a placeholder representing an input to the cond or body graph.
// TODO(skyewm): remove these from final graph
bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}

// Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
// `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
// will be prepended to copied node names. `control_deps` are nodes in
// `dst_graph` that the copied `src_graph` nodes will have control dependencies
// on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
// in `dst_graph` will be returned. `return_nodes` must be non-null.
Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}

bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}

bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}

#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

void FreeWhileResources(const TF_WhileParams* params) {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}

TF_WhileParams EmptyWhileParams() {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}

}
     {: {
     preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)

// Creates a placeholder representing an input to the cond or body graph.
// TODO(skyewm): remove these from final graph
bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}

// Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
// `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
// will be prepended to copied node names. `control_deps` are nodes in
// `dst_graph` that the copied `src_graph` nodes will have control dependencies
// on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
// in `dst_graph` will be returned. `return_nodes` must be non-null.
Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}

bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}

bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}

#endif
      #if: #if
      binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
       unary_expression: !defined(IS_MOBILE_PLATFORM)
        !: !
        preproc_defined: defined(IS_MOBILE_PLATFORM)
         defined: defined
         (: (
         identifier: IS_MOBILE_PLATFORM
         ): )
       &&: &&
       unary_expression: !defined(IS_SLIM_BUILD)
        !: !
        preproc_defined: defined(IS_SLIM_BUILD)
         defined: defined
         (: (
         identifier: IS_SLIM_BUILD
         ): )
      
: 


      comment: // Creates a placeholder representing an input to the cond or body graph.
      comment: // TODO(skyewm): remove these from final graph
      function_definition: bool CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status) {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}
       primitive_type: bool
       function_declarator: CreateInput(const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status)
        identifier: CreateInput
        parameter_list: (const TF_Output& parent_input, TF_Graph* g, const char* name,
                 TF_Output* input, TF_Status* status)
         (: (
         parameter_declaration: const TF_Output& parent_input
          type_qualifier: const
           const: const
          type_identifier: TF_Output
          reference_declarator: & parent_input
           &: &
           identifier: parent_input
         ,: ,
         parameter_declaration: TF_Graph* g
          type_identifier: TF_Graph
          pointer_declarator: * g
           *: *
           identifier: g
         ,: ,
         parameter_declaration: const char* name
          type_qualifier: const
           const: const
          primitive_type: char
          pointer_declarator: * name
           *: *
           identifier: name
         ,: ,
         parameter_declaration: TF_Output* input
          type_identifier: TF_Output
          pointer_declarator: * input
           *: *
           identifier: input
         ,: ,
         parameter_declaration: TF_Status* status
          type_identifier: TF_Status
          pointer_declarator: * status
           *: *
           identifier: status
         ): )
       compound_statement: {
  TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
  TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
  // TODO(skyewm): set placeholder shape
  TF_Operation* oper = TF_FinishOperation(desc, status);
  if (!status->status.ok()) return false;
  *input = {oper, 0};
  return true;
}
        {: {
        declaration: TF_OperationDescription* desc = TF_NewOperation(g, "Placeholder", name);
         type_identifier: TF_OperationDescription
         init_declarator: * desc = TF_NewOperation(g, "Placeholder", name)
          pointer_declarator: * desc
           *: *
           identifier: desc
          =: =
          call_expression: TF_NewOperation(g, "Placeholder", name)
           identifier: TF_NewOperation
           argument_list: (g, "Placeholder", name)
            (: (
            identifier: g
            ,: ,
            string_literal: "Placeholder"
             ": "
             string_content: Placeholder
             ": "
            ,: ,
            identifier: name
            ): )
         ;: ;
        expression_statement: TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input));
         call_expression: TF_SetAttrType(desc, "dtype", TF_OperationOutputType(parent_input))
          identifier: TF_SetAttrType
          argument_list: (desc, "dtype", TF_OperationOutputType(parent_input))
           (: (
           identifier: desc
           ,: ,
           string_literal: "dtype"
            ": "
            string_content: dtype
            ": "
           ,: ,
           call_expression: TF_OperationOutputType(parent_input)
            identifier: TF_OperationOutputType
            argument_list: (parent_input)
             (: (
             identifier: parent_input
             ): )
           ): )
         ;: ;
        comment: // TODO(skyewm): set placeholder shape
        declaration: TF_Operation* oper = TF_FinishOperation(desc, status);
         type_identifier: TF_Operation
         init_declarator: * oper = TF_FinishOperation(desc, status)
          pointer_declarator: * oper
           *: *
           identifier: oper
          =: =
          call_expression: TF_FinishOperation(desc, status)
           identifier: TF_FinishOperation
           argument_list: (desc, status)
            (: (
            identifier: desc
            ,: ,
            identifier: status
            ): )
         ;: ;
        if_statement: if (!status->status.ok()) return false;
         if: if
         condition_clause: (!status->status.ok())
          (: (
          unary_expression: !status->status.ok()
           !: !
           call_expression: status->status.ok()
            field_expression: status->status.ok
             field_expression: status->status
              identifier: status
              ->: ->
              field_identifier: status
             .: .
             field_identifier: ok
            argument_list: ()
             (: (
             ): )
          ): )
         return_statement: return false;
          return: return
          false: false
          ;: ;
        expression_statement: *input = {oper, 0};
         assignment_expression: *input = {oper, 0}
          pointer_expression: *input
           *: *
           identifier: input
          =: =
          initializer_list: {oper, 0}
           {: {
           identifier: oper
           ,: ,
           number_literal: 0
           }: }
         ;: ;
        return_statement: return true;
         return: return
         true: true
         ;: ;
        }: }
      comment: // Copies `src_graph` into `dst_graph`. Any node in `src_graph` with input
      comment: // `src_inputs[i]` will have that input replaced with `dst_inputs[i]`.  `prefix`
      comment: // will be prepended to copied node names. `control_deps` are nodes in
      comment: // `dst_graph` that the copied `src_graph` nodes will have control dependencies
      comment: // on. `return_nodes` are nodes in `src_graph`, and the new corresponding nodes
      comment: // in `dst_graph` will be returned. `return_nodes` must be non-null.
      function_definition: Status CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes) {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}
       type_identifier: Status
       function_declarator: CopyGraph(Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes)
        identifier: CopyGraph
        parameter_list: (Graph* src_graph, Graph* dst_graph,
                 tensorflow::ShapeRefiner* dst_refiner,
                 const TF_Output* src_inputs,
                 const std::vector<tensorflow::Output>& dst_inputs,
                 const string& prefix,
                 const std::vector<tensorflow::Operation>& control_deps,
                 const TF_Output* nodes_to_return, int nreturn_nodes,
                 std::vector<tensorflow::Output>* return_nodes)
         (: (
         parameter_declaration: Graph* src_graph
          type_identifier: Graph
          pointer_declarator: * src_graph
           *: *
           identifier: src_graph
         ,: ,
         parameter_declaration: Graph* dst_graph
          type_identifier: Graph
          pointer_declarator: * dst_graph
           *: *
           identifier: dst_graph
         ,: ,
         parameter_declaration: tensorflow::ShapeRefiner* dst_refiner
          qualified_identifier: tensorflow::ShapeRefiner
           namespace_identifier: tensorflow
           ::: ::
           type_identifier: ShapeRefiner
          pointer_declarator: * dst_refiner
           *: *
           identifier: dst_refiner
         ,: ,
         parameter_declaration: const TF_Output* src_inputs
          type_qualifier: const
           const: const
          type_identifier: TF_Output
          pointer_declarator: * src_inputs
           *: *
           identifier: src_inputs
         ,: ,
         parameter_declaration: const std::vector<tensorflow::Output>& dst_inputs
          type_qualifier: const
           const: const
          qualified_identifier: std::vector<tensorflow::Output>
           namespace_identifier: std
           ::: ::
           template_type: vector<tensorflow::Output>
            type_identifier: vector
            template_argument_list: <tensorflow::Output>
             <: <
             type_descriptor: tensorflow::Output
              qualified_identifier: tensorflow::Output
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: Output
             >: >
          reference_declarator: & dst_inputs
           &: &
           identifier: dst_inputs
         ,: ,
         parameter_declaration: const string& prefix
          type_qualifier: const
           const: const
          type_identifier: string
          reference_declarator: & prefix
           &: &
           identifier: prefix
         ,: ,
         parameter_declaration: const std::vector<tensorflow::Operation>& control_deps
          type_qualifier: const
           const: const
          qualified_identifier: std::vector<tensorflow::Operation>
           namespace_identifier: std
           ::: ::
           template_type: vector<tensorflow::Operation>
            type_identifier: vector
            template_argument_list: <tensorflow::Operation>
             <: <
             type_descriptor: tensorflow::Operation
              qualified_identifier: tensorflow::Operation
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: Operation
             >: >
          reference_declarator: & control_deps
           &: &
           identifier: control_deps
         ,: ,
         parameter_declaration: const TF_Output* nodes_to_return
          type_qualifier: const
           const: const
          type_identifier: TF_Output
          pointer_declarator: * nodes_to_return
           *: *
           identifier: nodes_to_return
         ,: ,
         parameter_declaration: int nreturn_nodes
          primitive_type: int
          identifier: nreturn_nodes
         ,: ,
         parameter_declaration: std::vector<tensorflow::Output>* return_nodes
          qualified_identifier: std::vector<tensorflow::Output>
           namespace_identifier: std
           ::: ::
           template_type: vector<tensorflow::Output>
            type_identifier: vector
            template_argument_list: <tensorflow::Output>
             <: <
             type_descriptor: tensorflow::Output
              qualified_identifier: tensorflow::Output
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: Output
             >: >
          pointer_declarator: * return_nodes
           *: *
           identifier: return_nodes
         ): )
       compound_statement: {
  DCHECK(return_nodes != nullptr);
  GraphDef gdef;
  src_graph->ToGraphDef(&gdef);

  tensorflow::ImportGraphDefOptions opts;
  opts.prefix = prefix;

  for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
  opts.skip_mapped_nodes = true;

  for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }

  for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }

  // TODO(skyewm): change to OutputTensor
  tensorflow::ImportGraphDefResults results;
  TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));

  for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
  return absl::OkStatus();
}
        {: {
        expression_statement: DCHECK(return_nodes != nullptr);
         call_expression: DCHECK(return_nodes != nullptr)
          identifier: DCHECK
          argument_list: (return_nodes != nullptr)
           (: (
           binary_expression: return_nodes != nullptr
            identifier: return_nodes
            !=: !=
            null: nullptr
             nullptr: nullptr
           ): )
         ;: ;
        declaration: GraphDef gdef;
         type_identifier: GraphDef
         identifier: gdef
         ;: ;
        expression_statement: src_graph->ToGraphDef(&gdef);
         call_expression: src_graph->ToGraphDef(&gdef)
          field_expression: src_graph->ToGraphDef
           identifier: src_graph
           ->: ->
           field_identifier: ToGraphDef
          argument_list: (&gdef)
           (: (
           pointer_expression: &gdef
            &: &
            identifier: gdef
           ): )
         ;: ;
        declaration: tensorflow::ImportGraphDefOptions opts;
         qualified_identifier: tensorflow::ImportGraphDefOptions
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: ImportGraphDefOptions
         identifier: opts
         ;: ;
        expression_statement: opts.prefix = prefix;
         assignment_expression: opts.prefix = prefix
          field_expression: opts.prefix
           identifier: opts
           .: .
           field_identifier: prefix
          =: =
          identifier: prefix
         ;: ;
        for_statement: for (int i = 0; i < dst_inputs.size(); ++i) {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
         for: for
         (: (
         declaration: int i = 0;
          primitive_type: int
          init_declarator: i = 0
           identifier: i
           =: =
           number_literal: 0
          ;: ;
         binary_expression: i < dst_inputs.size()
          identifier: i
          <: <
          call_expression: dst_inputs.size()
           field_expression: dst_inputs.size
            identifier: dst_inputs
            .: .
            field_identifier: size
           argument_list: ()
            (: (
            ): )
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
    opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
  }
          {: {
          expression_statement: opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index());
           assignment_expression: opts.input_map[ToTensorId(src_inputs[i])] =
        TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index())
            subscript_expression: opts.input_map[ToTensorId(src_inputs[i])]
             field_expression: opts.input_map
              identifier: opts
              .: .
              field_identifier: input_map
             subscript_argument_list: [ToTensorId(src_inputs[i])]
              [: [
              call_expression: ToTensorId(src_inputs[i])
               identifier: ToTensorId
               argument_list: (src_inputs[i])
                (: (
                subscript_expression: src_inputs[i]
                 identifier: src_inputs
                 subscript_argument_list: [i]
                  [: [
                  identifier: i
                  ]: ]
                ): )
              ]: ]
            =: =
            call_expression: TensorId(dst_inputs[i].node()->name(), dst_inputs[i].index())
             identifier: TensorId
             argument_list: (dst_inputs[i].node()->name(), dst_inputs[i].index())
              (: (
              call_expression: dst_inputs[i].node()->name()
               field_expression: dst_inputs[i].node()->name
                call_expression: dst_inputs[i].node()
                 field_expression: dst_inputs[i].node
                  subscript_expression: dst_inputs[i]
                   identifier: dst_inputs
                   subscript_argument_list: [i]
                    [: [
                    identifier: i
                    ]: ]
                  .: .
                  field_identifier: node
                 argument_list: ()
                  (: (
                  ): )
                ->: ->
                field_identifier: name
               argument_list: ()
                (: (
                ): )
              ,: ,
              call_expression: dst_inputs[i].index()
               field_expression: dst_inputs[i].index
                subscript_expression: dst_inputs[i]
                 identifier: dst_inputs
                 subscript_argument_list: [i]
                  [: [
                  identifier: i
                  ]: ]
                .: .
                field_identifier: index
               argument_list: ()
                (: (
                ): )
              ): )
           ;: ;
          }: }
        expression_statement: opts.skip_mapped_nodes = true;
         assignment_expression: opts.skip_mapped_nodes = true
          field_expression: opts.skip_mapped_nodes
           identifier: opts
           .: .
           field_identifier: skip_mapped_nodes
          =: =
          true: true
         ;: ;
        for_range_loop: for (const tensorflow::Operation& op : control_deps) {
    opts.control_dependencies.push_back(op.node()->name());
  }
         for: for
         (: (
         type_qualifier: const
          const: const
         qualified_identifier: tensorflow::Operation
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: Operation
         reference_declarator: & op
          &: &
          identifier: op
         :: :
         identifier: control_deps
         ): )
         compound_statement: {
    opts.control_dependencies.push_back(op.node()->name());
  }
          {: {
          expression_statement: opts.control_dependencies.push_back(op.node()->name());
           call_expression: opts.control_dependencies.push_back(op.node()->name())
            field_expression: opts.control_dependencies.push_back
             field_expression: opts.control_dependencies
              identifier: opts
              .: .
              field_identifier: control_dependencies
             .: .
             field_identifier: push_back
            argument_list: (op.node()->name())
             (: (
             call_expression: op.node()->name()
              field_expression: op.node()->name
               call_expression: op.node()
                field_expression: op.node
                 identifier: op
                 .: .
                 field_identifier: node
                argument_list: ()
                 (: (
                 ): )
               ->: ->
               field_identifier: name
              argument_list: ()
               (: (
               ): )
             ): )
           ;: ;
          }: }
        for_statement: for (int i = 0; i < nreturn_nodes; ++i) {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }
         for: for
         (: (
         declaration: int i = 0;
          primitive_type: int
          init_declarator: i = 0
           identifier: i
           =: =
           number_literal: 0
          ;: ;
         binary_expression: i < nreturn_nodes
          identifier: i
          <: <
          identifier: nreturn_nodes
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
    opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
  }
          {: {
          expression_statement: opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]));
           call_expression: opts.return_tensors.push_back(ToTensorId(nodes_to_return[i]))
            field_expression: opts.return_tensors.push_back
             field_expression: opts.return_tensors
              identifier: opts
              .: .
              field_identifier: return_tensors
             .: .
             field_identifier: push_back
            argument_list: (ToTensorId(nodes_to_return[i]))
             (: (
             call_expression: ToTensorId(nodes_to_return[i])
              identifier: ToTensorId
              argument_list: (nodes_to_return[i])
               (: (
               subscript_expression: nodes_to_return[i]
                identifier: nodes_to_return
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
               ): )
             ): )
           ;: ;
          }: }
        comment: // TODO(skyewm): change to OutputTensor
        declaration: tensorflow::ImportGraphDefResults results;
         qualified_identifier: tensorflow::ImportGraphDefResults
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: ImportGraphDefResults
         identifier: results
         ;: ;
        expression_statement: TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results));
         call_expression: TF_RETURN_IF_ERROR(
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results))
          identifier: TF_RETURN_IF_ERROR
          argument_list: (
      ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results))
           (: (
           call_expression: ImportGraphDef(opts, gdef, dst_graph, dst_refiner, &results)
            identifier: ImportGraphDef
            argument_list: (opts, gdef, dst_graph, dst_refiner, &results)
             (: (
             identifier: opts
             ,: ,
             identifier: gdef
             ,: ,
             identifier: dst_graph
             ,: ,
             identifier: dst_refiner
             ,: ,
             pointer_expression: &results
              &: &
              identifier: results
             ): )
           ): )
         ;: ;
        for_range_loop: for (const auto& pair : results.return_tensors) {
    return_nodes->emplace_back(pair.first, pair.second);
  }
         for: for
         (: (
         type_qualifier: const
          const: const
         placeholder_type_specifier: auto
          auto: auto
         reference_declarator: & pair
          &: &
          identifier: pair
         :: :
         field_expression: results.return_tensors
          identifier: results
          .: .
          field_identifier: return_tensors
         ): )
         compound_statement: {
    return_nodes->emplace_back(pair.first, pair.second);
  }
          {: {
          expression_statement: return_nodes->emplace_back(pair.first, pair.second);
           call_expression: return_nodes->emplace_back(pair.first, pair.second)
            field_expression: return_nodes->emplace_back
             identifier: return_nodes
             ->: ->
             field_identifier: emplace_back
            argument_list: (pair.first, pair.second)
             (: (
             field_expression: pair.first
              identifier: pair
              .: .
              field_identifier: first
             ,: ,
             field_expression: pair.second
              identifier: pair
              .: .
              field_identifier: second
             ): )
           ;: ;
          }: }
        return_statement: return absl::OkStatus();
         return: return
         call_expression: absl::OkStatus()
          qualified_identifier: absl::OkStatus
           namespace_identifier: absl
           ::: ::
           identifier: OkStatus
          argument_list: ()
           (: (
           ): )
         ;: ;
        }: }
      function_definition: bool ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}
       primitive_type: bool
       function_declarator: ValidateConstWhileParams(const TF_WhileParams& params, TF_Status* s)
        identifier: ValidateConstWhileParams
        parameter_list: (const TF_WhileParams& params, TF_Status* s)
         (: (
         parameter_declaration: const TF_WhileParams& params
          type_qualifier: const
           const: const
          type_identifier: TF_WhileParams
          reference_declarator: & params
           &: &
           identifier: params
         ,: ,
         parameter_declaration: TF_Status* s
          type_identifier: TF_Status
          pointer_declarator: * s
           *: *
           identifier: s
         ): )
       compound_statement: {
  if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
  return true;
}
        {: {
        if_statement: if (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr) {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
         if: if
         condition_clause: (params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr)
          (: (
          binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr || params.body_outputs == nullptr
           binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr ||
      params.body_inputs == nullptr
            binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0 || params.cond_inputs == nullptr
             binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs ||
      params.ninputs <= 0
              binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent ||
      params.cond_graph->parent_inputs != params.body_graph->parent_inputs
               binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr ||
      params.cond_graph->parent != params.body_graph->parent
                binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr ||
      params.cond_graph->parent == nullptr
                 binary_expression: params.cond_graph == nullptr || params.body_graph == nullptr
                  binary_expression: params.cond_graph == nullptr
                   field_expression: params.cond_graph
                    identifier: params
                    .: .
                    field_identifier: cond_graph
                   ==: ==
                   null: nullptr
                    nullptr: nullptr
                  ||: ||
                  binary_expression: params.body_graph == nullptr
                   field_expression: params.body_graph
                    identifier: params
                    .: .
                    field_identifier: body_graph
                   ==: ==
                   null: nullptr
                    nullptr: nullptr
                 ||: ||
                 binary_expression: params.cond_graph->parent == nullptr
                  field_expression: params.cond_graph->parent
                   field_expression: params.cond_graph
                    identifier: params
                    .: .
                    field_identifier: cond_graph
                   ->: ->
                   field_identifier: parent
                  ==: ==
                  null: nullptr
                   nullptr: nullptr
                ||: ||
                binary_expression: params.cond_graph->parent != params.body_graph->parent
                 field_expression: params.cond_graph->parent
                  field_expression: params.cond_graph
                   identifier: params
                   .: .
                   field_identifier: cond_graph
                  ->: ->
                  field_identifier: parent
                 !=: !=
                 field_expression: params.body_graph->parent
                  field_expression: params.body_graph
                   identifier: params
                   .: .
                   field_identifier: body_graph
                  ->: ->
                  field_identifier: parent
               ||: ||
               binary_expression: params.cond_graph->parent_inputs != params.body_graph->parent_inputs
                field_expression: params.cond_graph->parent_inputs
                 field_expression: params.cond_graph
                  identifier: params
                  .: .
                  field_identifier: cond_graph
                 ->: ->
                 field_identifier: parent_inputs
                !=: !=
                field_expression: params.body_graph->parent_inputs
                 field_expression: params.body_graph
                  identifier: params
                  .: .
                  field_identifier: body_graph
                 ->: ->
                 field_identifier: parent_inputs
              ||: ||
              binary_expression: params.ninputs <= 0
               field_expression: params.ninputs
                identifier: params
                .: .
                field_identifier: ninputs
               <=: <=
               number_literal: 0
             ||: ||
             binary_expression: params.cond_inputs == nullptr
              field_expression: params.cond_inputs
               identifier: params
               .: .
               field_identifier: cond_inputs
              ==: ==
              null: nullptr
               nullptr: nullptr
            ||: ||
            binary_expression: params.body_inputs == nullptr
             field_expression: params.body_inputs
              identifier: params
              .: .
              field_identifier: body_inputs
             ==: ==
             null: nullptr
              nullptr: nullptr
           ||: ||
           binary_expression: params.body_outputs == nullptr
            field_expression: params.body_outputs
             identifier: params
             .: .
             field_identifier: body_outputs
            ==: ==
            null: nullptr
             nullptr: nullptr
          ): )
         compound_statement: {
    s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
    return false;
  }
          {: {
          expression_statement: s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call");
           assignment_expression: s->status = InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call")
            field_expression: s->status
             identifier: s
             ->: ->
             field_identifier: status
            =: =
            call_expression: InvalidArgument(
        "TF_WhileParams must be created by successful TF_NewWhile() call")
             identifier: InvalidArgument
             argument_list: (
        "TF_WhileParams must be created by successful TF_NewWhile() call")
              (: (
              string_literal: "TF_WhileParams must be created by successful TF_NewWhile() call"
               ": "
               string_content: TF_WhileParams must be created by successful TF_NewWhile() call
               ": "
              ): )
           ;: ;
          return_statement: return false;
           return: return
           false: false
           ;: ;
          }: }
        return_statement: return true;
         return: return
         true: true
         ;: ;
        }: }
      function_definition: bool ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s) {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}
       primitive_type: bool
       function_declarator: ValidateInputWhileParams(const TF_WhileParams& params, TF_Status* s)
        identifier: ValidateInputWhileParams
        parameter_list: (const TF_WhileParams& params, TF_Status* s)
         (: (
         parameter_declaration: const TF_WhileParams& params
          type_qualifier: const
           const: const
          type_identifier: TF_WhileParams
          reference_declarator: & params
           &: &
           identifier: params
         ,: ,
         parameter_declaration: TF_Status* s
          type_identifier: TF_Status
          pointer_declarator: * s
           *: *
           identifier: s
         ): )
       compound_statement: {
  if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
  for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
  if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
  return true;
}
        {: {
        if_statement: if (params.cond_output.oper == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
         if: if
         condition_clause: (params.cond_output.oper == nullptr)
          (: (
          binary_expression: params.cond_output.oper == nullptr
           field_expression: params.cond_output.oper
            field_expression: params.cond_output
             identifier: params
             .: .
             field_identifier: cond_output
            .: .
            field_identifier: oper
           ==: ==
           null: nullptr
            nullptr: nullptr
          ): )
         compound_statement: {
    s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
    return false;
  }
          {: {
          expression_statement: s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set");
           assignment_expression: s->status = InvalidArgument("TF_WhileParams `cond_output` field isn't set")
            field_expression: s->status
             identifier: s
             ->: ->
             field_identifier: status
            =: =
            call_expression: InvalidArgument("TF_WhileParams `cond_output` field isn't set")
             identifier: InvalidArgument
             argument_list: ("TF_WhileParams `cond_output` field isn't set")
              (: (
              string_literal: "TF_WhileParams `cond_output` field isn't set"
               ": "
               string_content: TF_WhileParams `cond_output` field isn't set
               ": "
              ): )
           ;: ;
          return_statement: return false;
           return: return
           false: false
           ;: ;
          }: }
        for_statement: for (int i = 0; i < params.ninputs; ++i) {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
         for: for
         (: (
         declaration: int i = 0;
          primitive_type: int
          init_declarator: i = 0
           identifier: i
           =: =
           number_literal: 0
          ;: ;
         binary_expression: i < params.ninputs
          identifier: i
          <: <
          field_expression: params.ninputs
           identifier: params
           .: .
           field_identifier: ninputs
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
    if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
  }
          {: {
          if_statement: if (params.body_outputs[i].oper == nullptr) {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
           if: if
           condition_clause: (params.body_outputs[i].oper == nullptr)
            (: (
            binary_expression: params.body_outputs[i].oper == nullptr
             field_expression: params.body_outputs[i].oper
              subscript_expression: params.body_outputs[i]
               field_expression: params.body_outputs
                identifier: params
                .: .
                field_identifier: body_outputs
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              .: .
              field_identifier: oper
             ==: ==
             null: nullptr
              nullptr: nullptr
            ): )
           compound_statement: {
      s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
      return false;
    }
            {: {
            expression_statement: s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set");
             assignment_expression: s->status = InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set")
              field_expression: s->status
               identifier: s
               ->: ->
               field_identifier: status
              =: =
              call_expression: InvalidArgument("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set")
               identifier: InvalidArgument
               argument_list: ("TF_WhileParams `body_outputs[", i, "]` ",
                                  "field isn't set")
                (: (
                string_literal: "TF_WhileParams `body_outputs["
                 ": "
                 string_content: TF_WhileParams `body_outputs[
                 ": "
                ,: ,
                identifier: i
                ,: ,
                string_literal: "]` "
                 ": "
                 string_content: ]` 
                 ": "
                ,: ,
                string_literal: "field isn't set"
                 ": "
                 string_content: field isn't set
                 ": "
                ): )
             ;: ;
            return_statement: return false;
             return: return
             false: false
             ;: ;
            }: }
          }: }
        if_statement: if (params.name == nullptr) {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
         if: if
         condition_clause: (params.name == nullptr)
          (: (
          binary_expression: params.name == nullptr
           field_expression: params.name
            identifier: params
            .: .
            field_identifier: name
           ==: ==
           null: nullptr
            nullptr: nullptr
          ): )
         compound_statement: {
    s->status = InvalidArgument("TF_WhileParams `name` field is null");
    return false;
  }
          {: {
          expression_statement: s->status = InvalidArgument("TF_WhileParams `name` field is null");
           assignment_expression: s->status = InvalidArgument("TF_WhileParams `name` field is null")
            field_expression: s->status
             identifier: s
             ->: ->
             field_identifier: status
            =: =
            call_expression: InvalidArgument("TF_WhileParams `name` field is null")
             identifier: InvalidArgument
             argument_list: ("TF_WhileParams `name` field is null")
              (: (
              string_literal: "TF_WhileParams `name` field is null"
               ": "
               string_content: TF_WhileParams `name` field is null
               ": "
              ): )
           ;: ;
          return_statement: return false;
           return: return
           false: false
           ;: ;
          }: }
        return_statement: return true;
         return: return
         true: true
         ;: ;
        }: }
      #endif: #endif
     comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
     function_definition: void FreeWhileResources(const TF_WhileParams* params) {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}
      primitive_type: void
      function_declarator: FreeWhileResources(const TF_WhileParams* params)
       identifier: FreeWhileResources
       parameter_list: (const TF_WhileParams* params)
        (: (
        parameter_declaration: const TF_WhileParams* params
         type_qualifier: const
          const: const
         type_identifier: TF_WhileParams
         pointer_declarator: * params
          *: *
          identifier: params
        ): )
      compound_statement: {
  TF_DeleteGraph(params->cond_graph);
  TF_DeleteGraph(params->body_graph);
  delete[] params->cond_inputs;
  delete[] params->body_inputs;
  delete[] params->body_outputs;
}
       {: {
       expression_statement: TF_DeleteGraph(params->cond_graph);
        call_expression: TF_DeleteGraph(params->cond_graph)
         identifier: TF_DeleteGraph
         argument_list: (params->cond_graph)
          (: (
          field_expression: params->cond_graph
           identifier: params
           ->: ->
           field_identifier: cond_graph
          ): )
        ;: ;
       expression_statement: TF_DeleteGraph(params->body_graph);
        call_expression: TF_DeleteGraph(params->body_graph)
         identifier: TF_DeleteGraph
         argument_list: (params->body_graph)
          (: (
          field_expression: params->body_graph
           identifier: params
           ->: ->
           field_identifier: body_graph
          ): )
        ;: ;
       expression_statement: delete[] params->cond_inputs;
        delete_expression: delete[] params->cond_inputs
         delete: delete
         [: [
         ]: ]
         field_expression: params->cond_inputs
          identifier: params
          ->: ->
          field_identifier: cond_inputs
        ;: ;
       expression_statement: delete[] params->body_inputs;
        delete_expression: delete[] params->body_inputs
         delete: delete
         [: [
         ]: ]
         field_expression: params->body_inputs
          identifier: params
          ->: ->
          field_identifier: body_inputs
        ;: ;
       expression_statement: delete[] params->body_outputs;
        delete_expression: delete[] params->body_outputs
         delete: delete
         [: [
         ]: ]
         field_expression: params->body_outputs
          identifier: params
          ->: ->
          field_identifier: body_outputs
        ;: ;
       }: }
     function_definition: TF_WhileParams EmptyWhileParams() {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}
      type_identifier: TF_WhileParams
      function_declarator: EmptyWhileParams()
       identifier: EmptyWhileParams
       parameter_list: ()
        (: (
        ): )
      compound_statement: {
  return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
}
       {: {
       return_statement: return {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr};
        return: return
        initializer_list: {0,       nullptr, nullptr, {nullptr, 0},
          nullptr, nullptr, nullptr, nullptr}
         {: {
         number_literal: 0
         ,: ,
         null: nullptr
          nullptr: nullptr
         ,: ,
         null: nullptr
          nullptr: nullptr
         ,: ,
         initializer_list: {nullptr, 0}
          {: {
          null: nullptr
           nullptr: nullptr
          ,: ,
          number_literal: 0
          }: }
         ,: ,
         null: nullptr
          nullptr: nullptr
         ,: ,
         null: nullptr
          nullptr: nullptr
         ,: ,
         null: nullptr
          nullptr: nullptr
         ,: ,
         null: nullptr
          nullptr: nullptr
         }: }
        ;: ;
       }: }
     }: }
   comment: // namespace
   function_definition: TF_WhileParams TF_NewWhile(TF_Graph* g, TF_Output* inputs, int ninputs,
                           TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return EmptyWhileParams();
#else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    type_identifier: TF_WhileParams
    function_declarator: TF_NewWhile(TF_Graph* g, TF_Output* inputs, int ninputs,
                           TF_Status* status)
     identifier: TF_NewWhile
     parameter_list: (TF_Graph* g, TF_Output* inputs, int ninputs,
                           TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* g
       type_identifier: TF_Graph
       pointer_declarator: * g
        *: *
        identifier: g
      ,: ,
      parameter_declaration: TF_Output* inputs
       type_identifier: TF_Output
       pointer_declarator: * inputs
        *: *
        identifier: inputs
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return EmptyWhileParams();
#else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return EmptyWhileParams();
#else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
          (: (
          concatenated_string: "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you"
           string_literal: "Creating while loops is not supported on mobile. File a bug at "
            ": "
            string_content: Creating while loops is not supported on mobile. File a bug at 
            ": "
           string_literal: "https://github.com/tensorflow/tensorflow/issues if this feature is "
            ": "
            string_content: https://github.com/tensorflow/tensorflow/issues if this feature is 
            ": "
           string_literal: "important to you"
            ": "
            string_content: important to you
            ": "
          ): )
       ;: ;
      return_statement: return EmptyWhileParams();
       return: return
       call_expression: EmptyWhileParams()
        identifier: EmptyWhileParams
        argument_list: ()
         (: (
         ): )
       ;: ;
      preproc_else: #else
  if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }

  TF_Graph* cond_graph = TF_NewGraph();
  TF_Graph* body_graph = TF_NewGraph();
  cond_graph->parent = g;
  cond_graph->parent_inputs = inputs;
  body_graph->parent = g;
  body_graph->parent_inputs = inputs;

  TF_Output* cond_inputs = new TF_Output[ninputs];
  TF_Output cond_output = {nullptr, -1};
  TF_Output* body_inputs = new TF_Output[ninputs];
  TF_Output* body_outputs = new TF_Output[ninputs];
  for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
  const char* name = nullptr;

  for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }

  TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};

  if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
  return params;
       #else: #else
       if_statement: if (ninputs == 0) {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }
        if: if
        condition_clause: (ninputs == 0)
         (: (
         binary_expression: ninputs == 0
          identifier: ninputs
          ==: ==
          number_literal: 0
         ): )
        compound_statement: {
    status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
    return EmptyWhileParams();
  }
         {: {
         expression_statement: status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input");
          assignment_expression: status->status =
        InvalidArgument("TF_NewWhile() must be passed at least one input")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument("TF_NewWhile() must be passed at least one input")
            identifier: InvalidArgument
            argument_list: ("TF_NewWhile() must be passed at least one input")
             (: (
             string_literal: "TF_NewWhile() must be passed at least one input"
              ": "
              string_content: TF_NewWhile() must be passed at least one input
              ": "
             ): )
          ;: ;
         return_statement: return EmptyWhileParams();
          return: return
          call_expression: EmptyWhileParams()
           identifier: EmptyWhileParams
           argument_list: ()
            (: (
            ): )
          ;: ;
         }: }
       declaration: TF_Graph* cond_graph = TF_NewGraph();
        type_identifier: TF_Graph
        init_declarator: * cond_graph = TF_NewGraph()
         pointer_declarator: * cond_graph
          *: *
          identifier: cond_graph
         =: =
         call_expression: TF_NewGraph()
          identifier: TF_NewGraph
          argument_list: ()
           (: (
           ): )
        ;: ;
       declaration: TF_Graph* body_graph = TF_NewGraph();
        type_identifier: TF_Graph
        init_declarator: * body_graph = TF_NewGraph()
         pointer_declarator: * body_graph
          *: *
          identifier: body_graph
         =: =
         call_expression: TF_NewGraph()
          identifier: TF_NewGraph
          argument_list: ()
           (: (
           ): )
        ;: ;
       expression_statement: cond_graph->parent = g;
        assignment_expression: cond_graph->parent = g
         field_expression: cond_graph->parent
          identifier: cond_graph
          ->: ->
          field_identifier: parent
         =: =
         identifier: g
        ;: ;
       expression_statement: cond_graph->parent_inputs = inputs;
        assignment_expression: cond_graph->parent_inputs = inputs
         field_expression: cond_graph->parent_inputs
          identifier: cond_graph
          ->: ->
          field_identifier: parent_inputs
         =: =
         identifier: inputs
        ;: ;
       expression_statement: body_graph->parent = g;
        assignment_expression: body_graph->parent = g
         field_expression: body_graph->parent
          identifier: body_graph
          ->: ->
          field_identifier: parent
         =: =
         identifier: g
        ;: ;
       expression_statement: body_graph->parent_inputs = inputs;
        assignment_expression: body_graph->parent_inputs = inputs
         field_expression: body_graph->parent_inputs
          identifier: body_graph
          ->: ->
          field_identifier: parent_inputs
         =: =
         identifier: inputs
        ;: ;
       declaration: TF_Output* cond_inputs = new TF_Output[ninputs];
        type_identifier: TF_Output
        init_declarator: * cond_inputs = new TF_Output[ninputs]
         pointer_declarator: * cond_inputs
          *: *
          identifier: cond_inputs
         =: =
         new_expression: new TF_Output[ninputs]
          new: new
          type_identifier: TF_Output
          new_declarator: [ninputs]
           [: [
           identifier: ninputs
           ]: ]
        ;: ;
       declaration: TF_Output cond_output = {nullptr, -1};
        type_identifier: TF_Output
        init_declarator: cond_output = {nullptr, -1}
         identifier: cond_output
         =: =
         initializer_list: {nullptr, -1}
          {: {
          null: nullptr
           nullptr: nullptr
          ,: ,
          number_literal: -1
          }: }
        ;: ;
       declaration: TF_Output* body_inputs = new TF_Output[ninputs];
        type_identifier: TF_Output
        init_declarator: * body_inputs = new TF_Output[ninputs]
         pointer_declarator: * body_inputs
          *: *
          identifier: body_inputs
         =: =
         new_expression: new TF_Output[ninputs]
          new: new
          type_identifier: TF_Output
          new_declarator: [ninputs]
           [: [
           identifier: ninputs
           ]: ]
        ;: ;
       declaration: TF_Output* body_outputs = new TF_Output[ninputs];
        type_identifier: TF_Output
        init_declarator: * body_outputs = new TF_Output[ninputs]
         pointer_declarator: * body_outputs
          *: *
          identifier: body_outputs
         =: =
         new_expression: new TF_Output[ninputs]
          new: new
          type_identifier: TF_Output
          new_declarator: [ninputs]
           [: [
           identifier: ninputs
           ]: ]
        ;: ;
       for_statement: for (int i = 0; i < ninputs; ++i) body_outputs[i] = {nullptr, -1};
        for: for
        (: (
        declaration: int i = 0;
         primitive_type: int
         init_declarator: i = 0
          identifier: i
          =: =
          number_literal: 0
         ;: ;
        binary_expression: i < ninputs
         identifier: i
         <: <
         identifier: ninputs
        ;: ;
        update_expression: ++i
         ++: ++
         identifier: i
        ): )
        expression_statement: body_outputs[i] = {nullptr, -1};
         assignment_expression: body_outputs[i] = {nullptr, -1}
          subscript_expression: body_outputs[i]
           identifier: body_outputs
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          =: =
          initializer_list: {nullptr, -1}
           {: {
           null: nullptr
            nullptr: nullptr
           ,: ,
           number_literal: -1
           }: }
         ;: ;
       declaration: const char* name = nullptr;
        type_qualifier: const
         const: const
        primitive_type: char
        init_declarator: * name = nullptr
         pointer_declarator: * name
          *: *
          identifier: name
         =: =
         null: nullptr
          nullptr: nullptr
        ;: ;
       for_statement: for (int i = 0; i < ninputs; ++i) {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }
        for: for
        (: (
        declaration: int i = 0;
         primitive_type: int
         init_declarator: i = 0
          identifier: i
          =: =
          number_literal: 0
         ;: ;
        binary_expression: i < ninputs
         identifier: i
         <: <
         identifier: ninputs
        ;: ;
        update_expression: ++i
         ++: ++
         identifier: i
        ): )
        compound_statement: {
    // TODO(skyewm): prefix names with underscore (requires some plumbing)
    if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
    if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
  }
         {: {
         comment: // TODO(skyewm): prefix names with underscore (requires some plumbing)
         if_statement: if (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)) {
      break;
    }
          if: if
          condition_clause: (!CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status))
           (: (
           unary_expression: !CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)
            !: !
            call_expression: CreateInput(inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)
             identifier: CreateInput
             argument_list: (inputs[i], cond_graph, StrCat("cond_input", i).c_str(),
                     &cond_inputs[i], status)
              (: (
              subscript_expression: inputs[i]
               identifier: inputs
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              ,: ,
              identifier: cond_graph
              ,: ,
              call_expression: StrCat("cond_input", i).c_str()
               field_expression: StrCat("cond_input", i).c_str
                call_expression: StrCat("cond_input", i)
                 identifier: StrCat
                 argument_list: ("cond_input", i)
                  (: (
                  string_literal: "cond_input"
                   ": "
                   string_content: cond_input
                   ": "
                  ,: ,
                  identifier: i
                  ): )
                .: .
                field_identifier: c_str
               argument_list: ()
                (: (
                ): )
              ,: ,
              pointer_expression: &cond_inputs[i]
               &: &
               subscript_expression: cond_inputs[i]
                identifier: cond_inputs
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
              ,: ,
              identifier: status
              ): )
           ): )
          compound_statement: {
      break;
    }
           {: {
           break_statement: break;
            break: break
            ;: ;
           }: }
         if_statement: if (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)) {
      break;
    }
          if: if
          condition_clause: (!CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status))
           (: (
           unary_expression: !CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)
            !: !
            call_expression: CreateInput(inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)
             identifier: CreateInput
             argument_list: (inputs[i], body_graph, StrCat("body_input", i).c_str(),
                     &body_inputs[i], status)
              (: (
              subscript_expression: inputs[i]
               identifier: inputs
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              ,: ,
              identifier: body_graph
              ,: ,
              call_expression: StrCat("body_input", i).c_str()
               field_expression: StrCat("body_input", i).c_str
                call_expression: StrCat("body_input", i)
                 identifier: StrCat
                 argument_list: ("body_input", i)
                  (: (
                  string_literal: "body_input"
                   ": "
                   string_content: body_input
                   ": "
                  ,: ,
                  identifier: i
                  ): )
                .: .
                field_identifier: c_str
               argument_list: ()
                (: (
                ): )
              ,: ,
              pointer_expression: &body_inputs[i]
               &: &
               subscript_expression: body_inputs[i]
                identifier: body_inputs
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
              ,: ,
              identifier: status
              ): )
           ): )
          compound_statement: {
      break;
    }
           {: {
           break_statement: break;
            break: break
            ;: ;
           }: }
         }: }
       declaration: TF_WhileParams params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name};
        type_identifier: TF_WhileParams
        init_declarator: params = {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name}
         identifier: params
         =: =
         initializer_list: {ninputs,    cond_graph,  cond_inputs,  cond_output,
                           body_graph, body_inputs, body_outputs, name}
          {: {
          identifier: ninputs
          ,: ,
          identifier: cond_graph
          ,: ,
          identifier: cond_inputs
          ,: ,
          identifier: cond_output
          ,: ,
          identifier: body_graph
          ,: ,
          identifier: body_inputs
          ,: ,
          identifier: body_outputs
          ,: ,
          identifier: name
          }: }
        ;: ;
       if_statement: if (!status->status.ok()) {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
    FreeWhileResources(&params);
    return EmptyWhileParams();
  }
         {: {
         expression_statement: FreeWhileResources(&params);
          call_expression: FreeWhileResources(&params)
           identifier: FreeWhileResources
           argument_list: (&params)
            (: (
            pointer_expression: &params
             &: &
             identifier: params
            ): )
          ;: ;
         return_statement: return EmptyWhileParams();
          return: return
          call_expression: EmptyWhileParams()
           identifier: EmptyWhileParams
           argument_list: ()
            (: (
            ): )
          ;: ;
         }: }
       return_statement: return params;
        return: return
        identifier: params
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
namespace {

// TODO(skyewm): make nodes in while loop unfetchable like in Python version
void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}

}  // namespace
#endif
    #if: #if
    binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
     unary_expression: !defined(IS_MOBILE_PLATFORM)
      !: !
      preproc_defined: defined(IS_MOBILE_PLATFORM)
       defined: defined
       (: (
       identifier: IS_MOBILE_PLATFORM
       ): )
     &&: &&
     unary_expression: !defined(IS_SLIM_BUILD)
      !: !
      preproc_defined: defined(IS_SLIM_BUILD)
       defined: defined
       (: (
       identifier: IS_SLIM_BUILD
       ): )
    
: 

    namespace_definition: namespace {

// TODO(skyewm): make nodes in while loop unfetchable like in Python version
void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}

}
     namespace: namespace
     declaration_list: {

// TODO(skyewm): make nodes in while loop unfetchable like in Python version
void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}

}
      {: {
      comment: // TODO(skyewm): make nodes in while loop unfetchable like in Python version
      function_definition: void TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs) {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}
       primitive_type: void
       function_declarator: TF_FinishWhileHelper(const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs)
        identifier: TF_FinishWhileHelper
        parameter_list: (const TF_WhileParams* params, TF_Status* status,
                          TF_Output* outputs)
         (: (
         parameter_declaration: const TF_WhileParams* params
          type_qualifier: const
           const: const
          type_identifier: TF_WhileParams
          pointer_declarator: * params
           *: *
           identifier: params
         ,: ,
         parameter_declaration: TF_Status* status
          type_identifier: TF_Status
          pointer_declarator: * status
           *: *
           identifier: status
         ,: ,
         parameter_declaration: TF_Output* outputs
          type_identifier: TF_Output
          pointer_declarator: * outputs
           *: *
           identifier: outputs
         ): )
       compound_statement: {
  if (!ValidateInputWhileParams(*params, status)) return;

  TF_Graph* parent = params->cond_graph->parent;
  TF_Output* parent_inputs = params->cond_graph->parent_inputs;
  int num_loop_vars = params->ninputs;

  mutex_lock l(parent->mu);

  // 'cond_fn' copies the cond graph into the parent graph.
  tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };

  // 'body_fn' copies the body graph into the parent graph.
  tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };

  // Create the while loop using an internal scope.
  tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);

  const int first_new_node_id = parent->graph.num_node_ids();

  tensorflow::OutputList loop_outputs;
  status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);

  // Update name_map with newly-created ops.
  // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
  // a bad status. Once we fix this, we may want to return early instead of
  // executing the following code.
  for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }

  // Populate 'outputs'.
  DCHECK_LE(loop_outputs.size(), num_loop_vars);
  for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
}
        {: {
        if_statement: if (!ValidateInputWhileParams(*params, status)) return;
         if: if
         condition_clause: (!ValidateInputWhileParams(*params, status))
          (: (
          unary_expression: !ValidateInputWhileParams(*params, status)
           !: !
           call_expression: ValidateInputWhileParams(*params, status)
            identifier: ValidateInputWhileParams
            argument_list: (*params, status)
             (: (
             pointer_expression: *params
              *: *
              identifier: params
             ,: ,
             identifier: status
             ): )
          ): )
         return_statement: return;
          return: return
          ;: ;
        declaration: TF_Graph* parent = params->cond_graph->parent;
         type_identifier: TF_Graph
         init_declarator: * parent = params->cond_graph->parent
          pointer_declarator: * parent
           *: *
           identifier: parent
          =: =
          field_expression: params->cond_graph->parent
           field_expression: params->cond_graph
            identifier: params
            ->: ->
            field_identifier: cond_graph
           ->: ->
           field_identifier: parent
         ;: ;
        declaration: TF_Output* parent_inputs = params->cond_graph->parent_inputs;
         type_identifier: TF_Output
         init_declarator: * parent_inputs = params->cond_graph->parent_inputs
          pointer_declarator: * parent_inputs
           *: *
           identifier: parent_inputs
          =: =
          field_expression: params->cond_graph->parent_inputs
           field_expression: params->cond_graph
            identifier: params
            ->: ->
            field_identifier: cond_graph
           ->: ->
           field_identifier: parent_inputs
         ;: ;
        declaration: int num_loop_vars = params->ninputs;
         primitive_type: int
         init_declarator: num_loop_vars = params->ninputs
          identifier: num_loop_vars
          =: =
          field_expression: params->ninputs
           identifier: params
           ->: ->
           field_identifier: ninputs
         ;: ;
        declaration: mutex_lock l(parent->mu);
         type_identifier: mutex_lock
         init_declarator: l(parent->mu)
          identifier: l
          argument_list: (parent->mu)
           (: (
           field_expression: parent->mu
            identifier: parent
            ->: ->
            field_identifier: mu
           ): )
         ;: ;
        comment: // 'cond_fn' copies the cond graph into the parent graph.
        declaration: tensorflow::ops::CondGraphBuilderFn cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      };
         qualified_identifier: tensorflow::ops::CondGraphBuilderFn
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: ops::CondGraphBuilderFn
           namespace_identifier: ops
           ::: ::
           type_identifier: CondGraphBuilderFn
         init_declarator: cond_fn =
      [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      }
          identifier: cond_fn
          =: =
          lambda_expression: [params, parent](const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      }
           lambda_capture_specifier: [params, parent]
            [: [
            identifier: params
            ,: ,
            identifier: parent
            ]: ]
           lambda_declarator: (const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output)
            parameter_list: (const tensorflow::Scope& scope,
                       const std::vector<tensorflow::Output>& inputs,
                       tensorflow::Output* output)
             (: (
             parameter_declaration: const tensorflow::Scope& scope
              type_qualifier: const
               const: const
              qualified_identifier: tensorflow::Scope
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: Scope
              reference_declarator: & scope
               &: &
               identifier: scope
             ,: ,
             parameter_declaration: const std::vector<tensorflow::Output>& inputs
              type_qualifier: const
               const: const
              qualified_identifier: std::vector<tensorflow::Output>
               namespace_identifier: std
               ::: ::
               template_type: vector<tensorflow::Output>
                type_identifier: vector
                template_argument_list: <tensorflow::Output>
                 <: <
                 type_descriptor: tensorflow::Output
                  qualified_identifier: tensorflow::Output
                   namespace_identifier: tensorflow
                   ::: ::
                   type_identifier: Output
                 >: >
              reference_declarator: & inputs
               &: &
               identifier: inputs
             ,: ,
             parameter_declaration: tensorflow::Output* output
              qualified_identifier: tensorflow::Output
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: Output
              pointer_declarator: * output
               *: *
               identifier: output
             ): )
           compound_statement: {
        DCHECK_EQ(scope.graph(), &parent->graph);
        std::vector<tensorflow::Output> cond_output;
        TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
        *output = cond_output[0];
        return absl::OkStatus();
      }
            {: {
            expression_statement: DCHECK_EQ(scope.graph(), &parent->graph);
             call_expression: DCHECK_EQ(scope.graph(), &parent->graph)
              identifier: DCHECK_EQ
              argument_list: (scope.graph(), &parent->graph)
               (: (
               call_expression: scope.graph()
                field_expression: scope.graph
                 identifier: scope
                 .: .
                 field_identifier: graph
                argument_list: ()
                 (: (
                 ): )
               ,: ,
               pointer_expression: &parent->graph
                &: &
                field_expression: parent->graph
                 identifier: parent
                 ->: ->
                 field_identifier: graph
               ): )
             ;: ;
            declaration: std::vector<tensorflow::Output> cond_output;
             qualified_identifier: std::vector<tensorflow::Output>
              namespace_identifier: std
              ::: ::
              template_type: vector<tensorflow::Output>
               type_identifier: vector
               template_argument_list: <tensorflow::Output>
                <: <
                type_descriptor: tensorflow::Output
                 qualified_identifier: tensorflow::Output
                  namespace_identifier: tensorflow
                  ::: ::
                  type_identifier: Output
                >: >
             identifier: cond_output
             ;: ;
            expression_statement: TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output));
             call_expression: TF_RETURN_IF_ERROR(CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output))
              identifier: TF_RETURN_IF_ERROR
              argument_list: (CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output))
               (: (
               call_expression: CopyGraph(
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output)
                identifier: CopyGraph
                argument_list: (
            &params->cond_graph->graph, &parent->graph, &parent->refiner,
            params->cond_inputs, inputs, scope.impl()->name(),
            scope.impl()->control_deps(), &params->cond_output,
            /* nreturn_nodes */ 1, &cond_output)
                 (: (
                 pointer_expression: &params->cond_graph->graph
                  &: &
                  field_expression: params->cond_graph->graph
                   field_expression: params->cond_graph
                    identifier: params
                    ->: ->
                    field_identifier: cond_graph
                   ->: ->
                   field_identifier: graph
                 ,: ,
                 pointer_expression: &parent->graph
                  &: &
                  field_expression: parent->graph
                   identifier: parent
                   ->: ->
                   field_identifier: graph
                 ,: ,
                 pointer_expression: &parent->refiner
                  &: &
                  field_expression: parent->refiner
                   identifier: parent
                   ->: ->
                   field_identifier: refiner
                 ,: ,
                 field_expression: params->cond_inputs
                  identifier: params
                  ->: ->
                  field_identifier: cond_inputs
                 ,: ,
                 identifier: inputs
                 ,: ,
                 call_expression: scope.impl()->name()
                  field_expression: scope.impl()->name
                   call_expression: scope.impl()
                    field_expression: scope.impl
                     identifier: scope
                     .: .
                     field_identifier: impl
                    argument_list: ()
                     (: (
                     ): )
                   ->: ->
                   field_identifier: name
                  argument_list: ()
                   (: (
                   ): )
                 ,: ,
                 call_expression: scope.impl()->control_deps()
                  field_expression: scope.impl()->control_deps
                   call_expression: scope.impl()
                    field_expression: scope.impl
                     identifier: scope
                     .: .
                     field_identifier: impl
                    argument_list: ()
                     (: (
                     ): )
                   ->: ->
                   field_identifier: control_deps
                  argument_list: ()
                   (: (
                   ): )
                 ,: ,
                 pointer_expression: &params->cond_output
                  &: &
                  field_expression: params->cond_output
                   identifier: params
                   ->: ->
                   field_identifier: cond_output
                 ,: ,
                 comment: /* nreturn_nodes */
                 number_literal: 1
                 ,: ,
                 pointer_expression: &cond_output
                  &: &
                  identifier: cond_output
                 ): )
               ): )
             ;: ;
            expression_statement: *output = cond_output[0];
             assignment_expression: *output = cond_output[0]
              pointer_expression: *output
               *: *
               identifier: output
              =: =
              subscript_expression: cond_output[0]
               identifier: cond_output
               subscript_argument_list: [0]
                [: [
                number_literal: 0
                ]: ]
             ;: ;
            return_statement: return absl::OkStatus();
             return: return
             call_expression: absl::OkStatus()
              qualified_identifier: absl::OkStatus
               namespace_identifier: absl
               ::: ::
               identifier: OkStatus
              argument_list: ()
               (: (
               ): )
             ;: ;
            }: }
         ;: ;
        comment: // 'body_fn' copies the body graph into the parent graph.
        declaration: tensorflow::ops::BodyGraphBuilderFn body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      };
         qualified_identifier: tensorflow::ops::BodyGraphBuilderFn
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: ops::BodyGraphBuilderFn
           namespace_identifier: ops
           ::: ::
           type_identifier: BodyGraphBuilderFn
         init_declarator: body_fn =
      [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      }
          identifier: body_fn
          =: =
          lambda_expression: [params, parent, num_loop_vars](
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs) {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      }
           lambda_capture_specifier: [params, parent, num_loop_vars]
            [: [
            identifier: params
            ,: ,
            identifier: parent
            ,: ,
            identifier: num_loop_vars
            ]: ]
           lambda_declarator: (
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs)
            parameter_list: (
          const tensorflow::Scope& scope,
          const std::vector<tensorflow::Output>& inputs,
          std::vector<tensorflow::Output>* outputs)
             (: (
             parameter_declaration: const tensorflow::Scope& scope
              type_qualifier: const
               const: const
              qualified_identifier: tensorflow::Scope
               namespace_identifier: tensorflow
               ::: ::
               type_identifier: Scope
              reference_declarator: & scope
               &: &
               identifier: scope
             ,: ,
             parameter_declaration: const std::vector<tensorflow::Output>& inputs
              type_qualifier: const
               const: const
              qualified_identifier: std::vector<tensorflow::Output>
               namespace_identifier: std
               ::: ::
               template_type: vector<tensorflow::Output>
                type_identifier: vector
                template_argument_list: <tensorflow::Output>
                 <: <
                 type_descriptor: tensorflow::Output
                  qualified_identifier: tensorflow::Output
                   namespace_identifier: tensorflow
                   ::: ::
                   type_identifier: Output
                 >: >
              reference_declarator: & inputs
               &: &
               identifier: inputs
             ,: ,
             parameter_declaration: std::vector<tensorflow::Output>* outputs
              qualified_identifier: std::vector<tensorflow::Output>
               namespace_identifier: std
               ::: ::
               template_type: vector<tensorflow::Output>
                type_identifier: vector
                template_argument_list: <tensorflow::Output>
                 <: <
                 type_descriptor: tensorflow::Output
                  qualified_identifier: tensorflow::Output
                   namespace_identifier: tensorflow
                   ::: ::
                   type_identifier: Output
                 >: >
              pointer_declarator: * outputs
               *: *
               identifier: outputs
             ): )
           compound_statement: {
        DCHECK_EQ(scope.graph(), &parent->graph);
        TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
        return absl::OkStatus();
      }
            {: {
            expression_statement: DCHECK_EQ(scope.graph(), &parent->graph);
             call_expression: DCHECK_EQ(scope.graph(), &parent->graph)
              identifier: DCHECK_EQ
              argument_list: (scope.graph(), &parent->graph)
               (: (
               call_expression: scope.graph()
                field_expression: scope.graph
                 identifier: scope
                 .: .
                 field_identifier: graph
                argument_list: ()
                 (: (
                 ): )
               ,: ,
               pointer_expression: &parent->graph
                &: &
                field_expression: parent->graph
                 identifier: parent
                 ->: ->
                 field_identifier: graph
               ): )
             ;: ;
            expression_statement: TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs));
             call_expression: TF_RETURN_IF_ERROR(
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs))
              identifier: TF_RETURN_IF_ERROR
              argument_list: (
            CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs))
               (: (
               call_expression: CopyGraph(&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs)
                identifier: CopyGraph
                argument_list: (&params->body_graph->graph, &parent->graph,
                      &parent->refiner, params->body_inputs, inputs,
                      scope.impl()->name(), scope.impl()->control_deps(),
                      params->body_outputs, num_loop_vars, outputs)
                 (: (
                 pointer_expression: &params->body_graph->graph
                  &: &
                  field_expression: params->body_graph->graph
                   field_expression: params->body_graph
                    identifier: params
                    ->: ->
                    field_identifier: body_graph
                   ->: ->
                   field_identifier: graph
                 ,: ,
                 pointer_expression: &parent->graph
                  &: &
                  field_expression: parent->graph
                   identifier: parent
                   ->: ->
                   field_identifier: graph
                 ,: ,
                 pointer_expression: &parent->refiner
                  &: &
                  field_expression: parent->refiner
                   identifier: parent
                   ->: ->
                   field_identifier: refiner
                 ,: ,
                 field_expression: params->body_inputs
                  identifier: params
                  ->: ->
                  field_identifier: body_inputs
                 ,: ,
                 identifier: inputs
                 ,: ,
                 call_expression: scope.impl()->name()
                  field_expression: scope.impl()->name
                   call_expression: scope.impl()
                    field_expression: scope.impl
                     identifier: scope
                     .: .
                     field_identifier: impl
                    argument_list: ()
                     (: (
                     ): )
                   ->: ->
                   field_identifier: name
                  argument_list: ()
                   (: (
                   ): )
                 ,: ,
                 call_expression: scope.impl()->control_deps()
                  field_expression: scope.impl()->control_deps
                   call_expression: scope.impl()
                    field_expression: scope.impl
                     identifier: scope
                     .: .
                     field_identifier: impl
                    argument_list: ()
                     (: (
                     ): )
                   ->: ->
                   field_identifier: control_deps
                  argument_list: ()
                   (: (
                   ): )
                 ,: ,
                 field_expression: params->body_outputs
                  identifier: params
                  ->: ->
                  field_identifier: body_outputs
                 ,: ,
                 identifier: num_loop_vars
                 ,: ,
                 identifier: outputs
                 ): )
               ): )
             ;: ;
            return_statement: return absl::OkStatus();
             return: return
             call_expression: absl::OkStatus()
              qualified_identifier: absl::OkStatus
               namespace_identifier: absl
               ::: ::
               identifier: OkStatus
              argument_list: ()
               (: (
               ): )
             ;: ;
            }: }
         ;: ;
        comment: // Create the while loop using an internal scope.
        declaration: tensorflow::Scope scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name);
         qualified_identifier: tensorflow::Scope
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: Scope
         init_declarator: scope =
      NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name)
          identifier: scope
          =: =
          call_expression: NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope(params->name)
           field_expression: NewInternalScope(&parent->graph, &status->status, &parent->refiner)
          .NewSubScope
            call_expression: NewInternalScope(&parent->graph, &status->status, &parent->refiner)
             identifier: NewInternalScope
             argument_list: (&parent->graph, &status->status, &parent->refiner)
              (: (
              pointer_expression: &parent->graph
               &: &
               field_expression: parent->graph
                identifier: parent
                ->: ->
                field_identifier: graph
              ,: ,
              pointer_expression: &status->status
               &: &
               field_expression: status->status
                identifier: status
                ->: ->
                field_identifier: status
              ,: ,
              pointer_expression: &parent->refiner
               &: &
               field_expression: parent->refiner
                identifier: parent
                ->: ->
                field_identifier: refiner
              ): )
            .: .
            field_identifier: NewSubScope
           argument_list: (params->name)
            (: (
            field_expression: params->name
             identifier: params
             ->: ->
             field_identifier: name
            ): )
         ;: ;
        declaration: const int first_new_node_id = parent->graph.num_node_ids();
         type_qualifier: const
          const: const
         primitive_type: int
         init_declarator: first_new_node_id = parent->graph.num_node_ids()
          identifier: first_new_node_id
          =: =
          call_expression: parent->graph.num_node_ids()
           field_expression: parent->graph.num_node_ids
            field_expression: parent->graph
             identifier: parent
             ->: ->
             field_identifier: graph
            .: .
            field_identifier: num_node_ids
           argument_list: ()
            (: (
            ): )
         ;: ;
        declaration: tensorflow::OutputList loop_outputs;
         qualified_identifier: tensorflow::OutputList
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: OutputList
         identifier: loop_outputs
         ;: ;
        expression_statement: status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs);
         assignment_expression: status->status = tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs)
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          =: =
          call_expression: tensorflow::ops::BuildWhileLoop(
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs)
           qualified_identifier: tensorflow::ops::BuildWhileLoop
            namespace_identifier: tensorflow
            ::: ::
            qualified_identifier: ops::BuildWhileLoop
             namespace_identifier: ops
             ::: ::
             identifier: BuildWhileLoop
           argument_list: (
      scope, OutputsFromTFOutputs(parent_inputs, num_loop_vars), cond_fn,
      body_fn, params->name, &loop_outputs)
            (: (
            identifier: scope
            ,: ,
            call_expression: OutputsFromTFOutputs(parent_inputs, num_loop_vars)
             identifier: OutputsFromTFOutputs
             argument_list: (parent_inputs, num_loop_vars)
              (: (
              identifier: parent_inputs
              ,: ,
              identifier: num_loop_vars
              ): )
            ,: ,
            identifier: cond_fn
            ,: ,
            identifier: body_fn
            ,: ,
            field_expression: params->name
             identifier: params
             ->: ->
             field_identifier: name
            ,: ,
            pointer_expression: &loop_outputs
             &: &
             identifier: loop_outputs
            ): )
         ;: ;
        comment: // Update name_map with newly-created ops.
        comment: // TODO(skyewm): right now BuildWhileLoop() may alter the graph if it returns
        comment: // a bad status. Once we fix this, we may want to return early instead of
        comment: // executing the following code.
        for_statement: for (int i = first_new_node_id; i < parent->graph.num_node_ids(); ++i) {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }
         for: for
         (: (
         declaration: int i = first_new_node_id;
          primitive_type: int
          init_declarator: i = first_new_node_id
           identifier: i
           =: =
           identifier: first_new_node_id
          ;: ;
         binary_expression: i < parent->graph.num_node_ids()
          identifier: i
          <: <
          call_expression: parent->graph.num_node_ids()
           field_expression: parent->graph.num_node_ids
            field_expression: parent->graph
             identifier: parent
             ->: ->
             field_identifier: graph
            .: .
            field_identifier: num_node_ids
           argument_list: ()
            (: (
            ): )
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
    Node* new_node = parent->graph.FindNodeId(i);
    if (new_node == nullptr) continue;
    parent->name_map[new_node->name()] = new_node;
  }
          {: {
          declaration: Node* new_node = parent->graph.FindNodeId(i);
           type_identifier: Node
           init_declarator: * new_node = parent->graph.FindNodeId(i)
            pointer_declarator: * new_node
             *: *
             identifier: new_node
            =: =
            call_expression: parent->graph.FindNodeId(i)
             field_expression: parent->graph.FindNodeId
              field_expression: parent->graph
               identifier: parent
               ->: ->
               field_identifier: graph
              .: .
              field_identifier: FindNodeId
             argument_list: (i)
              (: (
              identifier: i
              ): )
           ;: ;
          if_statement: if (new_node == nullptr) continue;
           if: if
           condition_clause: (new_node == nullptr)
            (: (
            binary_expression: new_node == nullptr
             identifier: new_node
             ==: ==
             null: nullptr
              nullptr: nullptr
            ): )
           continue_statement: continue;
            continue: continue
            ;: ;
          expression_statement: parent->name_map[new_node->name()] = new_node;
           assignment_expression: parent->name_map[new_node->name()] = new_node
            subscript_expression: parent->name_map[new_node->name()]
             field_expression: parent->name_map
              identifier: parent
              ->: ->
              field_identifier: name_map
             subscript_argument_list: [new_node->name()]
              [: [
              call_expression: new_node->name()
               field_expression: new_node->name
                identifier: new_node
                ->: ->
                field_identifier: name
               argument_list: ()
                (: (
                ): )
              ]: ]
            =: =
            identifier: new_node
           ;: ;
          }: }
        comment: // Populate 'outputs'.
        expression_statement: DCHECK_LE(loop_outputs.size(), num_loop_vars);
         call_expression: DCHECK_LE(loop_outputs.size(), num_loop_vars)
          identifier: DCHECK_LE
          argument_list: (loop_outputs.size(), num_loop_vars)
           (: (
           call_expression: loop_outputs.size()
            field_expression: loop_outputs.size
             identifier: loop_outputs
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
           ,: ,
           identifier: num_loop_vars
           ): )
         ;: ;
        for_statement: for (int i = 0; i < loop_outputs.size(); ++i) {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
         for: for
         (: (
         declaration: int i = 0;
          primitive_type: int
          init_declarator: i = 0
           identifier: i
           =: =
           number_literal: 0
          ;: ;
         binary_expression: i < loop_outputs.size()
          identifier: i
          <: <
          call_expression: loop_outputs.size()
           field_expression: loop_outputs.size
            identifier: loop_outputs
            .: .
            field_identifier: size
           argument_list: ()
            (: (
            ): )
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
    outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
  }
          {: {
          expression_statement: outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()};
           assignment_expression: outputs[i] = {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()}
            subscript_expression: outputs[i]
             identifier: outputs
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            =: =
            initializer_list: {ToOperation(loop_outputs[i].node()), loop_outputs[i].index()}
             {: {
             call_expression: ToOperation(loop_outputs[i].node())
              identifier: ToOperation
              argument_list: (loop_outputs[i].node())
               (: (
               call_expression: loop_outputs[i].node()
                field_expression: loop_outputs[i].node
                 subscript_expression: loop_outputs[i]
                  identifier: loop_outputs
                  subscript_argument_list: [i]
                   [: [
                   identifier: i
                   ]: ]
                 .: .
                 field_identifier: node
                argument_list: ()
                 (: (
                 ): )
               ): )
             ,: ,
             call_expression: loop_outputs[i].index()
              field_expression: loop_outputs[i].index
               subscript_expression: loop_outputs[i]
                identifier: loop_outputs
                subscript_argument_list: [i]
                 [: [
                 identifier: i
                 ]: ]
               .: .
               field_identifier: index
              argument_list: ()
               (: (
               ): )
             }: }
           ;: ;
          }: }
        }: }
      }: }
    comment: // namespace
    #endif: #endif
   comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
   function_definition: void TF_FinishWhile(const TF_WhileParams* params, TF_Status* status,
                    TF_Output* outputs) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_FinishWhile(const TF_WhileParams* params, TF_Status* status,
                    TF_Output* outputs)
     identifier: TF_FinishWhile
     parameter_list: (const TF_WhileParams* params, TF_Status* status,
                    TF_Output* outputs)
      (: (
      parameter_declaration: const TF_WhileParams* params
       type_qualifier: const
        const: const
       type_identifier: TF_WhileParams
       pointer_declarator: * params
        *: *
        identifier: params
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ,: ,
      parameter_declaration: TF_Output* outputs
       type_identifier: TF_Output
       pointer_declarator: * outputs
        *: *
        identifier: outputs
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
          (: (
          concatenated_string: "Creating while loops is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you"
           string_literal: "Creating while loops is not supported on mobile. File a bug at "
            ": "
            string_content: Creating while loops is not supported on mobile. File a bug at 
            ": "
           string_literal: "https://github.com/tensorflow/tensorflow/issues if this feature is "
            ": "
            string_content: https://github.com/tensorflow/tensorflow/issues if this feature is 
            ": "
           string_literal: "important to you"
            ": "
            string_content: important to you
            ": "
          ): )
       ;: ;
      preproc_else: #else
  // If it appears the caller created or modified `params`, don't free resources
  if (!ValidateConstWhileParams(*params, status)) return;
  TF_FinishWhileHelper(params, status, outputs);
  FreeWhileResources(params);
       #else: #else
       comment: // If it appears the caller created or modified `params`, don't free resources
       if_statement: if (!ValidateConstWhileParams(*params, status)) return;
        if: if
        condition_clause: (!ValidateConstWhileParams(*params, status))
         (: (
         unary_expression: !ValidateConstWhileParams(*params, status)
          !: !
          call_expression: ValidateConstWhileParams(*params, status)
           identifier: ValidateConstWhileParams
           argument_list: (*params, status)
            (: (
            pointer_expression: *params
             *: *
             identifier: params
            ,: ,
            identifier: status
            ): )
         ): )
        return_statement: return;
         return: return
         ;: ;
       expression_statement: TF_FinishWhileHelper(params, status, outputs);
        call_expression: TF_FinishWhileHelper(params, status, outputs)
         identifier: TF_FinishWhileHelper
         argument_list: (params, status, outputs)
          (: (
          identifier: params
          ,: ,
          identifier: status
          ,: ,
          identifier: outputs
          ): )
        ;: ;
       expression_statement: FreeWhileResources(params);
        call_expression: FreeWhileResources(params)
         identifier: FreeWhileResources
         argument_list: (params)
          (: (
          identifier: params
          ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_AbortWhile(const TF_WhileParams* params) { FreeWhileResources(params); }
    primitive_type: void
    function_declarator: TF_AbortWhile(const TF_WhileParams* params)
     identifier: TF_AbortWhile
     parameter_list: (const TF_WhileParams* params)
      (: (
      parameter_declaration: const TF_WhileParams* params
       type_qualifier: const
        const: const
       type_identifier: TF_WhileParams
       pointer_declarator: * params
        *: *
        identifier: params
      ): )
    compound_statement: { FreeWhileResources(params); }
     {: {
     expression_statement: FreeWhileResources(params);
      call_expression: FreeWhileResources(params)
       identifier: FreeWhileResources
       argument_list: (params)
        (: (
        identifier: params
        ): )
      ;: ;
     }: }
   function_definition: void TF_AddGradients(TF_Graph* g, TF_Output* y, int ny, TF_Output* x, int nx,
                     TF_Output* dx, TF_Status* status, TF_Output* dy) {
  TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy);
}
    primitive_type: void
    function_declarator: TF_AddGradients(TF_Graph* g, TF_Output* y, int ny, TF_Output* x, int nx,
                     TF_Output* dx, TF_Status* status, TF_Output* dy)
     identifier: TF_AddGradients
     parameter_list: (TF_Graph* g, TF_Output* y, int ny, TF_Output* x, int nx,
                     TF_Output* dx, TF_Status* status, TF_Output* dy)
      (: (
      parameter_declaration: TF_Graph* g
       type_identifier: TF_Graph
       pointer_declarator: * g
        *: *
        identifier: g
      ,: ,
      parameter_declaration: TF_Output* y
       type_identifier: TF_Output
       pointer_declarator: * y
        *: *
        identifier: y
      ,: ,
      parameter_declaration: int ny
       primitive_type: int
       identifier: ny
      ,: ,
      parameter_declaration: TF_Output* x
       type_identifier: TF_Output
       pointer_declarator: * x
        *: *
        identifier: x
      ,: ,
      parameter_declaration: int nx
       primitive_type: int
       identifier: nx
      ,: ,
      parameter_declaration: TF_Output* dx
       type_identifier: TF_Output
       pointer_declarator: * dx
        *: *
        identifier: dx
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ,: ,
      parameter_declaration: TF_Output* dy
       type_identifier: TF_Output
       pointer_declarator: * dy
        *: *
        identifier: dy
      ): )
    compound_statement: {
  TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy);
}
     {: {
     expression_statement: TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy);
      call_expression: TF_AddGradientsWithPrefix(g, nullptr, y, ny, x, nx, dx, status, dy)
       identifier: TF_AddGradientsWithPrefix
       argument_list: (g, nullptr, y, ny, x, nx, dx, status, dy)
        (: (
        identifier: g
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: y
        ,: ,
        identifier: ny
        ,: ,
        identifier: x
        ,: ,
        identifier: nx
        ,: ,
        identifier: dx
        ,: ,
        identifier: status
        ,: ,
        identifier: dy
        ): )
      ;: ;
     }: }
   function_definition: void TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix, TF_Output* y,
                               int ny, TF_Output* x, int nx, TF_Output* dx,
                               TF_Status* status, TF_Output* dy) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix, TF_Output* y,
                               int ny, TF_Output* x, int nx, TF_Output* dx,
                               TF_Status* status, TF_Output* dy)
     identifier: TF_AddGradientsWithPrefix
     parameter_list: (TF_Graph* g, const char* prefix, TF_Output* y,
                               int ny, TF_Output* x, int nx, TF_Output* dx,
                               TF_Status* status, TF_Output* dy)
      (: (
      parameter_declaration: TF_Graph* g
       type_identifier: TF_Graph
       pointer_declarator: * g
        *: *
        identifier: g
      ,: ,
      parameter_declaration: const char* prefix
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * prefix
        *: *
        identifier: prefix
      ,: ,
      parameter_declaration: TF_Output* y
       type_identifier: TF_Output
       pointer_declarator: * y
        *: *
        identifier: y
      ,: ,
      parameter_declaration: int ny
       primitive_type: int
       identifier: ny
      ,: ,
      parameter_declaration: TF_Output* x
       type_identifier: TF_Output
       pointer_declarator: * x
        *: *
        identifier: x
      ,: ,
      parameter_declaration: int nx
       primitive_type: int
       identifier: nx
      ,: ,
      parameter_declaration: TF_Output* dx
       type_identifier: TF_Output
       pointer_declarator: * dx
        *: *
        identifier: dx
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ,: ,
      parameter_declaration: TF_Output* dy
       type_identifier: TF_Output
       pointer_declarator: * dy
        *: *
        identifier: dy
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
#else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
          (: (
          concatenated_string: "Adding gradients is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you"
           string_literal: "Adding gradients is not supported on mobile. File a bug at "
            ": "
            string_content: Adding gradients is not supported on mobile. File a bug at 
            ": "
           string_literal: "https://github.com/tensorflow/tensorflow/issues if this feature is "
            ": "
            string_content: https://github.com/tensorflow/tensorflow/issues if this feature is 
            ": "
           string_literal: "important to you"
            ": "
            string_content: important to you
            ": "
          ): )
       ;: ;
      preproc_else: #else
  std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
  std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
  std::vector<tensorflow::Output> dy_arg;

  {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }

  // Unpack the results from grad_outputs_arg.
  TFOutputsFromOutputs(dy_arg, dy);
       #else: #else
       declaration: std::vector<tensorflow::Output> y_arg = OutputsFromTFOutputs(y, ny);
        qualified_identifier: std::vector<tensorflow::Output>
         namespace_identifier: std
         ::: ::
         template_type: vector<tensorflow::Output>
          type_identifier: vector
          template_argument_list: <tensorflow::Output>
           <: <
           type_descriptor: tensorflow::Output
            qualified_identifier: tensorflow::Output
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: Output
           >: >
        init_declarator: y_arg = OutputsFromTFOutputs(y, ny)
         identifier: y_arg
         =: =
         call_expression: OutputsFromTFOutputs(y, ny)
          identifier: OutputsFromTFOutputs
          argument_list: (y, ny)
           (: (
           identifier: y
           ,: ,
           identifier: ny
           ): )
        ;: ;
       declaration: std::vector<tensorflow::Output> x_arg = OutputsFromTFOutputs(x, nx);
        qualified_identifier: std::vector<tensorflow::Output>
         namespace_identifier: std
         ::: ::
         template_type: vector<tensorflow::Output>
          type_identifier: vector
          template_argument_list: <tensorflow::Output>
           <: <
           type_descriptor: tensorflow::Output
            qualified_identifier: tensorflow::Output
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: Output
           >: >
        init_declarator: x_arg = OutputsFromTFOutputs(x, nx)
         identifier: x_arg
         =: =
         call_expression: OutputsFromTFOutputs(x, nx)
          identifier: OutputsFromTFOutputs
          argument_list: (x, nx)
           (: (
           identifier: x
           ,: ,
           identifier: nx
           ): )
        ;: ;
       declaration: std::vector<tensorflow::Output> dy_arg;
        qualified_identifier: std::vector<tensorflow::Output>
         namespace_identifier: std
         ::: ::
         template_type: vector<tensorflow::Output>
          type_identifier: vector
          template_argument_list: <tensorflow::Output>
           <: <
           type_descriptor: tensorflow::Output
            qualified_identifier: tensorflow::Output
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: Output
           >: >
        identifier: dy_arg
        ;: ;
       compound_statement: {
    // We need to hold on to the lock while we have a scope that uses TF_Graph.
    mutex_lock graph_lock(g->mu);

    const int first_new_node_id = g->graph.num_node_ids();

    string prefix_cmp;
    const char* child_scope_name;
    if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
    tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);

    if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }

    // Update g->name_map with the name_map from the scope, which will contain
    // the new gradient ops.
    for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
  }
        {: {
        comment: // We need to hold on to the lock while we have a scope that uses TF_Graph.
        declaration: mutex_lock graph_lock(g->mu);
         type_identifier: mutex_lock
         init_declarator: graph_lock(g->mu)
          identifier: graph_lock
          argument_list: (g->mu)
           (: (
           field_expression: g->mu
            identifier: g
            ->: ->
            field_identifier: mu
           ): )
         ;: ;
        declaration: const int first_new_node_id = g->graph.num_node_ids();
         type_qualifier: const
          const: const
         primitive_type: int
         init_declarator: first_new_node_id = g->graph.num_node_ids()
          identifier: first_new_node_id
          =: =
          call_expression: g->graph.num_node_ids()
           field_expression: g->graph.num_node_ids
            field_expression: g->graph
             identifier: g
             ->: ->
             field_identifier: graph
            .: .
            field_identifier: num_node_ids
           argument_list: ()
            (: (
            ): )
         ;: ;
        declaration: string prefix_cmp;
         type_identifier: string
         identifier: prefix_cmp
         ;: ;
        declaration: const char* child_scope_name;
         type_qualifier: const
          const: const
         primitive_type: char
         pointer_declarator: * child_scope_name
          *: *
          identifier: child_scope_name
         ;: ;
        if_statement: if (prefix == nullptr) {
      child_scope_name = "gradients";
    } else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
         if: if
         condition_clause: (prefix == nullptr)
          (: (
          binary_expression: prefix == nullptr
           identifier: prefix
           ==: ==
           null: nullptr
            nullptr: nullptr
          ): )
         compound_statement: {
      child_scope_name = "gradients";
    }
          {: {
          expression_statement: child_scope_name = "gradients";
           assignment_expression: child_scope_name = "gradients"
            identifier: child_scope_name
            =: =
            string_literal: "gradients"
             ": "
             string_content: gradients
             ": "
           ;: ;
          }: }
         else_clause: else {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
          else: else
          compound_statement: {
      prefix_cmp = string(prefix) + "/";
      // The operation should fail if the provided name prefix has already been
      // used in this graph
      for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
      child_scope_name = prefix;
    }
           {: {
           expression_statement: prefix_cmp = string(prefix) + "/";
            assignment_expression: prefix_cmp = string(prefix) + "/"
             identifier: prefix_cmp
             =: =
             binary_expression: string(prefix) + "/"
              call_expression: string(prefix)
               identifier: string
               argument_list: (prefix)
                (: (
                identifier: prefix
                ): )
              +: +
              string_literal: "/"
               ": "
               string_content: /
               ": "
            ;: ;
           comment: // The operation should fail if the provided name prefix has already been
           comment: // used in this graph
           for_range_loop: for (const auto& pair : g->name_map) {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
            for: for
            (: (
            type_qualifier: const
             const: const
            placeholder_type_specifier: auto
             auto: auto
            reference_declarator: & pair
             &: &
             identifier: pair
            :: :
            field_expression: g->name_map
             identifier: g
             ->: ->
             field_identifier: name_map
            ): )
            compound_statement: {
        const string& name = pair.first;
        if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
      }
             {: {
             declaration: const string& name = pair.first;
              type_qualifier: const
               const: const
              type_identifier: string
              init_declarator: & name = pair.first
               reference_declarator: & name
                &: &
                identifier: name
               =: =
               field_expression: pair.first
                identifier: pair
                .: .
                field_identifier: first
              ;: ;
             if_statement: if ((name == prefix) || absl::StartsWith(name, prefix_cmp)) {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
              if: if
              condition_clause: ((name == prefix) || absl::StartsWith(name, prefix_cmp))
               (: (
               binary_expression: (name == prefix) || absl::StartsWith(name, prefix_cmp)
                parenthesized_expression: (name == prefix)
                 (: (
                 binary_expression: name == prefix
                  identifier: name
                  ==: ==
                  identifier: prefix
                 ): )
                ||: ||
                call_expression: absl::StartsWith(name, prefix_cmp)
                 qualified_identifier: absl::StartsWith
                  namespace_identifier: absl
                  ::: ::
                  identifier: StartsWith
                 argument_list: (name, prefix_cmp)
                  (: (
                  identifier: name
                  ,: ,
                  identifier: prefix_cmp
                  ): )
               ): )
              compound_statement: {
          status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
          return;
        }
               {: {
               expression_statement: status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]");
                assignment_expression: status->status = InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]")
                 field_expression: status->status
                  identifier: status
                  ->: ->
                  field_identifier: status
                 =: =
                 call_expression: InvalidArgument(
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]")
                  identifier: InvalidArgument
                  argument_list: (
              "prefix [", prefix,
              "] conflicts with existing node in the graph named [", name, "]")
                   (: (
                   string_literal: "prefix ["
                    ": "
                    string_content: prefix [
                    ": "
                   ,: ,
                   identifier: prefix
                   ,: ,
                   string_literal: "] conflicts with existing node in the graph named ["
                    ": "
                    string_content: ] conflicts with existing node in the graph named [
                    ": "
                   ,: ,
                   identifier: name
                   ,: ,
                   string_literal: "]"
                    ": "
                    string_content: ]
                    ": "
                   ): )
                ;: ;
               return_statement: return;
                return: return
                ;: ;
               }: }
             }: }
           expression_statement: child_scope_name = prefix;
            assignment_expression: child_scope_name = prefix
             identifier: child_scope_name
             =: =
             identifier: prefix
            ;: ;
           }: }
        declaration: tensorflow::Scope scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name);
         qualified_identifier: tensorflow::Scope
          namespace_identifier: tensorflow
          ::: ::
          type_identifier: Scope
         init_declarator: scope =
        NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name)
          identifier: scope
          =: =
          call_expression: NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope(child_scope_name)
           field_expression: NewInternalScope(&g->graph, &status->status, &g->refiner)
            .NewSubScope
            call_expression: NewInternalScope(&g->graph, &status->status, &g->refiner)
             identifier: NewInternalScope
             argument_list: (&g->graph, &status->status, &g->refiner)
              (: (
              pointer_expression: &g->graph
               &: &
               field_expression: g->graph
                identifier: g
                ->: ->
                field_identifier: graph
              ,: ,
              pointer_expression: &status->status
               &: &
               field_expression: status->status
                identifier: status
                ->: ->
                field_identifier: status
              ,: ,
              pointer_expression: &g->refiner
               &: &
               field_expression: g->refiner
                identifier: g
                ->: ->
                field_identifier: refiner
              ): )
            .: .
            field_identifier: NewSubScope
           argument_list: (child_scope_name)
            (: (
            identifier: child_scope_name
            ): )
         ;: ;
        if_statement: if (dx != nullptr) {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    } else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }
         if: if
         condition_clause: (dx != nullptr)
          (: (
          binary_expression: dx != nullptr
           identifier: dx
           !=: !=
           null: nullptr
            nullptr: nullptr
          ): )
         compound_statement: {
      std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
      status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
    }
          {: {
          declaration: std::vector<tensorflow::Output> dx_arg = OutputsFromTFOutputs(dx, ny);
           qualified_identifier: std::vector<tensorflow::Output>
            namespace_identifier: std
            ::: ::
            template_type: vector<tensorflow::Output>
             type_identifier: vector
             template_argument_list: <tensorflow::Output>
              <: <
              type_descriptor: tensorflow::Output
               qualified_identifier: tensorflow::Output
                namespace_identifier: tensorflow
                ::: ::
                type_identifier: Output
              >: >
           init_declarator: dx_arg = OutputsFromTFOutputs(dx, ny)
            identifier: dx_arg
            =: =
            call_expression: OutputsFromTFOutputs(dx, ny)
             identifier: OutputsFromTFOutputs
             argument_list: (dx, ny)
              (: (
              identifier: dx
              ,: ,
              identifier: ny
              ): )
           ;: ;
          expression_statement: status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg);
           assignment_expression: status->status =
          AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg)
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            =: =
            call_expression: AddSymbolicGradients(scope, y_arg, x_arg, dx_arg, &dy_arg)
             identifier: AddSymbolicGradients
             argument_list: (scope, y_arg, x_arg, dx_arg, &dy_arg)
              (: (
              identifier: scope
              ,: ,
              identifier: y_arg
              ,: ,
              identifier: x_arg
              ,: ,
              identifier: dx_arg
              ,: ,
              pointer_expression: &dy_arg
               &: &
               identifier: dy_arg
              ): )
           ;: ;
          }: }
         else_clause: else {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }
          else: else
          compound_statement: {
      status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
    }
           {: {
           expression_statement: status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg);
            assignment_expression: status->status = AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg)
             field_expression: status->status
              identifier: status
              ->: ->
              field_identifier: status
             =: =
             call_expression: AddSymbolicGradients(scope, y_arg, x_arg, &dy_arg)
              identifier: AddSymbolicGradients
              argument_list: (scope, y_arg, x_arg, &dy_arg)
               (: (
               identifier: scope
               ,: ,
               identifier: y_arg
               ,: ,
               identifier: x_arg
               ,: ,
               pointer_expression: &dy_arg
                &: &
                identifier: dy_arg
               ): )
            ;: ;
           }: }
        comment: // Update g->name_map with the name_map from the scope, which will contain
        comment: // the new gradient ops.
        for_statement: for (int i = first_new_node_id; i < g->graph.num_node_ids(); ++i) {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
         for: for
         (: (
         declaration: int i = first_new_node_id;
          primitive_type: int
          init_declarator: i = first_new_node_id
           identifier: i
           =: =
           identifier: first_new_node_id
          ;: ;
         binary_expression: i < g->graph.num_node_ids()
          identifier: i
          <: <
          call_expression: g->graph.num_node_ids()
           field_expression: g->graph.num_node_ids
            field_expression: g->graph
             identifier: g
             ->: ->
             field_identifier: graph
            .: .
            field_identifier: num_node_ids
           argument_list: ()
            (: (
            ): )
         ;: ;
         update_expression: ++i
          ++: ++
          identifier: i
         ): )
         compound_statement: {
      Node* n = g->graph.FindNodeId(i);
      if (n == nullptr) continue;

      // Adding the gradients to the graph can alter the prefix to prevent
      // name collisions only if this prefix has not been provided explicitly
      // by the user. If it was provided, assert that it remained intact.
      if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
      // We have a convoluted scheme here: Using the C++ graph construction API
      // to add potentially many nodes to the graph without running the checks
      // (such as uniqueness of the names of nodes) we run with other functions
      // that add a node to the graph (like TF_FinishOperation).
      if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
    }
          {: {
          declaration: Node* n = g->graph.FindNodeId(i);
           type_identifier: Node
           init_declarator: * n = g->graph.FindNodeId(i)
            pointer_declarator: * n
             *: *
             identifier: n
            =: =
            call_expression: g->graph.FindNodeId(i)
             field_expression: g->graph.FindNodeId
              field_expression: g->graph
               identifier: g
               ->: ->
               field_identifier: graph
              .: .
              field_identifier: FindNodeId
             argument_list: (i)
              (: (
              identifier: i
              ): )
           ;: ;
          if_statement: if (n == nullptr) continue;
           if: if
           condition_clause: (n == nullptr)
            (: (
            binary_expression: n == nullptr
             identifier: n
             ==: ==
             null: nullptr
              nullptr: nullptr
            ): )
           continue_statement: continue;
            continue: continue
            ;: ;
          comment: // Adding the gradients to the graph can alter the prefix to prevent
          comment: // name collisions only if this prefix has not been provided explicitly
          comment: // by the user. If it was provided, assert that it remained intact.
          if_statement: if (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)) {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
           if: if
           condition_clause: (prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp))
            (: (
            binary_expression: prefix != nullptr && !absl::StartsWith(n->name(), prefix_cmp)
             binary_expression: prefix != nullptr
              identifier: prefix
              !=: !=
              null: nullptr
               nullptr: nullptr
             &&: &&
             unary_expression: !absl::StartsWith(n->name(), prefix_cmp)
              !: !
              call_expression: absl::StartsWith(n->name(), prefix_cmp)
               qualified_identifier: absl::StartsWith
                namespace_identifier: absl
                ::: ::
                identifier: StartsWith
               argument_list: (n->name(), prefix_cmp)
                (: (
                call_expression: n->name()
                 field_expression: n->name
                  identifier: n
                  ->: ->
                  field_identifier: name
                 argument_list: ()
                  (: (
                  ): )
                ,: ,
                identifier: prefix_cmp
                ): )
            ): )
           compound_statement: {
        status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
        return;
      }
            {: {
            expression_statement: status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.");
             assignment_expression: status->status = tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.")
              field_expression: status->status
               identifier: status
               ->: ->
               field_identifier: status
              =: =
              call_expression: tensorflow::errors::Internal(
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.")
               qualified_identifier: tensorflow::errors::Internal
                namespace_identifier: tensorflow
                ::: ::
                qualified_identifier: errors::Internal
                 namespace_identifier: errors
                 ::: ::
                 identifier: Internal
               argument_list: (
            "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues.")
                (: (
                concatenated_string: "BUG: The gradients prefix have been unexpectedly altered when "
            "adding the nodes to the graph. This is a bug. Please file an "
            "issue at https://github.com/tensorflow/tensorflow/issues."
                 string_literal: "BUG: The gradients prefix have been unexpectedly altered when "
                  ": "
                  string_content: BUG: The gradients prefix have been unexpectedly altered when 
                  ": "
                 string_literal: "adding the nodes to the graph. This is a bug. Please file an "
                  ": "
                  string_content: adding the nodes to the graph. This is a bug. Please file an 
                  ": "
                 string_literal: "issue at https://github.com/tensorflow/tensorflow/issues."
                  ": "
                  string_content: issue at https://github.com/tensorflow/tensorflow/issues.
                  ": "
                ): )
             ;: ;
            return_statement: return;
             return: return
             ;: ;
            }: }
          comment: // We have a convoluted scheme here: Using the C++ graph construction API
          comment: // to add potentially many nodes to the graph without running the checks
          comment: // (such as uniqueness of the names of nodes) we run with other functions
          comment: // that add a node to the graph (like TF_FinishOperation).
          if_statement: if (!g->name_map.insert(std::make_pair(n->name(), n)).second) {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
           if: if
           condition_clause: (!g->name_map.insert(std::make_pair(n->name(), n)).second)
            (: (
            unary_expression: !g->name_map.insert(std::make_pair(n->name(), n)).second
             !: !
             field_expression: g->name_map.insert(std::make_pair(n->name(), n)).second
              call_expression: g->name_map.insert(std::make_pair(n->name(), n))
               field_expression: g->name_map.insert
                field_expression: g->name_map
                 identifier: g
                 ->: ->
                 field_identifier: name_map
                .: .
                field_identifier: insert
               argument_list: (std::make_pair(n->name(), n))
                (: (
                call_expression: std::make_pair(n->name(), n)
                 qualified_identifier: std::make_pair
                  namespace_identifier: std
                  ::: ::
                  identifier: make_pair
                 argument_list: (n->name(), n)
                  (: (
                  call_expression: n->name()
                   field_expression: n->name
                    identifier: n
                    ->: ->
                    field_identifier: name
                   argument_list: ()
                    (: (
                    ): )
                  ,: ,
                  identifier: n
                  ): )
                ): )
              .: .
              field_identifier: second
            ): )
           compound_statement: {
        status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
      }
            {: {
            expression_statement: status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.");
             assignment_expression: status->status = tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.")
              field_expression: status->status
               identifier: status
               ->: ->
               field_identifier: status
              =: =
              call_expression: tensorflow::errors::Internal(
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.")
               qualified_identifier: tensorflow::errors::Internal
                namespace_identifier: tensorflow
                ::: ::
                qualified_identifier: errors::Internal
                 namespace_identifier: errors
                 ::: ::
                 identifier: Internal
               argument_list: (
            "BUG: The API allowed construction of a graph with duplicate node "
            "names (",
            n->name(),
            "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues.")
                (: (
                concatenated_string: "BUG: The API allowed construction of a graph with duplicate node "
            "names ("
                 string_literal: "BUG: The API allowed construction of a graph with duplicate node "
                  ": "
                  string_content: BUG: The API allowed construction of a graph with duplicate node 
                  ": "
                 string_literal: "names ("
                  ": "
                  string_content: names (
                  ": "
                ,: ,
                call_expression: n->name()
                 field_expression: n->name
                  identifier: n
                  ->: ->
                  field_identifier: name
                 argument_list: ()
                  (: (
                  ): )
                ,: ,
                concatenated_string: "). This is a bug. Please file an issue at "
            "https://github.com/tensorflow/tensorflow/issues."
                 string_literal: "). This is a bug. Please file an issue at "
                  ": "
                  string_content: ). This is a bug. Please file an issue at 
                  ": "
                 string_literal: "https://github.com/tensorflow/tensorflow/issues."
                  ": "
                  string_content: https://github.com/tensorflow/tensorflow/issues.
                  ": "
                ): )
             ;: ;
            }: }
          }: }
        }: }
       comment: // Unpack the results from grad_outputs_arg.
       expression_statement: TFOutputsFromOutputs(dy_arg, dy);
        call_expression: TFOutputsFromOutputs(dy_arg, dy)
         identifier: TFOutputsFromOutputs
         argument_list: (dy_arg, dy)
          (: (
          identifier: dy_arg
          ,: ,
          identifier: dy
          ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   comment: // TF_Session functions ----------------------------------------------
   function_definition: TF_Session::TF_Session(tensorflow::Session* s, TF_Graph* g)
    : session(s), graph(g), last_num_graph_nodes(0), extend_before_run(true) {}
    function_declarator: TF_Session::TF_Session(tensorflow::Session* s, TF_Graph* g)
     qualified_identifier: TF_Session::TF_Session
      namespace_identifier: TF_Session
      ::: ::
      identifier: TF_Session
     parameter_list: (tensorflow::Session* s, TF_Graph* g)
      (: (
      parameter_declaration: tensorflow::Session* s
       qualified_identifier: tensorflow::Session
        namespace_identifier: tensorflow
        ::: ::
        type_identifier: Session
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: TF_Graph* g
       type_identifier: TF_Graph
       pointer_declarator: * g
        *: *
        identifier: g
      ): )
    field_initializer_list: : session(s), graph(g), last_num_graph_nodes(0), extend_before_run(true)
     :: :
     field_initializer: session(s)
      field_identifier: session
      argument_list: (s)
       (: (
       identifier: s
       ): )
     ,: ,
     field_initializer: graph(g)
      field_identifier: graph
      argument_list: (g)
       (: (
       identifier: g
       ): )
     ,: ,
     field_initializer: last_num_graph_nodes(0)
      field_identifier: last_num_graph_nodes
      argument_list: (0)
       (: (
       number_literal: 0
       ): )
     ,: ,
     field_initializer: extend_before_run(true)
      field_identifier: extend_before_run
      argument_list: (true)
       (: (
       true: true
       ): )
    compound_statement: {}
     {: {
     }: }
   function_definition: TF_Session* TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  } else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}
    type_identifier: TF_Session
    pointer_declarator: * TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status)
     *: *
     function_declarator: TF_NewSession(TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status)
      identifier: TF_NewSession
      parameter_list: (TF_Graph* graph, const TF_SessionOptions* opt,
                          TF_Status* status)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: const TF_SessionOptions* opt
        type_qualifier: const
         const: const
        type_identifier: TF_SessionOptions
        pointer_declarator: * opt
         *: *
         identifier: opt
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  } else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
}
     {: {
     declaration: Session* session;
      type_identifier: Session
      pointer_declarator: * session
       *: *
       identifier: session
      ;: ;
     expression_statement: status->status = NewSession(opt->options, &session);
      assignment_expression: status->status = NewSession(opt->options, &session)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: NewSession(opt->options, &session)
        identifier: NewSession
        argument_list: (opt->options, &session)
         (: (
         field_expression: opt->options
          identifier: opt
          ->: ->
          field_identifier: options
         ,: ,
         pointer_expression: &session
          &: &
          identifier: session
         ): )
      ;: ;
     if_statement: if (status->status.ok()) {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  } else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
      if: if
      condition_clause: (status->status.ok())
       (: (
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
       ): )
      compound_statement: {
    TF_Session* new_session = new TF_Session(session, graph);
    if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
    return new_session;
  }
       {: {
       declaration: TF_Session* new_session = new TF_Session(session, graph);
        type_identifier: TF_Session
        init_declarator: * new_session = new TF_Session(session, graph)
         pointer_declarator: * new_session
          *: *
          identifier: new_session
         =: =
         new_expression: new TF_Session(session, graph)
          new: new
          type_identifier: TF_Session
          argument_list: (session, graph)
           (: (
           identifier: session
           ,: ,
           identifier: graph
           ): )
        ;: ;
       if_statement: if (graph != nullptr) {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
        if: if
        condition_clause: (graph != nullptr)
         (: (
         binary_expression: graph != nullptr
          identifier: graph
          !=: !=
          null: nullptr
           nullptr: nullptr
         ): )
        compound_statement: {
      mutex_lock l(graph->mu);
      graph->sessions[new_session] = "";
    }
         {: {
         declaration: mutex_lock l(graph->mu);
          type_identifier: mutex_lock
          init_declarator: l(graph->mu)
           identifier: l
           argument_list: (graph->mu)
            (: (
            field_expression: graph->mu
             identifier: graph
             ->: ->
             field_identifier: mu
            ): )
          ;: ;
         expression_statement: graph->sessions[new_session] = "";
          assignment_expression: graph->sessions[new_session] = ""
           subscript_expression: graph->sessions[new_session]
            field_expression: graph->sessions
             identifier: graph
             ->: ->
             field_identifier: sessions
            subscript_argument_list: [new_session]
             [: [
             identifier: new_session
             ]: ]
           =: =
           string_literal: ""
            ": "
            ": "
          ;: ;
         }: }
       return_statement: return new_session;
        return: return
        identifier: new_session
        ;: ;
       }: }
      else_clause: else {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
       else: else
       compound_statement: {
    LOG(ERROR) << status->status;
    DCHECK_EQ(nullptr, session);
    return nullptr;
  }
        {: {
        expression_statement: LOG(ERROR) << status->status;
         binary_expression: LOG(ERROR) << status->status
          call_expression: LOG(ERROR)
           identifier: LOG
           argument_list: (ERROR)
            (: (
            identifier: ERROR
            ): )
          <<: <<
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
         ;: ;
        expression_statement: DCHECK_EQ(nullptr, session);
         call_expression: DCHECK_EQ(nullptr, session)
          identifier: DCHECK_EQ
          argument_list: (nullptr, session)
           (: (
           null: nullptr
            nullptr: nullptr
           ,: ,
           identifier: session
           ): )
         ;: ;
        return_statement: return nullptr;
         return: return
         null: nullptr
          nullptr: nullptr
         ;: ;
        }: }
     }: }
   function_definition: TF_Session* TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status) {
// TODO(sjr): Remove the IS_MOBILE_PLATFORM guard. This will require ensuring
// that the tensorflow/cc/saved_model:loader build target is mobile friendly.
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return nullptr;
#else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    type_identifier: TF_Session
    pointer_declarator: * TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status)
     *: *
     function_declarator: TF_LoadSessionFromSavedModel(
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status)
      identifier: TF_LoadSessionFromSavedModel
      parameter_list: (
    const TF_SessionOptions* session_options, const TF_Buffer* run_options,
    const char* export_dir, const char* const* tags, int tags_len,
    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status)
       (: (
       parameter_declaration: const TF_SessionOptions* session_options
        type_qualifier: const
         const: const
        type_identifier: TF_SessionOptions
        pointer_declarator: * session_options
         *: *
         identifier: session_options
       ,: ,
       parameter_declaration: const TF_Buffer* run_options
        type_qualifier: const
         const: const
        type_identifier: TF_Buffer
        pointer_declarator: * run_options
         *: *
         identifier: run_options
       ,: ,
       parameter_declaration: const char* export_dir
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * export_dir
         *: *
         identifier: export_dir
       ,: ,
       parameter_declaration: const char* const* tags
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * const* tags
         *: *
         type_qualifier: const
          const: const
         pointer_declarator: * tags
          *: *
          identifier: tags
       ,: ,
       parameter_declaration: int tags_len
        primitive_type: int
        identifier: tags_len
       ,: ,
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: TF_Buffer* meta_graph_def
        type_identifier: TF_Buffer
        pointer_declarator: * meta_graph_def
         *: *
         identifier: meta_graph_def
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
// TODO(sjr): Remove the IS_MOBILE_PLATFORM guard. This will require ensuring
// that the tensorflow/cc/saved_model:loader build target is mobile friendly.
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return nullptr;
#else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     comment: // TODO(sjr): Remove the IS_MOBILE_PLATFORM guard. This will require ensuring
     comment: // that the tensorflow/cc/saved_model:loader build target is mobile friendly.
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
  return nullptr;
#else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you")
          (: (
          concatenated_string: "Loading a SavedModel is not supported on mobile. File a bug at "
      "https://github.com/tensorflow/tensorflow/issues if this feature is "
      "important to you"
           string_literal: "Loading a SavedModel is not supported on mobile. File a bug at "
            ": "
            string_content: Loading a SavedModel is not supported on mobile. File a bug at 
            ": "
           string_literal: "https://github.com/tensorflow/tensorflow/issues if this feature is "
            ": "
            string_content: https://github.com/tensorflow/tensorflow/issues if this feature is 
            ": "
           string_literal: "important to you"
            ": "
            string_content: important to you
            ": "
          ): )
       ;: ;
      return_statement: return nullptr;
       return: return
       null: nullptr
        nullptr: nullptr
       ;: ;
      preproc_else: #else
  mutex_lock l(graph->mu);
  if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }

  RunOptions run_options_proto;
  if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }

  std::unordered_set<string> tag_set;
  for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }

  tensorflow::SavedModelBundle bundle;
  status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
  if (!status->status.ok()) return nullptr;

  // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
  // extends using GraphDefs. The Graph instance is different, but equivalent
  // to the one used to create the session.
  //
  // TODO(jhseu): When Session is modified to take Graphs instead of
  // GraphDefs, return the Graph generated in LoadSavedModel().
  TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
  TF_ImportGraphDefResults results;
  GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
  TF_DeleteImportGraphDefOptions(import_opts);
  if (!status->status.ok()) return nullptr;

  if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }

  TF_Session* session = new TF_Session(bundle.session.release(), graph);

  graph->sessions[session] = "";
  session->last_num_graph_nodes = graph->graph.num_node_ids();
  return session;
       #else: #else
       declaration: mutex_lock l(graph->mu);
        type_identifier: mutex_lock
        init_declarator: l(graph->mu)
         identifier: l
         argument_list: (graph->mu)
          (: (
          field_expression: graph->mu
           identifier: graph
           ->: ->
           field_identifier: mu
          ): )
        ;: ;
       if_statement: if (!graph->name_map.empty()) {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }
        if: if
        condition_clause: (!graph->name_map.empty())
         (: (
         unary_expression: !graph->name_map.empty()
          !: !
          call_expression: graph->name_map.empty()
           field_expression: graph->name_map.empty
            field_expression: graph->name_map
             identifier: graph
             ->: ->
             field_identifier: name_map
            .: .
            field_identifier: empty
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
    status->status = InvalidArgument("Graph is non-empty.");
    return nullptr;
  }
         {: {
         expression_statement: status->status = InvalidArgument("Graph is non-empty.");
          assignment_expression: status->status = InvalidArgument("Graph is non-empty.")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument("Graph is non-empty.")
            identifier: InvalidArgument
            argument_list: ("Graph is non-empty.")
             (: (
             string_literal: "Graph is non-empty."
              ": "
              string_content: Graph is non-empty.
              ": "
             ): )
          ;: ;
         return_statement: return nullptr;
          return: return
          null: nullptr
           nullptr: nullptr
          ;: ;
         }: }
       declaration: RunOptions run_options_proto;
        type_identifier: RunOptions
        identifier: run_options_proto
        ;: ;
       if_statement: if (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)) {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }
        if: if
        condition_clause: (run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length))
         (: (
         binary_expression: run_options != nullptr && !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)
          binary_expression: run_options != nullptr
           identifier: run_options
           !=: !=
           null: nullptr
            nullptr: nullptr
          &&: &&
          unary_expression: !run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)
           !: !
           call_expression: run_options_proto.ParseFromArray(
                                    run_options->data, run_options->length)
            field_expression: run_options_proto.ParseFromArray
             identifier: run_options_proto
             .: .
             field_identifier: ParseFromArray
            argument_list: (
                                    run_options->data, run_options->length)
             (: (
             field_expression: run_options->data
              identifier: run_options
              ->: ->
              field_identifier: data
             ,: ,
             field_expression: run_options->length
              identifier: run_options
              ->: ->
              field_identifier: length
             ): )
         ): )
        compound_statement: {
    status->status = InvalidArgument("Unparseable RunOptions proto");
    return nullptr;
  }
         {: {
         expression_statement: status->status = InvalidArgument("Unparseable RunOptions proto");
          assignment_expression: status->status = InvalidArgument("Unparseable RunOptions proto")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument("Unparseable RunOptions proto")
            identifier: InvalidArgument
            argument_list: ("Unparseable RunOptions proto")
             (: (
             string_literal: "Unparseable RunOptions proto"
              ": "
              string_content: Unparseable RunOptions proto
              ": "
             ): )
          ;: ;
         return_statement: return nullptr;
          return: return
          null: nullptr
           nullptr: nullptr
          ;: ;
         }: }
       declaration: std::unordered_set<string> tag_set;
        qualified_identifier: std::unordered_set<string>
         namespace_identifier: std
         ::: ::
         template_type: unordered_set<string>
          type_identifier: unordered_set
          template_argument_list: <string>
           <: <
           type_descriptor: string
            type_identifier: string
           >: >
        identifier: tag_set
        ;: ;
       for_statement: for (int i = 0; i < tags_len; i++) {
    tag_set.insert(string(tags[i]));
  }
        for: for
        (: (
        declaration: int i = 0;
         primitive_type: int
         init_declarator: i = 0
          identifier: i
          =: =
          number_literal: 0
         ;: ;
        binary_expression: i < tags_len
         identifier: i
         <: <
         identifier: tags_len
        ;: ;
        update_expression: i++
         identifier: i
         ++: ++
        ): )
        compound_statement: {
    tag_set.insert(string(tags[i]));
  }
         {: {
         expression_statement: tag_set.insert(string(tags[i]));
          call_expression: tag_set.insert(string(tags[i]))
           field_expression: tag_set.insert
            identifier: tag_set
            .: .
            field_identifier: insert
           argument_list: (string(tags[i]))
            (: (
            call_expression: string(tags[i])
             identifier: string
             argument_list: (tags[i])
              (: (
              subscript_expression: tags[i]
               identifier: tags
               subscript_argument_list: [i]
                [: [
                identifier: i
                ]: ]
              ): )
            ): )
          ;: ;
         }: }
       declaration: tensorflow::SavedModelBundle bundle;
        qualified_identifier: tensorflow::SavedModelBundle
         namespace_identifier: tensorflow
         ::: ::
         type_identifier: SavedModelBundle
        identifier: bundle
        ;: ;
       expression_statement: status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle);
        assignment_expression: status->status =
      tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: tensorflow::LoadSavedModel(session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle)
          qualified_identifier: tensorflow::LoadSavedModel
           namespace_identifier: tensorflow
           ::: ::
           identifier: LoadSavedModel
          argument_list: (session_options->options, run_options_proto,
                                 export_dir, tag_set, &bundle)
           (: (
           field_expression: session_options->options
            identifier: session_options
            ->: ->
            field_identifier: options
           ,: ,
           identifier: run_options_proto
           ,: ,
           identifier: export_dir
           ,: ,
           identifier: tag_set
           ,: ,
           pointer_expression: &bundle
            &: &
            identifier: bundle
           ): )
        ;: ;
       if_statement: if (!status->status.ok()) return nullptr;
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        return_statement: return nullptr;
         return: return
         null: nullptr
          nullptr: nullptr
         ;: ;
       comment: // Create a TF_Graph from the MetaGraphDef. This is safe as long as Session
       comment: // extends using GraphDefs. The Graph instance is different, but equivalent
       comment: // to the one used to create the session.
       comment: //
       comment: // TODO(jhseu): When Session is modified to take Graphs instead of
       comment: // GraphDefs, return the Graph generated in LoadSavedModel().
       declaration: TF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();
        type_identifier: TF_ImportGraphDefOptions
        init_declarator: * import_opts = TF_NewImportGraphDefOptions()
         pointer_declarator: * import_opts
          *: *
          identifier: import_opts
         =: =
         call_expression: TF_NewImportGraphDefOptions()
          identifier: TF_NewImportGraphDefOptions
          argument_list: ()
           (: (
           ): )
        ;: ;
       declaration: TF_ImportGraphDefResults results;
        type_identifier: TF_ImportGraphDefResults
        identifier: results
        ;: ;
       expression_statement: GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status);
        call_expression: GraphImportGraphDefLocked(graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status)
         identifier: GraphImportGraphDefLocked
         argument_list: (graph, bundle.meta_graph_def.graph_def(),
                            import_opts, &results, status)
          (: (
          identifier: graph
          ,: ,
          call_expression: bundle.meta_graph_def.graph_def()
           field_expression: bundle.meta_graph_def.graph_def
            field_expression: bundle.meta_graph_def
             identifier: bundle
             .: .
             field_identifier: meta_graph_def
            .: .
            field_identifier: graph_def
           argument_list: ()
            (: (
            ): )
          ,: ,
          identifier: import_opts
          ,: ,
          pointer_expression: &results
           &: &
           identifier: results
          ,: ,
          identifier: status
          ): )
        ;: ;
       expression_statement: TF_DeleteImportGraphDefOptions(import_opts);
        call_expression: TF_DeleteImportGraphDefOptions(import_opts)
         identifier: TF_DeleteImportGraphDefOptions
         argument_list: (import_opts)
          (: (
          identifier: import_opts
          ): )
        ;: ;
       if_statement: if (!status->status.ok()) return nullptr;
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        return_statement: return nullptr;
         return: return
         null: nullptr
          nullptr: nullptr
         ;: ;
       if_statement: if (meta_graph_def != nullptr) {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }
        if: if
        condition_clause: (meta_graph_def != nullptr)
         (: (
         binary_expression: meta_graph_def != nullptr
          identifier: meta_graph_def
          !=: !=
          null: nullptr
           nullptr: nullptr
         ): )
        compound_statement: {
    status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
    if (!status->status.ok()) return nullptr;
  }
         {: {
         expression_statement: status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def);
          assignment_expression: status->status = MessageToBuffer(bundle.meta_graph_def, meta_graph_def)
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: MessageToBuffer(bundle.meta_graph_def, meta_graph_def)
            identifier: MessageToBuffer
            argument_list: (bundle.meta_graph_def, meta_graph_def)
             (: (
             field_expression: bundle.meta_graph_def
              identifier: bundle
              .: .
              field_identifier: meta_graph_def
             ,: ,
             identifier: meta_graph_def
             ): )
          ;: ;
         if_statement: if (!status->status.ok()) return nullptr;
          if: if
          condition_clause: (!status->status.ok())
           (: (
           unary_expression: !status->status.ok()
            !: !
            call_expression: status->status.ok()
             field_expression: status->status.ok
              field_expression: status->status
               identifier: status
               ->: ->
               field_identifier: status
              .: .
              field_identifier: ok
             argument_list: ()
              (: (
              ): )
           ): )
          return_statement: return nullptr;
           return: return
           null: nullptr
            nullptr: nullptr
           ;: ;
         }: }
       declaration: TF_Session* session = new TF_Session(bundle.session.release(), graph);
        type_identifier: TF_Session
        init_declarator: * session = new TF_Session(bundle.session.release(), graph)
         pointer_declarator: * session
          *: *
          identifier: session
         =: =
         new_expression: new TF_Session(bundle.session.release(), graph)
          new: new
          type_identifier: TF_Session
          argument_list: (bundle.session.release(), graph)
           (: (
           call_expression: bundle.session.release()
            field_expression: bundle.session.release
             field_expression: bundle.session
              identifier: bundle
              .: .
              field_identifier: session
             .: .
             field_identifier: release
            argument_list: ()
             (: (
             ): )
           ,: ,
           identifier: graph
           ): )
        ;: ;
       expression_statement: graph->sessions[session] = "";
        assignment_expression: graph->sessions[session] = ""
         subscript_expression: graph->sessions[session]
          field_expression: graph->sessions
           identifier: graph
           ->: ->
           field_identifier: sessions
          subscript_argument_list: [session]
           [: [
           identifier: session
           ]: ]
         =: =
         string_literal: ""
          ": "
          ": "
        ;: ;
       expression_statement: session->last_num_graph_nodes = graph->graph.num_node_ids();
        assignment_expression: session->last_num_graph_nodes = graph->graph.num_node_ids()
         field_expression: session->last_num_graph_nodes
          identifier: session
          ->: ->
          field_identifier: last_num_graph_nodes
         =: =
         call_expression: graph->graph.num_node_ids()
          field_expression: graph->graph.num_node_ids
           field_expression: graph->graph
            identifier: graph
            ->: ->
            field_identifier: graph
           .: .
           field_identifier: num_node_ids
          argument_list: ()
           (: (
           ): )
        ;: ;
       return_statement: return session;
        return: return
        identifier: session
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_CloseSession(TF_Session* s, TF_Status* status) {
  status->status = s->session->Close();
}
    primitive_type: void
    function_declarator: TF_CloseSession(TF_Session* s, TF_Status* status)
     identifier: TF_CloseSession
     parameter_list: (TF_Session* s, TF_Status* status)
      (: (
      parameter_declaration: TF_Session* s
       type_identifier: TF_Session
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  status->status = s->session->Close();
}
     {: {
     expression_statement: status->status = s->session->Close();
      assignment_expression: status->status = s->session->Close()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: s->session->Close()
        field_expression: s->session->Close
         field_expression: s->session
          identifier: s
          ->: ->
          field_identifier: session
         ->: ->
         field_identifier: Close
        argument_list: ()
         (: (
         ): )
      ;: ;
     }: }
   function_definition: void TF_DeleteSession(TF_Session* s, TF_Status* status) {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  TF_Graph* const graph = s->graph;
  if (graph != nullptr) {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
  delete s->session;
  delete s;
}
    primitive_type: void
    function_declarator: TF_DeleteSession(TF_Session* s, TF_Status* status)
     identifier: TF_DeleteSession
     parameter_list: (TF_Session* s, TF_Status* status)
      (: (
      parameter_declaration: TF_Session* s
       type_identifier: TF_Session
       pointer_declarator: * s
        *: *
        identifier: s
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  status->status = absl::OkStatus();
  if (s == nullptr) return;
  TF_Graph* const graph = s->graph;
  if (graph != nullptr) {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
  delete s->session;
  delete s;
}
     {: {
     expression_statement: status->status = absl::OkStatus();
      assignment_expression: status->status = absl::OkStatus()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: absl::OkStatus()
        qualified_identifier: absl::OkStatus
         namespace_identifier: absl
         ::: ::
         identifier: OkStatus
        argument_list: ()
         (: (
         ): )
      ;: ;
     if_statement: if (s == nullptr) return;
      if: if
      condition_clause: (s == nullptr)
       (: (
       binary_expression: s == nullptr
        identifier: s
        ==: ==
        null: nullptr
         nullptr: nullptr
       ): )
      return_statement: return;
       return: return
       ;: ;
     declaration: TF_Graph* const graph = s->graph;
      type_identifier: TF_Graph
      init_declarator: * const graph = s->graph
       pointer_declarator: * const graph
        *: *
        type_qualifier: const
         const: const
        identifier: graph
       =: =
       field_expression: s->graph
        identifier: s
        ->: ->
        field_identifier: graph
      ;: ;
     if_statement: if (graph != nullptr) {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
      if: if
      condition_clause: (graph != nullptr)
       (: (
       binary_expression: graph != nullptr
        identifier: graph
        !=: !=
        null: nullptr
         nullptr: nullptr
       ): )
      compound_statement: {
    graph->mu.lock();
    graph->sessions.erase(s);
    const bool del = graph->delete_requested && graph->sessions.empty();
    graph->mu.unlock();
    if (del) delete graph;
  }
       {: {
       expression_statement: graph->mu.lock();
        call_expression: graph->mu.lock()
         field_expression: graph->mu.lock
          field_expression: graph->mu
           identifier: graph
           ->: ->
           field_identifier: mu
          .: .
          field_identifier: lock
         argument_list: ()
          (: (
          ): )
        ;: ;
       expression_statement: graph->sessions.erase(s);
        call_expression: graph->sessions.erase(s)
         field_expression: graph->sessions.erase
          field_expression: graph->sessions
           identifier: graph
           ->: ->
           field_identifier: sessions
          .: .
          field_identifier: erase
         argument_list: (s)
          (: (
          identifier: s
          ): )
        ;: ;
       declaration: const bool del = graph->delete_requested && graph->sessions.empty();
        type_qualifier: const
         const: const
        primitive_type: bool
        init_declarator: del = graph->delete_requested && graph->sessions.empty()
         identifier: del
         =: =
         binary_expression: graph->delete_requested && graph->sessions.empty()
          field_expression: graph->delete_requested
           identifier: graph
           ->: ->
           field_identifier: delete_requested
          &&: &&
          call_expression: graph->sessions.empty()
           field_expression: graph->sessions.empty
            field_expression: graph->sessions
             identifier: graph
             ->: ->
             field_identifier: sessions
            .: .
            field_identifier: empty
           argument_list: ()
            (: (
            ): )
        ;: ;
       expression_statement: graph->mu.unlock();
        call_expression: graph->mu.unlock()
         field_expression: graph->mu.unlock
          field_expression: graph->mu
           identifier: graph
           ->: ->
           field_identifier: mu
          .: .
          field_identifier: unlock
         argument_list: ()
          (: (
          ): )
        ;: ;
       if_statement: if (del) delete graph;
        if: if
        condition_clause: (del)
         (: (
         identifier: del
         ): )
        expression_statement: delete graph;
         delete_expression: delete graph
          delete: delete
          identifier: graph
         ;: ;
       }: }
     expression_statement: delete s->session;
      delete_expression: delete s->session
       delete: delete
       field_expression: s->session
        identifier: s
        ->: ->
        field_identifier: session
      ;: ;
     expression_statement: delete s;
      delete_expression: delete s
       delete: delete
       identifier: s
      ;: ;
     }: }
   function_definition: void TF_SessionRun(TF_Session* session, const TF_Buffer* run_options,
                   const TF_Output* inputs, TF_Tensor* const* input_values,
                   int ninputs, const TF_Output* outputs,
                   TF_Tensor** output_values, int noutputs,
                   const TF_Operation* const* target_opers, int ntargets,
                   TF_Buffer* run_metadata, TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  // Actually run.
  TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status);
}
    primitive_type: void
    function_declarator: TF_SessionRun(TF_Session* session, const TF_Buffer* run_options,
                   const TF_Output* inputs, TF_Tensor* const* input_values,
                   int ninputs, const TF_Output* outputs,
                   TF_Tensor** output_values, int noutputs,
                   const TF_Operation* const* target_opers, int ntargets,
                   TF_Buffer* run_metadata, TF_Status* status)
     identifier: TF_SessionRun
     parameter_list: (TF_Session* session, const TF_Buffer* run_options,
                   const TF_Output* inputs, TF_Tensor* const* input_values,
                   int ninputs, const TF_Output* outputs,
                   TF_Tensor** output_values, int noutputs,
                   const TF_Operation* const* target_opers, int ntargets,
                   TF_Buffer* run_metadata, TF_Status* status)
      (: (
      parameter_declaration: TF_Session* session
       type_identifier: TF_Session
       pointer_declarator: * session
        *: *
        identifier: session
      ,: ,
      parameter_declaration: const TF_Buffer* run_options
       type_qualifier: const
        const: const
       type_identifier: TF_Buffer
       pointer_declarator: * run_options
        *: *
        identifier: run_options
      ,: ,
      parameter_declaration: const TF_Output* inputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * inputs
        *: *
        identifier: inputs
      ,: ,
      parameter_declaration: TF_Tensor* const* input_values
       type_identifier: TF_Tensor
       pointer_declarator: * const* input_values
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * input_values
         *: *
         identifier: input_values
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      parameter_declaration: const TF_Output* outputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * outputs
        *: *
        identifier: outputs
      ,: ,
      parameter_declaration: TF_Tensor** output_values
       type_identifier: TF_Tensor
       pointer_declarator: ** output_values
        *: *
        pointer_declarator: * output_values
         *: *
         identifier: output_values
      ,: ,
      parameter_declaration: int noutputs
       primitive_type: int
       identifier: noutputs
      ,: ,
      parameter_declaration: const TF_Operation* const* target_opers
       type_qualifier: const
        const: const
       type_identifier: TF_Operation
       pointer_declarator: * const* target_opers
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * target_opers
         *: *
         identifier: target_opers
      ,: ,
      parameter_declaration: int ntargets
       primitive_type: int
       identifier: ntargets
      ,: ,
      parameter_declaration: TF_Buffer* run_metadata
       type_identifier: TF_Buffer
       pointer_declarator: * run_metadata
        *: *
        identifier: run_metadata
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  // Actually run.
  TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status);
}
     {: {
     comment: // TODO(josh11b,mrry): Change Session to be able to use a Graph*
     comment: // directly, instead of requiring us to serialize to a GraphDef and
     comment: // call Session::Extend().
     if_statement: if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }
      if: if
      condition_clause: (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status))
       (: (
       binary_expression: session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)
        field_expression: session->extend_before_run
         identifier: session
         ->: ->
         field_identifier: extend_before_run
        &&: &&
        unary_expression: !ExtendSessionGraphHelper(session, status)
         !: !
         call_expression: ExtendSessionGraphHelper(session, status)
          identifier: ExtendSessionGraphHelper
          argument_list: (session, status)
           (: (
           identifier: session
           ,: ,
           identifier: status
           ): )
       ): )
      compound_statement: {
    return;
  }
       {: {
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: TF_Run_Setup(noutputs, output_values, status);
      call_expression: TF_Run_Setup(noutputs, output_values, status)
       identifier: TF_Run_Setup
       argument_list: (noutputs, output_values, status)
        (: (
        identifier: noutputs
        ,: ,
        identifier: output_values
        ,: ,
        identifier: status
        ): )
      ;: ;
     comment: // Convert from TF_Output and TF_Tensor to a string and Tensor.
     declaration: std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
      qualified_identifier: std::vector<std::pair<string, Tensor>>
       namespace_identifier: std
       ::: ::
       template_type: vector<std::pair<string, Tensor>>
        type_identifier: vector
        template_argument_list: <std::pair<string, Tensor>>
         <: <
         type_descriptor: std::pair<string, Tensor>
          qualified_identifier: std::pair<string, Tensor>
           namespace_identifier: std
           ::: ::
           template_type: pair<string, Tensor>
            type_identifier: pair
            template_argument_list: <string, Tensor>
             <: <
             type_descriptor: string
              type_identifier: string
             ,: ,
             type_descriptor: Tensor
              type_identifier: Tensor
             >: >
         >: >
      function_declarator: input_pairs(ninputs)
       identifier: input_pairs
       parameter_list: (ninputs)
        (: (
        parameter_declaration: ninputs
         type_identifier: ninputs
        ): )
      ;: ;
     if_statement: if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
      if: if
      condition_clause: (!TF_Run_Inputs(input_values, &input_pairs, status))
       (: (
       unary_expression: !TF_Run_Inputs(input_values, &input_pairs, status)
        !: !
        call_expression: TF_Run_Inputs(input_values, &input_pairs, status)
         identifier: TF_Run_Inputs
         argument_list: (input_values, &input_pairs, status)
          (: (
          identifier: input_values
          ,: ,
          pointer_expression: &input_pairs
           &: &
           identifier: input_pairs
          ,: ,
          identifier: status
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     for_statement: for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ninputs
       identifier: i
       <: <
       identifier: ninputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_pairs[i].first = OutputName(inputs[i]);
  }
       {: {
       expression_statement: input_pairs[i].first = OutputName(inputs[i]);
        assignment_expression: input_pairs[i].first = OutputName(inputs[i])
         field_expression: input_pairs[i].first
          subscript_expression: input_pairs[i]
           identifier: input_pairs
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          .: .
          field_identifier: first
         =: =
         call_expression: OutputName(inputs[i])
          identifier: OutputName
          argument_list: (inputs[i])
           (: (
           subscript_expression: inputs[i]
            identifier: inputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       }: }
     comment: // Convert from TF_Output to string names.
     declaration: std::vector<string> output_names(noutputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: output_names(noutputs)
       identifier: output_names
       parameter_list: (noutputs)
        (: (
        parameter_declaration: noutputs
         type_identifier: noutputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < noutputs
       identifier: i
       <: <
       identifier: noutputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    output_names[i] = OutputName(outputs[i]);
  }
       {: {
       expression_statement: output_names[i] = OutputName(outputs[i]);
        assignment_expression: output_names[i] = OutputName(outputs[i])
         subscript_expression: output_names[i]
          identifier: output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: OutputName(outputs[i])
          identifier: OutputName
          argument_list: (outputs[i])
           (: (
           subscript_expression: outputs[i]
            identifier: outputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       }: }
     comment: // Convert from TF_Operation* to string names.
     declaration: std::vector<string> target_names(ntargets);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: target_names(ntargets)
       identifier: target_names
       parameter_list: (ntargets)
        (: (
        parameter_declaration: ntargets
         type_identifier: ntargets
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ntargets
       identifier: i
       <: <
       identifier: ntargets
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    target_names[i] = target_opers[i]->node.name();
  }
       {: {
       expression_statement: target_names[i] = target_opers[i]->node.name();
        assignment_expression: target_names[i] = target_opers[i]->node.name()
         subscript_expression: target_names[i]
          identifier: target_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: target_opers[i]->node.name()
          field_expression: target_opers[i]->node.name
           field_expression: target_opers[i]->node
            subscript_expression: target_opers[i]
             identifier: target_opers
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            ->: ->
            field_identifier: node
           .: .
           field_identifier: name
          argument_list: ()
           (: (
           ): )
        ;: ;
       }: }
     comment: // Actually run.
     expression_statement: TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status);
      call_expression: TF_Run_Helper(session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status)
       identifier: TF_Run_Helper
       argument_list: (session->session, nullptr, run_options, input_pairs,
                output_names, output_values, target_names, run_metadata,
                status)
        (: (
        field_expression: session->session
         identifier: session
         ->: ->
         field_identifier: session
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: run_options
        ,: ,
        identifier: input_pairs
        ,: ,
        identifier: output_names
        ,: ,
        identifier: output_values
        ,: ,
        identifier: target_names
        ,: ,
        identifier: run_metadata
        ,: ,
        identifier: status
        ): )
      ;: ;
     }: }
   function_definition: void TF_SessionPRunSetup(TF_Session* session, const TF_Output* inputs,
                         int ninputs, const TF_Output* outputs, int noutputs,
                         const TF_Operation* const* target_opers, int ntargets,
                         const char** handle, TF_Status* status) {
  *handle = nullptr;

  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  std::vector<string> input_names(ninputs);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = OutputName(inputs[i]);
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  string new_handle;
  status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}
    primitive_type: void
    function_declarator: TF_SessionPRunSetup(TF_Session* session, const TF_Output* inputs,
                         int ninputs, const TF_Output* outputs, int noutputs,
                         const TF_Operation* const* target_opers, int ntargets,
                         const char** handle, TF_Status* status)
     identifier: TF_SessionPRunSetup
     parameter_list: (TF_Session* session, const TF_Output* inputs,
                         int ninputs, const TF_Output* outputs, int noutputs,
                         const TF_Operation* const* target_opers, int ntargets,
                         const char** handle, TF_Status* status)
      (: (
      parameter_declaration: TF_Session* session
       type_identifier: TF_Session
       pointer_declarator: * session
        *: *
        identifier: session
      ,: ,
      parameter_declaration: const TF_Output* inputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * inputs
        *: *
        identifier: inputs
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      parameter_declaration: const TF_Output* outputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * outputs
        *: *
        identifier: outputs
      ,: ,
      parameter_declaration: int noutputs
       primitive_type: int
       identifier: noutputs
      ,: ,
      parameter_declaration: const TF_Operation* const* target_opers
       type_qualifier: const
        const: const
       type_identifier: TF_Operation
       pointer_declarator: * const* target_opers
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * target_opers
         *: *
         identifier: target_opers
      ,: ,
      parameter_declaration: int ntargets
       primitive_type: int
       identifier: ntargets
      ,: ,
      parameter_declaration: const char** handle
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: ** handle
        *: *
        pointer_declarator: * handle
         *: *
         identifier: handle
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  *handle = nullptr;

  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  std::vector<string> input_names(ninputs);
  for (int i = 0; i < ninputs; ++i) {
    input_names[i] = OutputName(inputs[i]);
  }

  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  string new_handle;
  status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle);
  if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
}
     {: {
     expression_statement: *handle = nullptr;
      assignment_expression: *handle = nullptr
       pointer_expression: *handle
        *: *
        identifier: handle
       =: =
       null: nullptr
        nullptr: nullptr
      ;: ;
     if_statement: if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }
      if: if
      condition_clause: (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status))
       (: (
       binary_expression: session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)
        field_expression: session->extend_before_run
         identifier: session
         ->: ->
         field_identifier: extend_before_run
        &&: &&
        unary_expression: !ExtendSessionGraphHelper(session, status)
         !: !
         call_expression: ExtendSessionGraphHelper(session, status)
          identifier: ExtendSessionGraphHelper
          argument_list: (session, status)
           (: (
           identifier: session
           ,: ,
           identifier: status
           ): )
       ): )
      compound_statement: {
    return;
  }
       {: {
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: std::vector<string> input_names(ninputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: input_names(ninputs)
       identifier: input_names
       parameter_list: (ninputs)
        (: (
        parameter_declaration: ninputs
         type_identifier: ninputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ninputs; ++i) {
    input_names[i] = OutputName(inputs[i]);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ninputs
       identifier: i
       <: <
       identifier: ninputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_names[i] = OutputName(inputs[i]);
  }
       {: {
       expression_statement: input_names[i] = OutputName(inputs[i]);
        assignment_expression: input_names[i] = OutputName(inputs[i])
         subscript_expression: input_names[i]
          identifier: input_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: OutputName(inputs[i])
          identifier: OutputName
          argument_list: (inputs[i])
           (: (
           subscript_expression: inputs[i]
            identifier: inputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       }: }
     declaration: std::vector<string> output_names(noutputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: output_names(noutputs)
       identifier: output_names
       parameter_list: (noutputs)
        (: (
        parameter_declaration: noutputs
         type_identifier: noutputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < noutputs
       identifier: i
       <: <
       identifier: noutputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    output_names[i] = OutputName(outputs[i]);
  }
       {: {
       expression_statement: output_names[i] = OutputName(outputs[i]);
        assignment_expression: output_names[i] = OutputName(outputs[i])
         subscript_expression: output_names[i]
          identifier: output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: OutputName(outputs[i])
          identifier: OutputName
          argument_list: (outputs[i])
           (: (
           subscript_expression: outputs[i]
            identifier: outputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       }: }
     declaration: std::vector<string> target_names(ntargets);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: target_names(ntargets)
       identifier: target_names
       parameter_list: (ntargets)
        (: (
        parameter_declaration: ntargets
         type_identifier: ntargets
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ntargets
       identifier: i
       <: <
       identifier: ntargets
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    target_names[i] = target_opers[i]->node.name();
  }
       {: {
       expression_statement: target_names[i] = target_opers[i]->node.name();
        assignment_expression: target_names[i] = target_opers[i]->node.name()
         subscript_expression: target_names[i]
          identifier: target_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: target_opers[i]->node.name()
          field_expression: target_opers[i]->node.name
           field_expression: target_opers[i]->node
            subscript_expression: target_opers[i]
             identifier: target_opers
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            ->: ->
            field_identifier: node
           .: .
           field_identifier: name
          argument_list: ()
           (: (
           ): )
        ;: ;
       }: }
     declaration: string new_handle;
      type_identifier: string
      identifier: new_handle
      ;: ;
     expression_statement: status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle);
      assignment_expression: status->status = session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: session->session->PRunSetup(input_names, output_names,
                                               target_names, &new_handle)
        field_expression: session->session->PRunSetup
         field_expression: session->session
          identifier: session
          ->: ->
          field_identifier: session
         ->: ->
         field_identifier: PRunSetup
        argument_list: (input_names, output_names,
                                               target_names, &new_handle)
         (: (
         identifier: input_names
         ,: ,
         identifier: output_names
         ,: ,
         identifier: target_names
         ,: ,
         pointer_expression: &new_handle
          &: &
          identifier: new_handle
         ): )
      ;: ;
     if_statement: if (status->status.ok()) {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
      if: if
      condition_clause: (status->status.ok())
       (: (
       call_expression: status->status.ok()
        field_expression: status->status.ok
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         .: .
         field_identifier: ok
        argument_list: ()
         (: (
         ): )
       ): )
      compound_statement: {
    char* buf = new char[new_handle.size() + 1];
    memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
    *handle = buf;
  }
       {: {
       declaration: char* buf = new char[new_handle.size() + 1];
        primitive_type: char
        init_declarator: * buf = new char[new_handle.size() + 1]
         pointer_declarator: * buf
          *: *
          identifier: buf
         =: =
         new_expression: new char[new_handle.size() + 1]
          new: new
          primitive_type: char
          new_declarator: [new_handle.size() + 1]
           [: [
           binary_expression: new_handle.size() + 1
            call_expression: new_handle.size()
             field_expression: new_handle.size
              identifier: new_handle
              .: .
              field_identifier: size
             argument_list: ()
              (: (
              ): )
            +: +
            number_literal: 1
           ]: ]
        ;: ;
       expression_statement: memcpy(buf, new_handle.c_str(), new_handle.size() + 1);
        call_expression: memcpy(buf, new_handle.c_str(), new_handle.size() + 1)
         identifier: memcpy
         argument_list: (buf, new_handle.c_str(), new_handle.size() + 1)
          (: (
          identifier: buf
          ,: ,
          call_expression: new_handle.c_str()
           field_expression: new_handle.c_str
            identifier: new_handle
            .: .
            field_identifier: c_str
           argument_list: ()
            (: (
            ): )
          ,: ,
          binary_expression: new_handle.size() + 1
           call_expression: new_handle.size()
            field_expression: new_handle.size
             identifier: new_handle
             .: .
             field_identifier: size
            argument_list: ()
             (: (
             ): )
           +: +
           number_literal: 1
          ): )
        ;: ;
       expression_statement: *handle = buf;
        assignment_expression: *handle = buf
         pointer_expression: *handle
          *: *
          identifier: handle
         =: =
         identifier: buf
        ;: ;
       }: }
     }: }
   function_definition: void TF_DeletePRunHandle(const char* handle) {
  delete[] handle;
  // TODO(suharshs): Free up any resources held by the partial run state.
}
    primitive_type: void
    function_declarator: TF_DeletePRunHandle(const char* handle)
     identifier: TF_DeletePRunHandle
     parameter_list: (const char* handle)
      (: (
      parameter_declaration: const char* handle
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * handle
        *: *
        identifier: handle
      ): )
    compound_statement: {
  delete[] handle;
  // TODO(suharshs): Free up any resources held by the partial run state.
}
     {: {
     expression_statement: delete[] handle;
      delete_expression: delete[] handle
       delete: delete
       [: [
       ]: ]
       identifier: handle
      ;: ;
     comment: // TODO(suharshs): Free up any resources held by the partial run state.
     }: }
   function_definition: void TF_SessionPRun(TF_Session* session, const char* handle,
                    const TF_Output* inputs, TF_Tensor* const* input_values,
                    int ninputs, const TF_Output* outputs,
                    TF_Tensor** output_values, int noutputs,
                    const TF_Operation* const* target_opers, int ntargets,
                    TF_Status* status) {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status);
}
    primitive_type: void
    function_declarator: TF_SessionPRun(TF_Session* session, const char* handle,
                    const TF_Output* inputs, TF_Tensor* const* input_values,
                    int ninputs, const TF_Output* outputs,
                    TF_Tensor** output_values, int noutputs,
                    const TF_Operation* const* target_opers, int ntargets,
                    TF_Status* status)
     identifier: TF_SessionPRun
     parameter_list: (TF_Session* session, const char* handle,
                    const TF_Output* inputs, TF_Tensor* const* input_values,
                    int ninputs, const TF_Output* outputs,
                    TF_Tensor** output_values, int noutputs,
                    const TF_Operation* const* target_opers, int ntargets,
                    TF_Status* status)
      (: (
      parameter_declaration: TF_Session* session
       type_identifier: TF_Session
       pointer_declarator: * session
        *: *
        identifier: session
      ,: ,
      parameter_declaration: const char* handle
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * handle
        *: *
        identifier: handle
      ,: ,
      parameter_declaration: const TF_Output* inputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * inputs
        *: *
        identifier: inputs
      ,: ,
      parameter_declaration: TF_Tensor* const* input_values
       type_identifier: TF_Tensor
       pointer_declarator: * const* input_values
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * input_values
         *: *
         identifier: input_values
      ,: ,
      parameter_declaration: int ninputs
       primitive_type: int
       identifier: ninputs
      ,: ,
      parameter_declaration: const TF_Output* outputs
       type_qualifier: const
        const: const
       type_identifier: TF_Output
       pointer_declarator: * outputs
        *: *
        identifier: outputs
      ,: ,
      parameter_declaration: TF_Tensor** output_values
       type_identifier: TF_Tensor
       pointer_declarator: ** output_values
        *: *
        pointer_declarator: * output_values
         *: *
         identifier: output_values
      ,: ,
      parameter_declaration: int noutputs
       primitive_type: int
       identifier: noutputs
      ,: ,
      parameter_declaration: const TF_Operation* const* target_opers
       type_qualifier: const
        const: const
       type_identifier: TF_Operation
       pointer_declarator: * const* target_opers
        *: *
        type_qualifier: const
         const: const
        pointer_declarator: * target_opers
         *: *
         identifier: target_opers
      ,: ,
      parameter_declaration: int ntargets
       primitive_type: int
       identifier: ntargets
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  // TODO(josh11b,mrry): Change Session to be able to use a Graph*
  // directly, instead of requiring us to serialize to a GraphDef and
  // call Session::Extend().
  if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }

  TF_Run_Setup(noutputs, output_values, status);

  // Convert from TF_Output and TF_Tensor to a string and Tensor.
  std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
  if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
  for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }

  // Convert from TF_Output to string names.
  std::vector<string> output_names(noutputs);
  for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }

  // Convert from TF_Operation* to string names.
  std::vector<string> target_names(ntargets);
  for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }

  TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status);
}
     {: {
     comment: // TODO(josh11b,mrry): Change Session to be able to use a Graph*
     comment: // directly, instead of requiring us to serialize to a GraphDef and
     comment: // call Session::Extend().
     if_statement: if (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)) {
    return;
  }
      if: if
      condition_clause: (session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status))
       (: (
       binary_expression: session->extend_before_run &&
      !ExtendSessionGraphHelper(session, status)
        field_expression: session->extend_before_run
         identifier: session
         ->: ->
         field_identifier: extend_before_run
        &&: &&
        unary_expression: !ExtendSessionGraphHelper(session, status)
         !: !
         call_expression: ExtendSessionGraphHelper(session, status)
          identifier: ExtendSessionGraphHelper
          argument_list: (session, status)
           (: (
           identifier: session
           ,: ,
           identifier: status
           ): )
       ): )
      compound_statement: {
    return;
  }
       {: {
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: TF_Run_Setup(noutputs, output_values, status);
      call_expression: TF_Run_Setup(noutputs, output_values, status)
       identifier: TF_Run_Setup
       argument_list: (noutputs, output_values, status)
        (: (
        identifier: noutputs
        ,: ,
        identifier: output_values
        ,: ,
        identifier: status
        ): )
      ;: ;
     comment: // Convert from TF_Output and TF_Tensor to a string and Tensor.
     declaration: std::vector<std::pair<string, Tensor>> input_pairs(ninputs);
      qualified_identifier: std::vector<std::pair<string, Tensor>>
       namespace_identifier: std
       ::: ::
       template_type: vector<std::pair<string, Tensor>>
        type_identifier: vector
        template_argument_list: <std::pair<string, Tensor>>
         <: <
         type_descriptor: std::pair<string, Tensor>
          qualified_identifier: std::pair<string, Tensor>
           namespace_identifier: std
           ::: ::
           template_type: pair<string, Tensor>
            type_identifier: pair
            template_argument_list: <string, Tensor>
             <: <
             type_descriptor: string
              type_identifier: string
             ,: ,
             type_descriptor: Tensor
              type_identifier: Tensor
             >: >
         >: >
      function_declarator: input_pairs(ninputs)
       identifier: input_pairs
       parameter_list: (ninputs)
        (: (
        parameter_declaration: ninputs
         type_identifier: ninputs
        ): )
      ;: ;
     if_statement: if (!TF_Run_Inputs(input_values, &input_pairs, status)) return;
      if: if
      condition_clause: (!TF_Run_Inputs(input_values, &input_pairs, status))
       (: (
       unary_expression: !TF_Run_Inputs(input_values, &input_pairs, status)
        !: !
        call_expression: TF_Run_Inputs(input_values, &input_pairs, status)
         identifier: TF_Run_Inputs
         argument_list: (input_values, &input_pairs, status)
          (: (
          identifier: input_values
          ,: ,
          pointer_expression: &input_pairs
           &: &
           identifier: input_pairs
          ,: ,
          identifier: status
          ): )
       ): )
      return_statement: return;
       return: return
       ;: ;
     for_statement: for (int i = 0; i < ninputs; ++i) {
    input_pairs[i].first = OutputName(inputs[i]);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ninputs
       identifier: i
       <: <
       identifier: ninputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    input_pairs[i].first = OutputName(inputs[i]);
  }
       {: {
       expression_statement: input_pairs[i].first = OutputName(inputs[i]);
        assignment_expression: input_pairs[i].first = OutputName(inputs[i])
         field_expression: input_pairs[i].first
          subscript_expression: input_pairs[i]
           identifier: input_pairs
           subscript_argument_list: [i]
            [: [
            identifier: i
            ]: ]
          .: .
          field_identifier: first
         =: =
         call_expression: OutputName(inputs[i])
          identifier: OutputName
          argument_list: (inputs[i])
           (: (
           subscript_expression: inputs[i]
            identifier: inputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       }: }
     comment: // Convert from TF_Output to string names.
     declaration: std::vector<string> output_names(noutputs);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: output_names(noutputs)
       identifier: output_names
       parameter_list: (noutputs)
        (: (
        parameter_declaration: noutputs
         type_identifier: noutputs
        ): )
      ;: ;
     for_statement: for (int i = 0; i < noutputs; ++i) {
    output_names[i] = OutputName(outputs[i]);
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < noutputs
       identifier: i
       <: <
       identifier: noutputs
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    output_names[i] = OutputName(outputs[i]);
  }
       {: {
       expression_statement: output_names[i] = OutputName(outputs[i]);
        assignment_expression: output_names[i] = OutputName(outputs[i])
         subscript_expression: output_names[i]
          identifier: output_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: OutputName(outputs[i])
          identifier: OutputName
          argument_list: (outputs[i])
           (: (
           subscript_expression: outputs[i]
            identifier: outputs
            subscript_argument_list: [i]
             [: [
             identifier: i
             ]: ]
           ): )
        ;: ;
       }: }
     comment: // Convert from TF_Operation* to string names.
     declaration: std::vector<string> target_names(ntargets);
      qualified_identifier: std::vector<string>
       namespace_identifier: std
       ::: ::
       template_type: vector<string>
        type_identifier: vector
        template_argument_list: <string>
         <: <
         type_descriptor: string
          type_identifier: string
         >: >
      function_declarator: target_names(ntargets)
       identifier: target_names
       parameter_list: (ntargets)
        (: (
        parameter_declaration: ntargets
         type_identifier: ntargets
        ): )
      ;: ;
     for_statement: for (int i = 0; i < ntargets; ++i) {
    target_names[i] = target_opers[i]->node.name();
  }
      for: for
      (: (
      declaration: int i = 0;
       primitive_type: int
       init_declarator: i = 0
        identifier: i
        =: =
        number_literal: 0
       ;: ;
      binary_expression: i < ntargets
       identifier: i
       <: <
       identifier: ntargets
      ;: ;
      update_expression: ++i
       ++: ++
       identifier: i
      ): )
      compound_statement: {
    target_names[i] = target_opers[i]->node.name();
  }
       {: {
       expression_statement: target_names[i] = target_opers[i]->node.name();
        assignment_expression: target_names[i] = target_opers[i]->node.name()
         subscript_expression: target_names[i]
          identifier: target_names
          subscript_argument_list: [i]
           [: [
           identifier: i
           ]: ]
         =: =
         call_expression: target_opers[i]->node.name()
          field_expression: target_opers[i]->node.name
           field_expression: target_opers[i]->node
            subscript_expression: target_opers[i]
             identifier: target_opers
             subscript_argument_list: [i]
              [: [
              identifier: i
              ]: ]
            ->: ->
            field_identifier: node
           .: .
           field_identifier: name
          argument_list: ()
           (: (
           ): )
        ;: ;
       }: }
     expression_statement: TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status);
      call_expression: TF_Run_Helper(session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status)
       identifier: TF_Run_Helper
       argument_list: (session->session, handle, nullptr, input_pairs, output_names,
                output_values, target_names, nullptr, status)
        (: (
        field_expression: session->session
         identifier: session
         ->: ->
         field_identifier: session
        ,: ,
        identifier: handle
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: input_pairs
        ,: ,
        identifier: output_names
        ,: ,
        identifier: output_values
        ,: ,
        identifier: target_names
        ,: ,
        null: nullptr
         nullptr: nullptr
        ,: ,
        identifier: status
        ): )
      ;: ;
     }: }
   function_definition: unsigned char TF_TryEvaluateConstant(TF_Graph* graph, TF_Output output,
                                     TF_Tensor** result, TF_Status* status) {
  mutex_lock l(graph->mu);
  auto status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      });
  if (!status_or.ok() || !status_or->has_value()) {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
  *result = TF_TensorFromTensor(**status_or, &status->status);
  return status->status.ok();
}
    sized_type_specifier: unsigned char
     unsigned: unsigned
     primitive_type: char
    function_declarator: TF_TryEvaluateConstant(TF_Graph* graph, TF_Output output,
                                     TF_Tensor** result, TF_Status* status)
     identifier: TF_TryEvaluateConstant
     parameter_list: (TF_Graph* graph, TF_Output output,
                                     TF_Tensor** result, TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Output output
       type_identifier: TF_Output
       identifier: output
      ,: ,
      parameter_declaration: TF_Tensor** result
       type_identifier: TF_Tensor
       pointer_declarator: ** result
        *: *
        pointer_declarator: * result
         *: *
         identifier: result
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  mutex_lock l(graph->mu);
  auto status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      });
  if (!status_or.ok() || !status_or->has_value()) {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
  *result = TF_TensorFromTensor(**status_or, &status->status);
  return status->status.ok();
}
     {: {
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: auto status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      });
      placeholder_type_specifier: auto
       auto: auto
      init_declarator: status_or = EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      })
       identifier: status_or
       =: =
       call_expression: EvaluateConstantTensor(
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      })
        identifier: EvaluateConstantTensor
        argument_list: (
      output.oper->node, output.index, graph->refiner,
      [](const Node&, int) { return std::optional<Tensor>(); },
      tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      })
         (: (
         field_expression: output.oper->node
          field_expression: output.oper
           identifier: output
           .: .
           field_identifier: oper
          ->: ->
          field_identifier: node
         ,: ,
         field_expression: output.index
          identifier: output
          .: .
          field_identifier: index
         ,: ,
         field_expression: graph->refiner
          identifier: graph
          ->: ->
          field_identifier: refiner
         ,: ,
         lambda_expression: [](const Node&, int) { return std::optional<Tensor>(); }
          lambda_capture_specifier: []
           [: [
           ]: ]
          lambda_declarator: (const Node&, int)
           parameter_list: (const Node&, int)
            (: (
            parameter_declaration: const Node&
             type_qualifier: const
              const: const
             type_identifier: Node
             abstract_reference_declarator: &
              &: &
            ,: ,
            parameter_declaration: int
             primitive_type: int
            ): )
          compound_statement: { return std::optional<Tensor>(); }
           {: {
           return_statement: return std::optional<Tensor>();
            return: return
            call_expression: std::optional<Tensor>()
             qualified_identifier: std::optional<Tensor>
              namespace_identifier: std
              ::: ::
              template_function: optional<Tensor>
               identifier: optional
               template_argument_list: <Tensor>
                <: <
                type_descriptor: Tensor
                 type_identifier: Tensor
                >: >
             argument_list: ()
              (: (
              ): )
            ;: ;
           }: }
         ,: ,
         compound_literal_expression: tensorflow::EvaluateConstantTensorRunner{
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      }
          qualified_identifier: tensorflow::EvaluateConstantTensorRunner
           namespace_identifier: tensorflow
           ::: ::
           type_identifier: EvaluateConstantTensorRunner
          initializer_list: {
          graph->graph.op_registry(),
          graph->graph.versions().producer(),
      }
           {: {
           call_expression: graph->graph.op_registry()
            field_expression: graph->graph.op_registry
             field_expression: graph->graph
              identifier: graph
              ->: ->
              field_identifier: graph
             .: .
             field_identifier: op_registry
            argument_list: ()
             (: (
             ): )
           ,: ,
           call_expression: graph->graph.versions().producer()
            field_expression: graph->graph.versions().producer
             call_expression: graph->graph.versions()
              field_expression: graph->graph.versions
               field_expression: graph->graph
                identifier: graph
                ->: ->
                field_identifier: graph
               .: .
               field_identifier: versions
              argument_list: ()
               (: (
               ): )
             .: .
             field_identifier: producer
            argument_list: ()
             (: (
             ): )
           ,: ,
           }: }
         ): )
      ;: ;
     if_statement: if (!status_or.ok() || !status_or->has_value()) {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
      if: if
      condition_clause: (!status_or.ok() || !status_or->has_value())
       (: (
       binary_expression: !status_or.ok() || !status_or->has_value()
        unary_expression: !status_or.ok()
         !: !
         call_expression: status_or.ok()
          field_expression: status_or.ok
           identifier: status_or
           .: .
           field_identifier: ok
          argument_list: ()
           (: (
           ): )
        ||: ||
        unary_expression: !status_or->has_value()
         !: !
         call_expression: status_or->has_value()
          field_expression: status_or->has_value
           identifier: status_or
           ->: ->
           field_identifier: has_value
          argument_list: ()
           (: (
           ): )
       ): )
      compound_statement: {
    *result = nullptr;
    status->status = std::move(status_or).status();
    return false;
  }
       {: {
       expression_statement: *result = nullptr;
        assignment_expression: *result = nullptr
         pointer_expression: *result
          *: *
          identifier: result
         =: =
         null: nullptr
          nullptr: nullptr
        ;: ;
       expression_statement: status->status = std::move(status_or).status();
        assignment_expression: status->status = std::move(status_or).status()
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: std::move(status_or).status()
          field_expression: std::move(status_or).status
           call_expression: std::move(status_or)
            qualified_identifier: std::move
             namespace_identifier: std
             ::: ::
             identifier: move
            argument_list: (status_or)
             (: (
             identifier: status_or
             ): )
           .: .
           field_identifier: status
          argument_list: ()
           (: (
           ): )
        ;: ;
       return_statement: return false;
        return: return
        false: false
        ;: ;
       }: }
     expression_statement: *result = TF_TensorFromTensor(**status_or, &status->status);
      assignment_expression: *result = TF_TensorFromTensor(**status_or, &status->status)
       pointer_expression: *result
        *: *
        identifier: result
       =: =
       call_expression: TF_TensorFromTensor(**status_or, &status->status)
        identifier: TF_TensorFromTensor
        argument_list: (**status_or, &status->status)
         (: (
         pointer_expression: **status_or
          *: *
          pointer_expression: *status_or
           *: *
           identifier: status_or
         ,: ,
         pointer_expression: &status->status
          &: &
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
         ): )
      ;: ;
     return_statement: return status->status.ok();
      return: return
      call_expression: status->status.ok()
       field_expression: status->status.ok
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        .: .
        field_identifier: ok
       argument_list: ()
        (: (
        ): )
      ;: ;
     }: }
   function_definition: TF_ApiDefMap* TF_NewApiDefMap(TF_Buffer* op_list_buffer, TF_Status* status) {
  tensorflow::OpList op_list;
  if (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)) {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
  status->status = absl::OkStatus();
  return new TF_ApiDefMap(op_list);
}
    type_identifier: TF_ApiDefMap
    pointer_declarator: * TF_NewApiDefMap(TF_Buffer* op_list_buffer, TF_Status* status)
     *: *
     function_declarator: TF_NewApiDefMap(TF_Buffer* op_list_buffer, TF_Status* status)
      identifier: TF_NewApiDefMap
      parameter_list: (TF_Buffer* op_list_buffer, TF_Status* status)
       (: (
       parameter_declaration: TF_Buffer* op_list_buffer
        type_identifier: TF_Buffer
        pointer_declarator: * op_list_buffer
         *: *
         identifier: op_list_buffer
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  tensorflow::OpList op_list;
  if (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)) {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
  status->status = absl::OkStatus();
  return new TF_ApiDefMap(op_list);
}
     {: {
     declaration: tensorflow::OpList op_list;
      qualified_identifier: tensorflow::OpList
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: OpList
      identifier: op_list
      ;: ;
     if_statement: if (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)) {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
      if: if
      condition_clause: (!op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length))
       (: (
       unary_expression: !op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)
        !: !
        call_expression: op_list.ParseFromArray(op_list_buffer->data, op_list_buffer->length)
         field_expression: op_list.ParseFromArray
          identifier: op_list
          .: .
          field_identifier: ParseFromArray
         argument_list: (op_list_buffer->data, op_list_buffer->length)
          (: (
          field_expression: op_list_buffer->data
           identifier: op_list_buffer
           ->: ->
           field_identifier: data
          ,: ,
          field_expression: op_list_buffer->length
           identifier: op_list_buffer
           ->: ->
           field_identifier: length
          ): )
       ): )
      compound_statement: {
    status->status = InvalidArgument("Unparseable OpList");
    return nullptr;
  }
       {: {
       expression_statement: status->status = InvalidArgument("Unparseable OpList");
        assignment_expression: status->status = InvalidArgument("Unparseable OpList")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: InvalidArgument("Unparseable OpList")
          identifier: InvalidArgument
          argument_list: ("Unparseable OpList")
           (: (
           string_literal: "Unparseable OpList"
            ": "
            string_content: Unparseable OpList
            ": "
           ): )
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     expression_statement: status->status = absl::OkStatus();
      assignment_expression: status->status = absl::OkStatus()
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: absl::OkStatus()
        qualified_identifier: absl::OkStatus
         namespace_identifier: absl
         ::: ::
         identifier: OkStatus
        argument_list: ()
         (: (
         ): )
      ;: ;
     return_statement: return new TF_ApiDefMap(op_list);
      return: return
      new_expression: new TF_ApiDefMap(op_list)
       new: new
       type_identifier: TF_ApiDefMap
       argument_list: (op_list)
        (: (
        identifier: op_list
        ): )
      ;: ;
     }: }
   function_definition: void TF_DeleteApiDefMap(TF_ApiDefMap* apimap) { delete apimap; }
    primitive_type: void
    function_declarator: TF_DeleteApiDefMap(TF_ApiDefMap* apimap)
     identifier: TF_DeleteApiDefMap
     parameter_list: (TF_ApiDefMap* apimap)
      (: (
      parameter_declaration: TF_ApiDefMap* apimap
       type_identifier: TF_ApiDefMap
       pointer_declarator: * apimap
        *: *
        identifier: apimap
      ): )
    compound_statement: { delete apimap; }
     {: {
     expression_statement: delete apimap;
      delete_expression: delete apimap
       delete: delete
       identifier: apimap
      ;: ;
     }: }
   function_definition: void TF_ApiDefMapPut(TF_ApiDefMap* api_def_map, const char* text,
                     size_t text_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
#else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_ApiDefMapPut(TF_ApiDefMap* api_def_map, const char* text,
                     size_t text_len, TF_Status* status)
     identifier: TF_ApiDefMapPut
     parameter_list: (TF_ApiDefMap* api_def_map, const char* text,
                     size_t text_len, TF_Status* status)
      (: (
      parameter_declaration: TF_ApiDefMap* api_def_map
       type_identifier: TF_ApiDefMap
       pointer_declarator: * api_def_map
        *: *
        identifier: api_def_map
      ,: ,
      parameter_declaration: const char* text
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * text
        *: *
        identifier: text
      ,: ,
      parameter_declaration: size_t text_len
       primitive_type: size_t
       identifier: text_len
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
#else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
#else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "ApiDefMap is not supported on mobile.")
          (: (
          string_literal: "ApiDefMap is not supported on mobile."
           ": "
           string_content: ApiDefMap is not supported on mobile.
           ": "
          ): )
       ;: ;
      preproc_else: #else
  mutex_lock l(api_def_map->lock);
  if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
  string api_def_text(text, text_len);
  status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
       #else: #else
       declaration: mutex_lock l(api_def_map->lock);
        type_identifier: mutex_lock
        init_declarator: l(api_def_map->lock)
         identifier: l
         argument_list: (api_def_map->lock)
          (: (
          field_expression: api_def_map->lock
           identifier: api_def_map
           ->: ->
           field_identifier: lock
          ): )
        ;: ;
       if_statement: if (api_def_map->update_docs_called) {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
        if: if
        condition_clause: (api_def_map->update_docs_called)
         (: (
         field_expression: api_def_map->update_docs_called
          identifier: api_def_map
          ->: ->
          field_identifier: update_docs_called
         ): )
        compound_statement: {
    status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
    return;
  }
         {: {
         expression_statement: status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.");
          assignment_expression: status->status = FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: FailedPrecondition(
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.")
            identifier: FailedPrecondition
            argument_list: (
        "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called.")
             (: (
             concatenated_string: "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
        "called."
              string_literal: "TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been "
               ": "
               string_content: TF_ApiDefMapPut cannot be called after TF_ApiDefMapGet has been 
               ": "
              string_literal: "called."
               ": "
               string_content: called.
               ": "
             ): )
          ;: ;
         return_statement: return;
          return: return
          ;: ;
         }: }
       declaration: string api_def_text(text, text_len);
        type_identifier: string
        function_declarator: api_def_text(text, text_len)
         identifier: api_def_text
         parameter_list: (text, text_len)
          (: (
          parameter_declaration: text
           type_identifier: text
          ,: ,
          parameter_declaration: text_len
           type_identifier: text_len
          ): )
        ;: ;
       expression_statement: status->status = api_def_map->api_def_map.LoadApiDef(api_def_text);
        assignment_expression: status->status = api_def_map->api_def_map.LoadApiDef(api_def_text)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: api_def_map->api_def_map.LoadApiDef(api_def_text)
          field_expression: api_def_map->api_def_map.LoadApiDef
           field_expression: api_def_map->api_def_map
            identifier: api_def_map
            ->: ->
            field_identifier: api_def_map
           .: .
           field_identifier: LoadApiDef
          argument_list: (api_def_text)
           (: (
           identifier: api_def_text
           ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: TF_Buffer* TF_ApiDefMapGet(TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
  return nullptr;
#else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    type_identifier: TF_Buffer
    pointer_declarator: * TF_ApiDefMapGet(TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status)
     *: *
     function_declarator: TF_ApiDefMapGet(TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status)
      identifier: TF_ApiDefMapGet
      parameter_list: (TF_ApiDefMap* api_def_map, const char* name,
                           size_t name_len, TF_Status* status)
       (: (
       parameter_declaration: TF_ApiDefMap* api_def_map
        type_identifier: TF_ApiDefMap
        pointer_declarator: * api_def_map
         *: *
         identifier: api_def_map
       ,: ,
       parameter_declaration: const char* name
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * name
         *: *
         identifier: name
       ,: ,
       parameter_declaration: size_t name_len
        primitive_type: size_t
        identifier: name_len
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
  return nullptr;
#else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
  return nullptr;
#else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "ApiDefMap is not supported on mobile.")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "ApiDefMap is not supported on mobile.")
          (: (
          string_literal: "ApiDefMap is not supported on mobile."
           ": "
           string_content: ApiDefMap is not supported on mobile.
           ": "
          ): )
       ;: ;
      return_statement: return nullptr;
       return: return
       null: nullptr
        nullptr: nullptr
       ;: ;
      preproc_else: #else
  mutex_lock l(api_def_map->lock);
  if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
  string name_str(name, name_len);
  const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
  if (api_def == nullptr) {
    return nullptr;
  }

  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(*api_def, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
       #else: #else
       declaration: mutex_lock l(api_def_map->lock);
        type_identifier: mutex_lock
        init_declarator: l(api_def_map->lock)
         identifier: l
         argument_list: (api_def_map->lock)
          (: (
          field_expression: api_def_map->lock
           identifier: api_def_map
           ->: ->
           field_identifier: lock
          ): )
        ;: ;
       if_statement: if (!api_def_map->update_docs_called) {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
        if: if
        condition_clause: (!api_def_map->update_docs_called)
         (: (
         unary_expression: !api_def_map->update_docs_called
          !: !
          field_expression: api_def_map->update_docs_called
           identifier: api_def_map
           ->: ->
           field_identifier: update_docs_called
         ): )
        compound_statement: {
    api_def_map->api_def_map.UpdateDocs();
    api_def_map->update_docs_called = true;
  }
         {: {
         expression_statement: api_def_map->api_def_map.UpdateDocs();
          call_expression: api_def_map->api_def_map.UpdateDocs()
           field_expression: api_def_map->api_def_map.UpdateDocs
            field_expression: api_def_map->api_def_map
             identifier: api_def_map
             ->: ->
             field_identifier: api_def_map
            .: .
            field_identifier: UpdateDocs
           argument_list: ()
            (: (
            ): )
          ;: ;
         expression_statement: api_def_map->update_docs_called = true;
          assignment_expression: api_def_map->update_docs_called = true
           field_expression: api_def_map->update_docs_called
            identifier: api_def_map
            ->: ->
            field_identifier: update_docs_called
           =: =
           true: true
          ;: ;
         }: }
       declaration: string name_str(name, name_len);
        type_identifier: string
        function_declarator: name_str(name, name_len)
         identifier: name_str
         parameter_list: (name, name_len)
          (: (
          parameter_declaration: name
           type_identifier: name
          ,: ,
          parameter_declaration: name_len
           type_identifier: name_len
          ): )
        ;: ;
       declaration: const auto* api_def = api_def_map->api_def_map.GetApiDef(name_str);
        type_qualifier: const
         const: const
        placeholder_type_specifier: auto
         auto: auto
        init_declarator: * api_def = api_def_map->api_def_map.GetApiDef(name_str)
         pointer_declarator: * api_def
          *: *
          identifier: api_def
         =: =
         call_expression: api_def_map->api_def_map.GetApiDef(name_str)
          field_expression: api_def_map->api_def_map.GetApiDef
           field_expression: api_def_map->api_def_map
            identifier: api_def_map
            ->: ->
            field_identifier: api_def_map
           .: .
           field_identifier: GetApiDef
          argument_list: (name_str)
           (: (
           identifier: name_str
           ): )
        ;: ;
       if_statement: if (api_def == nullptr) {
    return nullptr;
  }
        if: if
        condition_clause: (api_def == nullptr)
         (: (
         binary_expression: api_def == nullptr
          identifier: api_def
          ==: ==
          null: nullptr
           nullptr: nullptr
         ): )
        compound_statement: {
    return nullptr;
  }
         {: {
         return_statement: return nullptr;
          return: return
          null: nullptr
           nullptr: nullptr
          ;: ;
         }: }
       declaration: TF_Buffer* ret = TF_NewBuffer();
        type_identifier: TF_Buffer
        init_declarator: * ret = TF_NewBuffer()
         pointer_declarator: * ret
          *: *
          identifier: ret
         =: =
         call_expression: TF_NewBuffer()
          identifier: TF_NewBuffer
          argument_list: ()
           (: (
           ): )
        ;: ;
       expression_statement: status->status = MessageToBuffer(*api_def, ret);
        assignment_expression: status->status = MessageToBuffer(*api_def, ret)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: MessageToBuffer(*api_def, ret)
          identifier: MessageToBuffer
          argument_list: (*api_def, ret)
           (: (
           pointer_expression: *api_def
            *: *
            identifier: api_def
           ,: ,
           identifier: ret
           ): )
        ;: ;
       if_statement: if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        compound_statement: {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
         {: {
         expression_statement: TF_DeleteBuffer(ret);
          call_expression: TF_DeleteBuffer(ret)
           identifier: TF_DeleteBuffer
           argument_list: (ret)
            (: (
            identifier: ret
            ): )
          ;: ;
         return_statement: return nullptr;
          return: return
          null: nullptr
           nullptr: nullptr
          ;: ;
         }: }
       return_statement: return ret;
        return: return
        identifier: ret
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: TF_Buffer* TF_GetAllRegisteredKernels(TF_Status* status) {
  tensorflow::KernelList kernel_list = tensorflow::GetAllRegisteredKernels();
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}
    type_identifier: TF_Buffer
    pointer_declarator: * TF_GetAllRegisteredKernels(TF_Status* status)
     *: *
     function_declarator: TF_GetAllRegisteredKernels(TF_Status* status)
      identifier: TF_GetAllRegisteredKernels
      parameter_list: (TF_Status* status)
       (: (
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  tensorflow::KernelList kernel_list = tensorflow::GetAllRegisteredKernels();
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}
     {: {
     declaration: tensorflow::KernelList kernel_list = tensorflow::GetAllRegisteredKernels();
      qualified_identifier: tensorflow::KernelList
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: KernelList
      init_declarator: kernel_list = tensorflow::GetAllRegisteredKernels()
       identifier: kernel_list
       =: =
       call_expression: tensorflow::GetAllRegisteredKernels()
        qualified_identifier: tensorflow::GetAllRegisteredKernels
         namespace_identifier: tensorflow
         ::: ::
         identifier: GetAllRegisteredKernels
        argument_list: ()
         (: (
         ): )
      ;: ;
     declaration: TF_Buffer* ret = TF_NewBuffer();
      type_identifier: TF_Buffer
      init_declarator: * ret = TF_NewBuffer()
       pointer_declarator: * ret
        *: *
        identifier: ret
       =: =
       call_expression: TF_NewBuffer()
        identifier: TF_NewBuffer
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: status->status = MessageToBuffer(kernel_list, ret);
      assignment_expression: status->status = MessageToBuffer(kernel_list, ret)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(kernel_list, ret)
        identifier: MessageToBuffer
        argument_list: (kernel_list, ret)
         (: (
         identifier: kernel_list
         ,: ,
         identifier: ret
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
       {: {
       expression_statement: TF_DeleteBuffer(ret);
        call_expression: TF_DeleteBuffer(ret)
         identifier: TF_DeleteBuffer
         argument_list: (ret)
          (: (
          identifier: ret
          ): )
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     return_statement: return ret;
      return: return
      identifier: ret
      ;: ;
     }: }
   function_definition: TF_Buffer* TF_GetRegisteredKernelsForOp(const char* name, TF_Status* status) {
  tensorflow::KernelList kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name);
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}
    type_identifier: TF_Buffer
    pointer_declarator: * TF_GetRegisteredKernelsForOp(const char* name, TF_Status* status)
     *: *
     function_declarator: TF_GetRegisteredKernelsForOp(const char* name, TF_Status* status)
      identifier: TF_GetRegisteredKernelsForOp
      parameter_list: (const char* name, TF_Status* status)
       (: (
       parameter_declaration: const char* name
        type_qualifier: const
         const: const
        primitive_type: char
        pointer_declarator: * name
         *: *
         identifier: name
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
  tensorflow::KernelList kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name);
  TF_Buffer* ret = TF_NewBuffer();
  status->status = MessageToBuffer(kernel_list, ret);
  if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
  return ret;
}
     {: {
     declaration: tensorflow::KernelList kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name);
      qualified_identifier: tensorflow::KernelList
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: KernelList
      init_declarator: kernel_list =
      tensorflow::GetRegisteredKernelsForOp(name)
       identifier: kernel_list
       =: =
       call_expression: tensorflow::GetRegisteredKernelsForOp(name)
        qualified_identifier: tensorflow::GetRegisteredKernelsForOp
         namespace_identifier: tensorflow
         ::: ::
         identifier: GetRegisteredKernelsForOp
        argument_list: (name)
         (: (
         identifier: name
         ): )
      ;: ;
     declaration: TF_Buffer* ret = TF_NewBuffer();
      type_identifier: TF_Buffer
      init_declarator: * ret = TF_NewBuffer()
       pointer_declarator: * ret
        *: *
        identifier: ret
       =: =
       call_expression: TF_NewBuffer()
        identifier: TF_NewBuffer
        argument_list: ()
         (: (
         ): )
      ;: ;
     expression_statement: status->status = MessageToBuffer(kernel_list, ret);
      assignment_expression: status->status = MessageToBuffer(kernel_list, ret)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: MessageToBuffer(kernel_list, ret)
        identifier: MessageToBuffer
        argument_list: (kernel_list, ret)
         (: (
         identifier: kernel_list
         ,: ,
         identifier: ret
         ): )
      ;: ;
     if_statement: if (!status->status.ok()) {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
      if: if
      condition_clause: (!status->status.ok())
       (: (
       unary_expression: !status->status.ok()
        !: !
        call_expression: status->status.ok()
         field_expression: status->status.ok
          field_expression: status->status
           identifier: status
           ->: ->
           field_identifier: status
          .: .
          field_identifier: ok
         argument_list: ()
          (: (
          ): )
       ): )
      compound_statement: {
    TF_DeleteBuffer(ret);
    return nullptr;
  }
       {: {
       expression_statement: TF_DeleteBuffer(ret);
        call_expression: TF_DeleteBuffer(ret)
         identifier: TF_DeleteBuffer
         argument_list: (ret)
          (: (
          identifier: ret
          ): )
        ;: ;
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
       }: }
     return_statement: return ret;
      return: return
      identifier: ret
      ;: ;
     }: }
   function_definition: void TF_UpdateEdge(TF_Graph* graph, TF_Output new_src, TF_Input dst,
                   TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&new_src.oper->node);

  if (ic->num_outputs() <= new_src.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
  tensorflow::shape_inference::ShapeHandle shape = ic->output(new_src.index);

  tensorflow::shape_inference::InferenceContext* ic_dst =
      graph->refiner.GetContext(&dst.oper->node);
  if (ic_dst->num_inputs() <= dst.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
  if (!ic_dst->MergeInput(dst.index, shape)) {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
  status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index);

  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
}
    primitive_type: void
    function_declarator: TF_UpdateEdge(TF_Graph* graph, TF_Output new_src, TF_Input dst,
                   TF_Status* status)
     identifier: TF_UpdateEdge
     parameter_list: (TF_Graph* graph, TF_Output new_src, TF_Input dst,
                   TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Output new_src
       type_identifier: TF_Output
       identifier: new_src
      ,: ,
      parameter_declaration: TF_Input dst
       type_identifier: TF_Input
       identifier: dst
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&new_src.oper->node);

  if (ic->num_outputs() <= new_src.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
  tensorflow::shape_inference::ShapeHandle shape = ic->output(new_src.index);

  tensorflow::shape_inference::InferenceContext* ic_dst =
      graph->refiner.GetContext(&dst.oper->node);
  if (ic_dst->num_inputs() <= dst.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
  if (!ic_dst->MergeInput(dst.index, shape)) {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
  status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index);

  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&new_src.oper->node);
      qualified_identifier: tensorflow::shape_inference::InferenceContext
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: shape_inference::InferenceContext
        namespace_identifier: shape_inference
        ::: ::
        type_identifier: InferenceContext
      init_declarator: * ic =
      graph->refiner.GetContext(&new_src.oper->node)
       pointer_declarator: * ic
        *: *
        identifier: ic
       =: =
       call_expression: graph->refiner.GetContext(&new_src.oper->node)
        field_expression: graph->refiner.GetContext
         field_expression: graph->refiner
          identifier: graph
          ->: ->
          field_identifier: refiner
         .: .
         field_identifier: GetContext
        argument_list: (&new_src.oper->node)
         (: (
         pointer_expression: &new_src.oper->node
          &: &
          field_expression: new_src.oper->node
           field_expression: new_src.oper
            identifier: new_src
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
         ): )
      ;: ;
     if_statement: if (ic->num_outputs() <= new_src.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
      if: if
      condition_clause: (ic->num_outputs() <= new_src.index)
       (: (
       binary_expression: ic->num_outputs() <= new_src.index
        call_expression: ic->num_outputs()
         field_expression: ic->num_outputs
          identifier: ic
          ->: ->
          field_identifier: num_outputs
         argument_list: ()
          (: (
          ): )
        <=: <=
        field_expression: new_src.index
         identifier: new_src
         .: .
         field_identifier: index
       ): )
      compound_statement: {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
    return;
  }
       {: {
       expression_statement: status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].");
        assignment_expression: status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: tensorflow::errors::OutOfRange(
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].")
          qualified_identifier: tensorflow::errors::OutOfRange
           namespace_identifier: tensorflow
           ::: ::
           qualified_identifier: errors::OutOfRange
            namespace_identifier: errors
            ::: ::
            identifier: OutOfRange
          argument_list: (
        "Cannot update edge. Output index [", new_src.index,
        "] is greater than the number of total outputs [", ic->num_outputs(),
        "].")
           (: (
           string_literal: "Cannot update edge. Output index ["
            ": "
            string_content: Cannot update edge. Output index [
            ": "
           ,: ,
           field_expression: new_src.index
            identifier: new_src
            .: .
            field_identifier: index
           ,: ,
           string_literal: "] is greater than the number of total outputs ["
            ": "
            string_content: ] is greater than the number of total outputs [
            ": "
           ,: ,
           call_expression: ic->num_outputs()
            field_expression: ic->num_outputs
             identifier: ic
             ->: ->
             field_identifier: num_outputs
            argument_list: ()
             (: (
             ): )
           ,: ,
           string_literal: "]."
            ": "
            string_content: ].
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: tensorflow::shape_inference::ShapeHandle shape = ic->output(new_src.index);
      qualified_identifier: tensorflow::shape_inference::ShapeHandle
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: shape_inference::ShapeHandle
        namespace_identifier: shape_inference
        ::: ::
        type_identifier: ShapeHandle
      init_declarator: shape = ic->output(new_src.index)
       identifier: shape
       =: =
       call_expression: ic->output(new_src.index)
        field_expression: ic->output
         identifier: ic
         ->: ->
         field_identifier: output
        argument_list: (new_src.index)
         (: (
         field_expression: new_src.index
          identifier: new_src
          .: .
          field_identifier: index
         ): )
      ;: ;
     declaration: tensorflow::shape_inference::InferenceContext* ic_dst =
      graph->refiner.GetContext(&dst.oper->node);
      qualified_identifier: tensorflow::shape_inference::InferenceContext
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: shape_inference::InferenceContext
        namespace_identifier: shape_inference
        ::: ::
        type_identifier: InferenceContext
      init_declarator: * ic_dst =
      graph->refiner.GetContext(&dst.oper->node)
       pointer_declarator: * ic_dst
        *: *
        identifier: ic_dst
       =: =
       call_expression: graph->refiner.GetContext(&dst.oper->node)
        field_expression: graph->refiner.GetContext
         field_expression: graph->refiner
          identifier: graph
          ->: ->
          field_identifier: refiner
         .: .
         field_identifier: GetContext
        argument_list: (&dst.oper->node)
         (: (
         pointer_expression: &dst.oper->node
          &: &
          field_expression: dst.oper->node
           field_expression: dst.oper
            identifier: dst
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
         ): )
      ;: ;
     if_statement: if (ic_dst->num_inputs() <= dst.index) {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
      if: if
      condition_clause: (ic_dst->num_inputs() <= dst.index)
       (: (
       binary_expression: ic_dst->num_inputs() <= dst.index
        call_expression: ic_dst->num_inputs()
         field_expression: ic_dst->num_inputs
          identifier: ic_dst
          ->: ->
          field_identifier: num_inputs
         argument_list: ()
          (: (
          ): )
        <=: <=
        field_expression: dst.index
         identifier: dst
         .: .
         field_identifier: index
       ): )
      compound_statement: {
    status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
    return;
  }
       {: {
       expression_statement: status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].");
        assignment_expression: status->status = tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: tensorflow::errors::OutOfRange(
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].")
          qualified_identifier: tensorflow::errors::OutOfRange
           namespace_identifier: tensorflow
           ::: ::
           qualified_identifier: errors::OutOfRange
            namespace_identifier: errors
            ::: ::
            identifier: OutOfRange
          argument_list: (
        "Cannot update edge. Input index [", dst.index,
        "] is greater than the number of total inputs [", ic_dst->num_inputs(),
        "].")
           (: (
           string_literal: "Cannot update edge. Input index ["
            ": "
            string_content: Cannot update edge. Input index [
            ": "
           ,: ,
           field_expression: dst.index
            identifier: dst
            .: .
            field_identifier: index
           ,: ,
           string_literal: "] is greater than the number of total inputs ["
            ": "
            string_content: ] is greater than the number of total inputs [
            ": "
           ,: ,
           call_expression: ic_dst->num_inputs()
            field_expression: ic_dst->num_inputs
             identifier: ic_dst
             ->: ->
             field_identifier: num_inputs
            argument_list: ()
             (: (
             ): )
           ,: ,
           string_literal: "]."
            ": "
            string_content: ].
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     if_statement: if (!ic_dst->MergeInput(dst.index, shape)) {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
      if: if
      condition_clause: (!ic_dst->MergeInput(dst.index, shape))
       (: (
       unary_expression: !ic_dst->MergeInput(dst.index, shape)
        !: !
        call_expression: ic_dst->MergeInput(dst.index, shape)
         field_expression: ic_dst->MergeInput
          identifier: ic_dst
          ->: ->
          field_identifier: MergeInput
         argument_list: (dst.index, shape)
          (: (
          field_expression: dst.index
           identifier: dst
           .: .
           field_identifier: index
          ,: ,
          identifier: shape
          ): )
       ): )
      compound_statement: {
    status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
    return;
  }
       {: {
       expression_statement: status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".");
        assignment_expression: status->status = tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: tensorflow::errors::InvalidArgument(
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".")
          qualified_identifier: tensorflow::errors::InvalidArgument
           namespace_identifier: tensorflow
           ::: ::
           qualified_identifier: errors::InvalidArgument
            namespace_identifier: errors
            ::: ::
            identifier: InvalidArgument
          argument_list: (
        "Cannot update edge, incompatible shapes: ", ic_dst->DebugString(shape),
        " and ", ic_dst->DebugString(ic_dst->input(dst.index)), ".")
           (: (
           string_literal: "Cannot update edge, incompatible shapes: "
            ": "
            string_content: Cannot update edge, incompatible shapes: 
            ": "
           ,: ,
           call_expression: ic_dst->DebugString(shape)
            field_expression: ic_dst->DebugString
             identifier: ic_dst
             ->: ->
             field_identifier: DebugString
            argument_list: (shape)
             (: (
             identifier: shape
             ): )
           ,: ,
           string_literal: " and "
            ": "
            string_content:  and 
            ": "
           ,: ,
           call_expression: ic_dst->DebugString(ic_dst->input(dst.index))
            field_expression: ic_dst->DebugString
             identifier: ic_dst
             ->: ->
             field_identifier: DebugString
            argument_list: (ic_dst->input(dst.index))
             (: (
             call_expression: ic_dst->input(dst.index)
              field_expression: ic_dst->input
               identifier: ic_dst
               ->: ->
               field_identifier: input
              argument_list: (dst.index)
               (: (
               field_expression: dst.index
                identifier: dst
                .: .
                field_identifier: index
               ): )
             ): )
           ,: ,
           string_literal: "."
            ": "
            string_content: .
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index);
      assignment_expression: status->status = graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: graph->graph.UpdateEdge(&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index)
        field_expression: graph->graph.UpdateEdge
         field_expression: graph->graph
          identifier: graph
          ->: ->
          field_identifier: graph
         .: .
         field_identifier: UpdateEdge
        argument_list: (&new_src.oper->node, new_src.index,
                                           &dst.oper->node, dst.index)
         (: (
         pointer_expression: &new_src.oper->node
          &: &
          field_expression: new_src.oper->node
           field_expression: new_src.oper
            identifier: new_src
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
         ,: ,
         field_expression: new_src.index
          identifier: new_src
          .: .
          field_identifier: index
         ,: ,
         pointer_expression: &dst.oper->node
          &: &
          field_expression: dst.oper->node
           field_expression: dst.oper
            identifier: dst
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
         ,: ,
         field_expression: dst.index
          identifier: dst
          .: .
          field_identifier: index
         ): )
      ;: ;
     if_statement: if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
      if: if
      condition_clause: (TF_GetCode(status) == TF_OK)
       (: (
       binary_expression: TF_GetCode(status) == TF_OK
        call_expression: TF_GetCode(status)
         identifier: TF_GetCode
         argument_list: (status)
          (: (
          identifier: status
          ): )
        ==: ==
        identifier: TF_OK
       ): )
      compound_statement: {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst.oper, "updating input tensor");
  }
       {: {
       comment: // This modification only updates the destination node for
       comment: // the purposes of running this graph in a session. Thus, we don't
       comment: // record the source node as being modified.
       expression_statement: RecordMutation(graph, *dst.oper, "updating input tensor");
        call_expression: RecordMutation(graph, *dst.oper, "updating input tensor")
         identifier: RecordMutation
         argument_list: (graph, *dst.oper, "updating input tensor")
          (: (
          identifier: graph
          ,: ,
          pointer_expression: *dst.oper
           *: *
           field_expression: dst.oper
            identifier: dst
            .: .
            field_identifier: oper
          ,: ,
          string_literal: "updating input tensor"
           ": "
           string_content: updating input tensor
           ": "
          ): )
        ;: ;
       }: }
     }: }
   comment: // Apis that are corresponding to python c api. --------------------------
   function_definition: void TF_AddOperationControlInput(TF_Graph* graph, TF_Operation* op,
                                 TF_Operation* input) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  graph->graph.AddControlEdge(&input->node, &op->node);
  RecordMutation(graph, *op, "adding control input");
}
    primitive_type: void
    function_declarator: TF_AddOperationControlInput(TF_Graph* graph, TF_Operation* op,
                                 TF_Operation* input)
     identifier: TF_AddOperationControlInput
     parameter_list: (TF_Graph* graph, TF_Operation* op,
                                 TF_Operation* input)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ,: ,
      parameter_declaration: TF_Operation* input
       type_identifier: TF_Operation
       pointer_declarator: * input
        *: *
        identifier: input
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  graph->graph.AddControlEdge(&input->node, &op->node);
  RecordMutation(graph, *op, "adding control input");
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: graph->graph.AddControlEdge(&input->node, &op->node);
      call_expression: graph->graph.AddControlEdge(&input->node, &op->node)
       field_expression: graph->graph.AddControlEdge
        field_expression: graph->graph
         identifier: graph
         ->: ->
         field_identifier: graph
        .: .
        field_identifier: AddControlEdge
       argument_list: (&input->node, &op->node)
        (: (
        pointer_expression: &input->node
         &: &
         field_expression: input->node
          identifier: input
          ->: ->
          field_identifier: node
        ,: ,
        pointer_expression: &op->node
         &: &
         field_expression: op->node
          identifier: op
          ->: ->
          field_identifier: node
        ): )
      ;: ;
     expression_statement: RecordMutation(graph, *op, "adding control input");
      call_expression: RecordMutation(graph, *op, "adding control input")
       identifier: RecordMutation
       argument_list: (graph, *op, "adding control input")
        (: (
        identifier: graph
        ,: ,
        pointer_expression: *op
         *: *
         identifier: op
        ,: ,
        string_literal: "adding control input"
         ": "
         string_content: adding control input
         ": "
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                TF_Buffer* attr_value_proto, TF_Status* status) {
  using tensorflow::RecordMutation;
  tensorflow::AttrValue attr_val;
  if (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)) {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }

  mutex_lock l(graph->mu);
  op->node.AddAttr(attr_name, attr_val);
  RecordMutation(graph, *op, "setting attribute");
}
    primitive_type: void
    function_declarator: TF_SetAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                TF_Buffer* attr_value_proto, TF_Status* status)
     identifier: TF_SetAttr
     parameter_list: (TF_Graph* graph, TF_Operation* op, const char* attr_name,
                TF_Buffer* attr_value_proto, TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Buffer* attr_value_proto
       type_identifier: TF_Buffer
       pointer_declarator: * attr_value_proto
        *: *
        identifier: attr_value_proto
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  tensorflow::AttrValue attr_val;
  if (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)) {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }

  mutex_lock l(graph->mu);
  op->node.AddAttr(attr_name, attr_val);
  RecordMutation(graph, *op, "setting attribute");
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: tensorflow::AttrValue attr_val;
      qualified_identifier: tensorflow::AttrValue
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: AttrValue
      identifier: attr_val
      ;: ;
     if_statement: if (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)) {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }
      if: if
      condition_clause: (!attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length))
       (: (
       unary_expression: !attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)
        !: !
        call_expression: attr_val.ParseFromArray(attr_value_proto->data,
                               attr_value_proto->length)
         field_expression: attr_val.ParseFromArray
          identifier: attr_val
          .: .
          field_identifier: ParseFromArray
         argument_list: (attr_value_proto->data,
                               attr_value_proto->length)
          (: (
          field_expression: attr_value_proto->data
           identifier: attr_value_proto
           ->: ->
           field_identifier: data
          ,: ,
          field_expression: attr_value_proto->length
           identifier: attr_value_proto
           ->: ->
           field_identifier: length
          ): )
       ): )
      compound_statement: {
    status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
    return;
  }
       {: {
       expression_statement: status->status = absl::InvalidArgumentError("Invalid AttrValue proto");
        assignment_expression: status->status = absl::InvalidArgumentError("Invalid AttrValue proto")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: absl::InvalidArgumentError("Invalid AttrValue proto")
          qualified_identifier: absl::InvalidArgumentError
           namespace_identifier: absl
           ::: ::
           identifier: InvalidArgumentError
          argument_list: ("Invalid AttrValue proto")
           (: (
           string_literal: "Invalid AttrValue proto"
            ": "
            string_content: Invalid AttrValue proto
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: op->node.AddAttr(attr_name, attr_val);
      call_expression: op->node.AddAttr(attr_name, attr_val)
       field_expression: op->node.AddAttr
        field_expression: op->node
         identifier: op
         ->: ->
         field_identifier: node
        .: .
        field_identifier: AddAttr
       argument_list: (attr_name, attr_val)
        (: (
        identifier: attr_name
        ,: ,
        identifier: attr_val
        ): )
      ;: ;
     expression_statement: RecordMutation(graph, *op, "setting attribute");
      call_expression: RecordMutation(graph, *op, "setting attribute")
       identifier: RecordMutation
       argument_list: (graph, *op, "setting attribute")
        (: (
        identifier: graph
        ,: ,
        pointer_expression: *op
         *: *
         identifier: op
        ,: ,
        string_literal: "setting attribute"
         ": "
         string_content: setting attribute
         ": "
        ): )
      ;: ;
     }: }
   function_definition: void TF_ClearAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                  TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.ClearAttr(attr_name);
  RecordMutation(graph, *op, "clearing attribute");
}
    primitive_type: void
    function_declarator: TF_ClearAttr(TF_Graph* graph, TF_Operation* op, const char* attr_name,
                  TF_Status* status)
     identifier: TF_ClearAttr
     parameter_list: (TF_Graph* graph, TF_Operation* op, const char* attr_name,
                  TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ,: ,
      parameter_declaration: const char* attr_name
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * attr_name
        *: *
        identifier: attr_name
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.ClearAttr(attr_name);
  RecordMutation(graph, *op, "clearing attribute");
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: op->node.ClearAttr(attr_name);
      call_expression: op->node.ClearAttr(attr_name)
       field_expression: op->node.ClearAttr
        field_expression: op->node
         identifier: op
         ->: ->
         field_identifier: node
        .: .
        field_identifier: ClearAttr
       argument_list: (attr_name)
        (: (
        identifier: attr_name
        ): )
      ;: ;
     expression_statement: RecordMutation(graph, *op, "clearing attribute");
      call_expression: RecordMutation(graph, *op, "clearing attribute")
       identifier: RecordMutation
       argument_list: (graph, *op, "clearing attribute")
        (: (
        identifier: graph
        ,: ,
        pointer_expression: *op
         *: *
         identifier: op
        ,: ,
        string_literal: "clearing attribute"
         ": "
         string_content: clearing attribute
         ": "
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetFullType(TF_Graph* graph, TF_Operation* op,
                    const TF_Buffer* full_type_proto) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  FullTypeDef full_type;
  full_type.ParseFromArray(full_type_proto->data, full_type_proto->length);
  *op->node.mutable_def()->mutable_experimental_type() = full_type;
  RecordMutation(graph, *op, "setting fulltype");
}
    primitive_type: void
    function_declarator: TF_SetFullType(TF_Graph* graph, TF_Operation* op,
                    const TF_Buffer* full_type_proto)
     identifier: TF_SetFullType
     parameter_list: (TF_Graph* graph, TF_Operation* op,
                    const TF_Buffer* full_type_proto)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ,: ,
      parameter_declaration: const TF_Buffer* full_type_proto
       type_qualifier: const
        const: const
       type_identifier: TF_Buffer
       pointer_declarator: * full_type_proto
        *: *
        identifier: full_type_proto
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  FullTypeDef full_type;
  full_type.ParseFromArray(full_type_proto->data, full_type_proto->length);
  *op->node.mutable_def()->mutable_experimental_type() = full_type;
  RecordMutation(graph, *op, "setting fulltype");
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: FullTypeDef full_type;
      type_identifier: FullTypeDef
      identifier: full_type
      ;: ;
     expression_statement: full_type.ParseFromArray(full_type_proto->data, full_type_proto->length);
      call_expression: full_type.ParseFromArray(full_type_proto->data, full_type_proto->length)
       field_expression: full_type.ParseFromArray
        identifier: full_type
        .: .
        field_identifier: ParseFromArray
       argument_list: (full_type_proto->data, full_type_proto->length)
        (: (
        field_expression: full_type_proto->data
         identifier: full_type_proto
         ->: ->
         field_identifier: data
        ,: ,
        field_expression: full_type_proto->length
         identifier: full_type_proto
         ->: ->
         field_identifier: length
        ): )
      ;: ;
     expression_statement: *op->node.mutable_def()->mutable_experimental_type() = full_type;
      assignment_expression: *op->node.mutable_def()->mutable_experimental_type() = full_type
       pointer_expression: *op->node.mutable_def()->mutable_experimental_type()
        *: *
        call_expression: op->node.mutable_def()->mutable_experimental_type()
         field_expression: op->node.mutable_def()->mutable_experimental_type
          call_expression: op->node.mutable_def()
           field_expression: op->node.mutable_def
            field_expression: op->node
             identifier: op
             ->: ->
             field_identifier: node
            .: .
            field_identifier: mutable_def
           argument_list: ()
            (: (
            ): )
          ->: ->
          field_identifier: mutable_experimental_type
         argument_list: ()
          (: (
          ): )
       =: =
       identifier: full_type
      ;: ;
     expression_statement: RecordMutation(graph, *op, "setting fulltype");
      call_expression: RecordMutation(graph, *op, "setting fulltype")
       identifier: RecordMutation
       argument_list: (graph, *op, "setting fulltype")
        (: (
        identifier: graph
        ,: ,
        pointer_expression: *op
         *: *
         identifier: op
        ,: ,
        string_literal: "setting fulltype"
         ": "
         string_content: setting fulltype
         ": "
        ): )
      ;: ;
     }: }
   function_definition: void TF_SetRequestedDevice(TF_Graph* graph, TF_Operation* op,
                           const char* device) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.set_requested_device(device);
  RecordMutation(graph, *op, "setting device");
}
    primitive_type: void
    function_declarator: TF_SetRequestedDevice(TF_Graph* graph, TF_Operation* op,
                           const char* device)
     identifier: TF_SetRequestedDevice
     parameter_list: (TF_Graph* graph, TF_Operation* op,
                           const char* device)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ,: ,
      parameter_declaration: const char* device
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * device
        *: *
        identifier: device
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  op->node.set_requested_device(device);
  RecordMutation(graph, *op, "setting device");
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: op->node.set_requested_device(device);
      call_expression: op->node.set_requested_device(device)
       field_expression: op->node.set_requested_device
        field_expression: op->node
         identifier: op
         ->: ->
         field_identifier: node
        .: .
        field_identifier: set_requested_device
       argument_list: (device)
        (: (
        identifier: device
        ): )
      ;: ;
     expression_statement: RecordMutation(graph, *op, "setting device");
      call_expression: RecordMutation(graph, *op, "setting device")
       identifier: RecordMutation
       argument_list: (graph, *op, "setting device")
        (: (
        identifier: graph
        ,: ,
        pointer_expression: *op
         *: *
         identifier: op
        ,: ,
        string_literal: "setting device"
         ": "
         string_content: setting device
         ": "
        ): )
      ;: ;
     }: }
   function_definition: void TF_RemoveAllControlInputs(TF_Graph* graph, TF_Operation* op) {
  mutex_lock l(graph->mu);
  std::vector<const tensorflow::Edge*> control_edges;
  for (const tensorflow::Edge* edge : op->node.in_edges()) {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
  for (const tensorflow::Edge* edge : control_edges) {
    graph->graph.RemoveControlEdge(edge);
  }
}
    primitive_type: void
    function_declarator: TF_RemoveAllControlInputs(TF_Graph* graph, TF_Operation* op)
     identifier: TF_RemoveAllControlInputs
     parameter_list: (TF_Graph* graph, TF_Operation* op)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Operation* op
       type_identifier: TF_Operation
       pointer_declarator: * op
        *: *
        identifier: op
      ): )
    compound_statement: {
  mutex_lock l(graph->mu);
  std::vector<const tensorflow::Edge*> control_edges;
  for (const tensorflow::Edge* edge : op->node.in_edges()) {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
  for (const tensorflow::Edge* edge : control_edges) {
    graph->graph.RemoveControlEdge(edge);
  }
}
     {: {
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: std::vector<const tensorflow::Edge*> control_edges;
      qualified_identifier: std::vector<const tensorflow::Edge*>
       namespace_identifier: std
       ::: ::
       template_type: vector<const tensorflow::Edge*>
        type_identifier: vector
        template_argument_list: <const tensorflow::Edge*>
         <: <
         type_descriptor: const tensorflow::Edge*
          type_qualifier: const
           const: const
          qualified_identifier: tensorflow::Edge
           namespace_identifier: tensorflow
           ::: ::
           type_identifier: Edge
          abstract_pointer_declarator: *
           *: *
         >: >
      identifier: control_edges
      ;: ;
     for_range_loop: for (const tensorflow::Edge* edge : op->node.in_edges()) {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      qualified_identifier: tensorflow::Edge
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: Edge
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      call_expression: op->node.in_edges()
       field_expression: op->node.in_edges
        field_expression: op->node
         identifier: op
         ->: ->
         field_identifier: node
        .: .
        field_identifier: in_edges
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    if (!edge->IsControlEdge()) continue;
    control_edges.push_back(edge);
  }
       {: {
       if_statement: if (!edge->IsControlEdge()) continue;
        if: if
        condition_clause: (!edge->IsControlEdge())
         (: (
         unary_expression: !edge->IsControlEdge()
          !: !
          call_expression: edge->IsControlEdge()
           field_expression: edge->IsControlEdge
            identifier: edge
            ->: ->
            field_identifier: IsControlEdge
           argument_list: ()
            (: (
            ): )
         ): )
        continue_statement: continue;
         continue: continue
         ;: ;
       expression_statement: control_edges.push_back(edge);
        call_expression: control_edges.push_back(edge)
         field_expression: control_edges.push_back
          identifier: control_edges
          .: .
          field_identifier: push_back
         argument_list: (edge)
          (: (
          identifier: edge
          ): )
        ;: ;
       }: }
     for_range_loop: for (const tensorflow::Edge* edge : control_edges) {
    graph->graph.RemoveControlEdge(edge);
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      qualified_identifier: tensorflow::Edge
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: Edge
      pointer_declarator: * edge
       *: *
       identifier: edge
      :: :
      identifier: control_edges
      ): )
      compound_statement: {
    graph->graph.RemoveControlEdge(edge);
  }
       {: {
       expression_statement: graph->graph.RemoveControlEdge(edge);
        call_expression: graph->graph.RemoveControlEdge(edge)
         field_expression: graph->graph.RemoveControlEdge
          field_expression: graph->graph
           identifier: graph
           ->: ->
           field_identifier: graph
          .: .
          field_identifier: RemoveControlEdge
         argument_list: (edge)
          (: (
          identifier: edge
          ): )
        ;: ;
       }: }
     }: }
   function_definition: void TF_SetRequireShapeInferenceFns(TF_Graph* graph, bool require) {
  mutex_lock l(graph->mu);
  graph->refiner.set_require_shape_inference_fns(require);
}
    primitive_type: void
    function_declarator: TF_SetRequireShapeInferenceFns(TF_Graph* graph, bool require)
     identifier: TF_SetRequireShapeInferenceFns
     parameter_list: (TF_Graph* graph, bool require)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: bool require
       primitive_type: bool
       identifier: require
      ): )
    compound_statement: {
  mutex_lock l(graph->mu);
  graph->refiner.set_require_shape_inference_fns(require);
}
     {: {
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: graph->refiner.set_require_shape_inference_fns(require);
      call_expression: graph->refiner.set_require_shape_inference_fns(require)
       field_expression: graph->refiner.set_require_shape_inference_fns
        field_expression: graph->refiner
         identifier: graph
         ->: ->
         field_identifier: refiner
        .: .
        field_identifier: set_require_shape_inference_fns
       argument_list: (require)
        (: (
        identifier: require
        ): )
      ;: ;
     }: }
   function_definition: void TF_ExtendSession(TF_Session* session, TF_Status* status) {
  ExtendSessionGraphHelper(session, status);
  session->extend_before_run = false;
}
    primitive_type: void
    function_declarator: TF_ExtendSession(TF_Session* session, TF_Status* status)
     identifier: TF_ExtendSession
     parameter_list: (TF_Session* session, TF_Status* status)
      (: (
      parameter_declaration: TF_Session* session
       type_identifier: TF_Session
       pointer_declarator: * session
        *: *
        identifier: session
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  ExtendSessionGraphHelper(session, status);
  session->extend_before_run = false;
}
     {: {
     expression_statement: ExtendSessionGraphHelper(session, status);
      call_expression: ExtendSessionGraphHelper(session, status)
       identifier: ExtendSessionGraphHelper
       argument_list: (session, status)
        (: (
        identifier: session
        ,: ,
        identifier: status
        ): )
      ;: ;
     expression_statement: session->extend_before_run = false;
      assignment_expression: session->extend_before_run = false
       field_expression: session->extend_before_run
        identifier: session
        ->: ->
        field_identifier: extend_before_run
       =: =
       false: false
      ;: ;
     }: }
   function_definition: TF_Buffer* TF_GetHandleShapeAndType(TF_Graph* graph, TF_Output output) {
  Node* node = &output.oper->node;
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  handle_data.set_is_set(true);
  {
    mutex_lock l(graph->mu);
    tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
    CHECK(ic != nullptr);                       // Crash OK
    CHECK_LT(output.index, ic->num_outputs());  // Crash OK
    const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
    if (shapes_and_types == nullptr) return nullptr;

    for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
  }
  string str_data;
  handle_data.SerializeToString(&str_data);

  TF_Buffer* result = TF_NewBufferFromString(str_data.c_str(), str_data.size());
  return result;
}
    type_identifier: TF_Buffer
    pointer_declarator: * TF_GetHandleShapeAndType(TF_Graph* graph, TF_Output output)
     *: *
     function_declarator: TF_GetHandleShapeAndType(TF_Graph* graph, TF_Output output)
      identifier: TF_GetHandleShapeAndType
      parameter_list: (TF_Graph* graph, TF_Output output)
       (: (
       parameter_declaration: TF_Graph* graph
        type_identifier: TF_Graph
        pointer_declarator: * graph
         *: *
         identifier: graph
       ,: ,
       parameter_declaration: TF_Output output
        type_identifier: TF_Output
        identifier: output
       ): )
    compound_statement: {
  Node* node = &output.oper->node;
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  handle_data.set_is_set(true);
  {
    mutex_lock l(graph->mu);
    tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
    CHECK(ic != nullptr);                       // Crash OK
    CHECK_LT(output.index, ic->num_outputs());  // Crash OK
    const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
    if (shapes_and_types == nullptr) return nullptr;

    for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
  }
  string str_data;
  handle_data.SerializeToString(&str_data);

  TF_Buffer* result = TF_NewBufferFromString(str_data.c_str(), str_data.size());
  return result;
}
     {: {
     declaration: Node* node = &output.oper->node;
      type_identifier: Node
      init_declarator: * node = &output.oper->node
       pointer_declarator: * node
        *: *
        identifier: node
       =: =
       pointer_expression: &output.oper->node
        &: &
        field_expression: output.oper->node
         field_expression: output.oper
          identifier: output
          .: .
          field_identifier: oper
         ->: ->
         field_identifier: node
      ;: ;
     declaration: tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
      qualified_identifier: tensorflow::core::CppShapeInferenceResult::HandleData
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: core::CppShapeInferenceResult::HandleData
        namespace_identifier: core
        ::: ::
        qualified_identifier: CppShapeInferenceResult::HandleData
         namespace_identifier: CppShapeInferenceResult
         ::: ::
         type_identifier: HandleData
      identifier: handle_data
      ;: ;
     expression_statement: handle_data.set_is_set(true);
      call_expression: handle_data.set_is_set(true)
       field_expression: handle_data.set_is_set
        identifier: handle_data
        .: .
        field_identifier: set_is_set
       argument_list: (true)
        (: (
        true: true
        ): )
      ;: ;
     compound_statement: {
    mutex_lock l(graph->mu);
    tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
    CHECK(ic != nullptr);                       // Crash OK
    CHECK_LT(output.index, ic->num_outputs());  // Crash OK
    const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
    if (shapes_and_types == nullptr) return nullptr;

    for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
  }
      {: {
      declaration: mutex_lock l(graph->mu);
       type_identifier: mutex_lock
       init_declarator: l(graph->mu)
        identifier: l
        argument_list: (graph->mu)
         (: (
         field_expression: graph->mu
          identifier: graph
          ->: ->
          field_identifier: mu
         ): )
       ;: ;
      declaration: tensorflow::shape_inference::InferenceContext* ic =
        graph->refiner.GetContext(node);
       qualified_identifier: tensorflow::shape_inference::InferenceContext
        namespace_identifier: tensorflow
        ::: ::
        qualified_identifier: shape_inference::InferenceContext
         namespace_identifier: shape_inference
         ::: ::
         type_identifier: InferenceContext
       init_declarator: * ic =
        graph->refiner.GetContext(node)
        pointer_declarator: * ic
         *: *
         identifier: ic
        =: =
        call_expression: graph->refiner.GetContext(node)
         field_expression: graph->refiner.GetContext
          field_expression: graph->refiner
           identifier: graph
           ->: ->
           field_identifier: refiner
          .: .
          field_identifier: GetContext
         argument_list: (node)
          (: (
          identifier: node
          ): )
       ;: ;
      expression_statement: CHECK(ic != nullptr);
       call_expression: CHECK(ic != nullptr)
        identifier: CHECK
        argument_list: (ic != nullptr)
         (: (
         binary_expression: ic != nullptr
          identifier: ic
          !=: !=
          null: nullptr
           nullptr: nullptr
         ): )
       ;: ;
      comment: // Crash OK
      expression_statement: CHECK_LT(output.index, ic->num_outputs());
       call_expression: CHECK_LT(output.index, ic->num_outputs())
        identifier: CHECK_LT
        argument_list: (output.index, ic->num_outputs())
         (: (
         field_expression: output.index
          identifier: output
          .: .
          field_identifier: index
         ,: ,
         call_expression: ic->num_outputs()
          field_expression: ic->num_outputs
           identifier: ic
           ->: ->
           field_identifier: num_outputs
          argument_list: ()
           (: (
           ): )
         ): )
       ;: ;
      comment: // Crash OK
      declaration: const auto* shapes_and_types =
        ic->output_handle_shapes_and_types(output.index);
       type_qualifier: const
        const: const
       placeholder_type_specifier: auto
        auto: auto
       init_declarator: * shapes_and_types =
        ic->output_handle_shapes_and_types(output.index)
        pointer_declarator: * shapes_and_types
         *: *
         identifier: shapes_and_types
        =: =
        call_expression: ic->output_handle_shapes_and_types(output.index)
         field_expression: ic->output_handle_shapes_and_types
          identifier: ic
          ->: ->
          field_identifier: output_handle_shapes_and_types
         argument_list: (output.index)
          (: (
          field_expression: output.index
           identifier: output
           .: .
           field_identifier: index
          ): )
       ;: ;
      if_statement: if (shapes_and_types == nullptr) return nullptr;
       if: if
       condition_clause: (shapes_and_types == nullptr)
        (: (
        binary_expression: shapes_and_types == nullptr
         identifier: shapes_and_types
         ==: ==
         null: nullptr
          nullptr: nullptr
        ): )
       return_statement: return nullptr;
        return: return
        null: nullptr
         nullptr: nullptr
        ;: ;
      for_range_loop: for (const auto& p : *shapes_and_types) {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
       for: for
       (: (
       type_qualifier: const
        const: const
       placeholder_type_specifier: auto
        auto: auto
       reference_declarator: & p
        &: &
        identifier: p
       :: :
       pointer_expression: *shapes_and_types
        *: *
        identifier: shapes_and_types
       ): )
       compound_statement: {
      auto* out_shape_and_type = handle_data.add_shape_and_type();
      ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
      out_shape_and_type->set_dtype(p.dtype);
      *out_shape_and_type->mutable_type() = p.type;
    }
        {: {
        declaration: auto* out_shape_and_type = handle_data.add_shape_and_type();
         placeholder_type_specifier: auto
          auto: auto
         init_declarator: * out_shape_and_type = handle_data.add_shape_and_type()
          pointer_declarator: * out_shape_and_type
           *: *
           identifier: out_shape_and_type
          =: =
          call_expression: handle_data.add_shape_and_type()
           field_expression: handle_data.add_shape_and_type
            identifier: handle_data
            .: .
            field_identifier: add_shape_and_type
           argument_list: ()
            (: (
            ): )
         ;: ;
        expression_statement: ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape());
         call_expression: ic->ShapeHandleToProto(p.shape, out_shape_and_type->mutable_shape())
          field_expression: ic->ShapeHandleToProto
           identifier: ic
           ->: ->
           field_identifier: ShapeHandleToProto
          argument_list: (p.shape, out_shape_and_type->mutable_shape())
           (: (
           field_expression: p.shape
            identifier: p
            .: .
            field_identifier: shape
           ,: ,
           call_expression: out_shape_and_type->mutable_shape()
            field_expression: out_shape_and_type->mutable_shape
             identifier: out_shape_and_type
             ->: ->
             field_identifier: mutable_shape
            argument_list: ()
             (: (
             ): )
           ): )
         ;: ;
        expression_statement: out_shape_and_type->set_dtype(p.dtype);
         call_expression: out_shape_and_type->set_dtype(p.dtype)
          field_expression: out_shape_and_type->set_dtype
           identifier: out_shape_and_type
           ->: ->
           field_identifier: set_dtype
          argument_list: (p.dtype)
           (: (
           field_expression: p.dtype
            identifier: p
            .: .
            field_identifier: dtype
           ): )
         ;: ;
        expression_statement: *out_shape_and_type->mutable_type() = p.type;
         assignment_expression: *out_shape_and_type->mutable_type() = p.type
          pointer_expression: *out_shape_and_type->mutable_type()
           *: *
           call_expression: out_shape_and_type->mutable_type()
            field_expression: out_shape_and_type->mutable_type
             identifier: out_shape_and_type
             ->: ->
             field_identifier: mutable_type
            argument_list: ()
             (: (
             ): )
          =: =
          field_expression: p.type
           identifier: p
           .: .
           field_identifier: type
         ;: ;
        }: }
      }: }
     declaration: string str_data;
      type_identifier: string
      identifier: str_data
      ;: ;
     expression_statement: handle_data.SerializeToString(&str_data);
      call_expression: handle_data.SerializeToString(&str_data)
       field_expression: handle_data.SerializeToString
        identifier: handle_data
        .: .
        field_identifier: SerializeToString
       argument_list: (&str_data)
        (: (
        pointer_expression: &str_data
         &: &
         identifier: str_data
        ): )
      ;: ;
     declaration: TF_Buffer* result = TF_NewBufferFromString(str_data.c_str(), str_data.size());
      type_identifier: TF_Buffer
      init_declarator: * result = TF_NewBufferFromString(str_data.c_str(), str_data.size())
       pointer_declarator: * result
        *: *
        identifier: result
       =: =
       call_expression: TF_NewBufferFromString(str_data.c_str(), str_data.size())
        identifier: TF_NewBufferFromString
        argument_list: (str_data.c_str(), str_data.size())
         (: (
         call_expression: str_data.c_str()
          field_expression: str_data.c_str
           identifier: str_data
           .: .
           field_identifier: c_str
          argument_list: ()
           (: (
           ): )
         ,: ,
         call_expression: str_data.size()
          field_expression: str_data.size
           identifier: str_data
           .: .
           field_identifier: size
          argument_list: ()
           (: (
           ): )
         ): )
      ;: ;
     return_statement: return result;
      return: return
      identifier: result
      ;: ;
     }: }
   function_definition: void TF_SetHandleShapeAndType(TF_Graph* graph, TF_Output output,
                              const void* proto, size_t proto_len,
                              TF_Status* status) {
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  if (!handle_data.ParseFromArray(proto, proto_len)) {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
  DCHECK(handle_data.is_set());

  tensorflow::mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&output.oper->node);

  std::vector<tensorflow::shape_inference::ShapeAndType> shapes_and_types;
  for (const auto& shape_and_type_proto : handle_data.shape_and_type()) {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
  ic->set_output_handle_shapes_and_types(output.index, shapes_and_types);
}
    primitive_type: void
    function_declarator: TF_SetHandleShapeAndType(TF_Graph* graph, TF_Output output,
                              const void* proto, size_t proto_len,
                              TF_Status* status)
     identifier: TF_SetHandleShapeAndType
     parameter_list: (TF_Graph* graph, TF_Output output,
                              const void* proto, size_t proto_len,
                              TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Output output
       type_identifier: TF_Output
       identifier: output
      ,: ,
      parameter_declaration: const void* proto
       type_qualifier: const
        const: const
       primitive_type: void
       pointer_declarator: * proto
        *: *
        identifier: proto
      ,: ,
      parameter_declaration: size_t proto_len
       primitive_type: size_t
       identifier: proto_len
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
  if (!handle_data.ParseFromArray(proto, proto_len)) {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
  DCHECK(handle_data.is_set());

  tensorflow::mutex_lock l(graph->mu);
  tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&output.oper->node);

  std::vector<tensorflow::shape_inference::ShapeAndType> shapes_and_types;
  for (const auto& shape_and_type_proto : handle_data.shape_and_type()) {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
  ic->set_output_handle_shapes_and_types(output.index, shapes_and_types);
}
     {: {
     declaration: tensorflow::core::CppShapeInferenceResult::HandleData handle_data;
      qualified_identifier: tensorflow::core::CppShapeInferenceResult::HandleData
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: core::CppShapeInferenceResult::HandleData
        namespace_identifier: core
        ::: ::
        qualified_identifier: CppShapeInferenceResult::HandleData
         namespace_identifier: CppShapeInferenceResult
         ::: ::
         type_identifier: HandleData
      identifier: handle_data
      ;: ;
     if_statement: if (!handle_data.ParseFromArray(proto, proto_len)) {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
      if: if
      condition_clause: (!handle_data.ParseFromArray(proto, proto_len))
       (: (
       unary_expression: !handle_data.ParseFromArray(proto, proto_len)
        !: !
        call_expression: handle_data.ParseFromArray(proto, proto_len)
         field_expression: handle_data.ParseFromArray
          identifier: handle_data
          .: .
          field_identifier: ParseFromArray
         argument_list: (proto, proto_len)
          (: (
          identifier: proto
          ,: ,
          identifier: proto_len
          ): )
       ): )
      compound_statement: {
    status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
    return;
  }
       {: {
       expression_statement: status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto");
        assignment_expression: status->status =
        absl::InvalidArgumentError("Couldn't deserialize HandleData proto")
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: absl::InvalidArgumentError("Couldn't deserialize HandleData proto")
          qualified_identifier: absl::InvalidArgumentError
           namespace_identifier: absl
           ::: ::
           identifier: InvalidArgumentError
          argument_list: ("Couldn't deserialize HandleData proto")
           (: (
           string_literal: "Couldn't deserialize HandleData proto"
            ": "
            string_content: Couldn't deserialize HandleData proto
            ": "
           ): )
        ;: ;
       return_statement: return;
        return: return
        ;: ;
       }: }
     expression_statement: DCHECK(handle_data.is_set());
      call_expression: DCHECK(handle_data.is_set())
       identifier: DCHECK
       argument_list: (handle_data.is_set())
        (: (
        call_expression: handle_data.is_set()
         field_expression: handle_data.is_set
          identifier: handle_data
          .: .
          field_identifier: is_set
         argument_list: ()
          (: (
          ): )
        ): )
      ;: ;
     declaration: tensorflow::mutex_lock l(graph->mu);
      qualified_identifier: tensorflow::mutex_lock
       namespace_identifier: tensorflow
       ::: ::
       type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     declaration: tensorflow::shape_inference::InferenceContext* ic =
      graph->refiner.GetContext(&output.oper->node);
      qualified_identifier: tensorflow::shape_inference::InferenceContext
       namespace_identifier: tensorflow
       ::: ::
       qualified_identifier: shape_inference::InferenceContext
        namespace_identifier: shape_inference
        ::: ::
        type_identifier: InferenceContext
      init_declarator: * ic =
      graph->refiner.GetContext(&output.oper->node)
       pointer_declarator: * ic
        *: *
        identifier: ic
       =: =
       call_expression: graph->refiner.GetContext(&output.oper->node)
        field_expression: graph->refiner.GetContext
         field_expression: graph->refiner
          identifier: graph
          ->: ->
          field_identifier: refiner
         .: .
         field_identifier: GetContext
        argument_list: (&output.oper->node)
         (: (
         pointer_expression: &output.oper->node
          &: &
          field_expression: output.oper->node
           field_expression: output.oper
            identifier: output
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
         ): )
      ;: ;
     declaration: std::vector<tensorflow::shape_inference::ShapeAndType> shapes_and_types;
      qualified_identifier: std::vector<tensorflow::shape_inference::ShapeAndType>
       namespace_identifier: std
       ::: ::
       template_type: vector<tensorflow::shape_inference::ShapeAndType>
        type_identifier: vector
        template_argument_list: <tensorflow::shape_inference::ShapeAndType>
         <: <
         type_descriptor: tensorflow::shape_inference::ShapeAndType
          qualified_identifier: tensorflow::shape_inference::ShapeAndType
           namespace_identifier: tensorflow
           ::: ::
           qualified_identifier: shape_inference::ShapeAndType
            namespace_identifier: shape_inference
            ::: ::
            type_identifier: ShapeAndType
         >: >
      identifier: shapes_and_types
      ;: ;
     for_range_loop: for (const auto& shape_and_type_proto : handle_data.shape_and_type()) {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
      for: for
      (: (
      type_qualifier: const
       const: const
      placeholder_type_specifier: auto
       auto: auto
      reference_declarator: & shape_and_type_proto
       &: &
       identifier: shape_and_type_proto
      :: :
      call_expression: handle_data.shape_and_type()
       field_expression: handle_data.shape_and_type
        identifier: handle_data
        .: .
        field_identifier: shape_and_type
       argument_list: ()
        (: (
        ): )
      ): )
      compound_statement: {
    tensorflow::shape_inference::ShapeHandle shape;
    status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
    if (TF_GetCode(status) != TF_OK) return;
    shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
  }
       {: {
       declaration: tensorflow::shape_inference::ShapeHandle shape;
        qualified_identifier: tensorflow::shape_inference::ShapeHandle
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: shape_inference::ShapeHandle
          namespace_identifier: shape_inference
          ::: ::
          type_identifier: ShapeHandle
        identifier: shape
        ;: ;
       expression_statement: status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape);
        assignment_expression: status->status =
        ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: ic->MakeShapeFromShapeProto(shape_and_type_proto.shape(), &shape)
          field_expression: ic->MakeShapeFromShapeProto
           identifier: ic
           ->: ->
           field_identifier: MakeShapeFromShapeProto
          argument_list: (shape_and_type_proto.shape(), &shape)
           (: (
           call_expression: shape_and_type_proto.shape()
            field_expression: shape_and_type_proto.shape
             identifier: shape_and_type_proto
             .: .
             field_identifier: shape
            argument_list: ()
             (: (
             ): )
           ,: ,
           pointer_expression: &shape
            &: &
            identifier: shape
           ): )
        ;: ;
       if_statement: if (TF_GetCode(status) != TF_OK) return;
        if: if
        condition_clause: (TF_GetCode(status) != TF_OK)
         (: (
         binary_expression: TF_GetCode(status) != TF_OK
          call_expression: TF_GetCode(status)
           identifier: TF_GetCode
           argument_list: (status)
            (: (
            identifier: status
            ): )
          !=: !=
          identifier: TF_OK
         ): )
        return_statement: return;
         return: return
         ;: ;
       expression_statement: shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type());
        call_expression: shapes_and_types.emplace_back(shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type())
         field_expression: shapes_and_types.emplace_back
          identifier: shapes_and_types
          .: .
          field_identifier: emplace_back
         argument_list: (shape, shape_and_type_proto.dtype(),
                                  shape_and_type_proto.type())
          (: (
          identifier: shape
          ,: ,
          call_expression: shape_and_type_proto.dtype()
           field_expression: shape_and_type_proto.dtype
            identifier: shape_and_type_proto
            .: .
            field_identifier: dtype
           argument_list: ()
            (: (
            ): )
          ,: ,
          call_expression: shape_and_type_proto.type()
           field_expression: shape_and_type_proto.type
            identifier: shape_and_type_proto
            .: .
            field_identifier: type
           argument_list: ()
            (: (
            ): )
          ): )
        ;: ;
       }: }
     expression_statement: ic->set_output_handle_shapes_and_types(output.index, shapes_and_types);
      call_expression: ic->set_output_handle_shapes_and_types(output.index, shapes_and_types)
       field_expression: ic->set_output_handle_shapes_and_types
        identifier: ic
        ->: ->
        field_identifier: set_output_handle_shapes_and_types
       argument_list: (output.index, shapes_and_types)
        (: (
        field_expression: output.index
         identifier: output
         .: .
         field_identifier: index
        ,: ,
        identifier: shapes_and_types
        ): )
      ;: ;
     }: }
   function_definition: void TF_AddWhileInputHack(TF_Graph* graph, TF_Output new_src, TF_Operation* dst,
                          TF_Status* status) {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node);
  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
}
    primitive_type: void
    function_declarator: TF_AddWhileInputHack(TF_Graph* graph, TF_Output new_src, TF_Operation* dst,
                          TF_Status* status)
     identifier: TF_AddWhileInputHack
     parameter_list: (TF_Graph* graph, TF_Output new_src, TF_Operation* dst,
                          TF_Status* status)
      (: (
      parameter_declaration: TF_Graph* graph
       type_identifier: TF_Graph
       pointer_declarator: * graph
        *: *
        identifier: graph
      ,: ,
      parameter_declaration: TF_Output new_src
       type_identifier: TF_Output
       identifier: new_src
      ,: ,
      parameter_declaration: TF_Operation* dst
       type_identifier: TF_Operation
       pointer_declarator: * dst
        *: *
        identifier: dst
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
  using tensorflow::RecordMutation;
  mutex_lock l(graph->mu);
  status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node);
  if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
}
     {: {
     using_declaration: using tensorflow::RecordMutation;
      using: using
      qualified_identifier: tensorflow::RecordMutation
       namespace_identifier: tensorflow
       ::: ::
       identifier: RecordMutation
      ;: ;
     declaration: mutex_lock l(graph->mu);
      type_identifier: mutex_lock
      init_declarator: l(graph->mu)
       identifier: l
       argument_list: (graph->mu)
        (: (
        field_expression: graph->mu
         identifier: graph
         ->: ->
         field_identifier: mu
        ): )
      ;: ;
     expression_statement: status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node);
      assignment_expression: status->status = graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node)
       field_expression: status->status
        identifier: status
        ->: ->
        field_identifier: status
       =: =
       call_expression: graph->graph.AddWhileInputHack(&new_src.oper->node,
                                                  new_src.index, &dst->node)
        field_expression: graph->graph.AddWhileInputHack
         field_expression: graph->graph
          identifier: graph
          ->: ->
          field_identifier: graph
         .: .
         field_identifier: AddWhileInputHack
        argument_list: (&new_src.oper->node,
                                                  new_src.index, &dst->node)
         (: (
         pointer_expression: &new_src.oper->node
          &: &
          field_expression: new_src.oper->node
           field_expression: new_src.oper
            identifier: new_src
            .: .
            field_identifier: oper
           ->: ->
           field_identifier: node
         ,: ,
         field_expression: new_src.index
          identifier: new_src
          .: .
          field_identifier: index
         ,: ,
         pointer_expression: &dst->node
          &: &
          field_expression: dst->node
           identifier: dst
           ->: ->
           field_identifier: node
         ): )
      ;: ;
     if_statement: if (TF_GetCode(status) == TF_OK) {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
      if: if
      condition_clause: (TF_GetCode(status) == TF_OK)
       (: (
       binary_expression: TF_GetCode(status) == TF_OK
        call_expression: TF_GetCode(status)
         identifier: TF_GetCode
         argument_list: (status)
          (: (
          identifier: status
          ): )
        ==: ==
        identifier: TF_OK
       ): )
      compound_statement: {
    // This modification only updates the destination node for
    // the purposes of running this graph in a session. Thus, we don't
    // record the source node as being modified.
    RecordMutation(graph, *dst, "adding input tensor");
  }
       {: {
       comment: // This modification only updates the destination node for
       comment: // the purposes of running this graph in a session. Thus, we don't
       comment: // record the source node as being modified.
       expression_statement: RecordMutation(graph, *dst, "adding input tensor");
        call_expression: RecordMutation(graph, *dst, "adding input tensor")
         identifier: RecordMutation
         argument_list: (graph, *dst, "adding input tensor")
          (: (
          identifier: graph
          ,: ,
          pointer_expression: *dst
           *: *
           identifier: dst
          ,: ,
          string_literal: "adding input tensor"
           ": "
           string_content: adding input tensor
           ": "
          ): )
        ;: ;
       }: }
     }: }
   comment: // -------------------------------------------------------------------
   comment: // TF_Server functions ----------------------------------------------
   preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
TF_Server::TF_Server(std::unique_ptr<tensorflow::ServerInterface> server)
    : target(server->target()), server(std::move(server)) {}
#endif
    #if: #if
    binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
     unary_expression: !defined(IS_MOBILE_PLATFORM)
      !: !
      preproc_defined: defined(IS_MOBILE_PLATFORM)
       defined: defined
       (: (
       identifier: IS_MOBILE_PLATFORM
       ): )
     &&: &&
     unary_expression: !defined(IS_SLIM_BUILD)
      !: !
      preproc_defined: defined(IS_SLIM_BUILD)
       defined: defined
       (: (
       identifier: IS_SLIM_BUILD
       ): )
    
: 

    function_definition: TF_Server::TF_Server(std::unique_ptr<tensorflow::ServerInterface> server)
    : target(server->target()), server(std::move(server)) {}
     function_declarator: TF_Server::TF_Server(std::unique_ptr<tensorflow::ServerInterface> server)
      qualified_identifier: TF_Server::TF_Server
       namespace_identifier: TF_Server
       ::: ::
       identifier: TF_Server
      parameter_list: (std::unique_ptr<tensorflow::ServerInterface> server)
       (: (
       parameter_declaration: std::unique_ptr<tensorflow::ServerInterface> server
        qualified_identifier: std::unique_ptr<tensorflow::ServerInterface>
         namespace_identifier: std
         ::: ::
         template_type: unique_ptr<tensorflow::ServerInterface>
          type_identifier: unique_ptr
          template_argument_list: <tensorflow::ServerInterface>
           <: <
           type_descriptor: tensorflow::ServerInterface
            qualified_identifier: tensorflow::ServerInterface
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: ServerInterface
           >: >
        identifier: server
       ): )
     field_initializer_list: : target(server->target()), server(std::move(server))
      :: :
      field_initializer: target(server->target())
       field_identifier: target
       argument_list: (server->target())
        (: (
        call_expression: server->target()
         field_expression: server->target
          identifier: server
          ->: ->
          field_identifier: target
         argument_list: ()
          (: (
          ): )
        ): )
      ,: ,
      field_initializer: server(std::move(server))
       field_identifier: server
       argument_list: (std::move(server))
        (: (
        call_expression: std::move(server)
         qualified_identifier: std::move
          namespace_identifier: std
          ::: ::
          identifier: move
         argument_list: (server)
          (: (
          identifier: server
          ): )
        ): )
     compound_statement: {}
      {: {
      }: }
    #endif: #endif
   comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
   function_definition: TF_Server* TF_NewServer(const void* proto, size_t proto_len,
                        TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
  return nullptr;
#else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    type_identifier: TF_Server
    pointer_declarator: * TF_NewServer(const void* proto, size_t proto_len,
                        TF_Status* status)
     *: *
     function_declarator: TF_NewServer(const void* proto, size_t proto_len,
                        TF_Status* status)
      identifier: TF_NewServer
      parameter_list: (const void* proto, size_t proto_len,
                        TF_Status* status)
       (: (
       parameter_declaration: const void* proto
        type_qualifier: const
         const: const
        primitive_type: void
        pointer_declarator: * proto
         *: *
         identifier: proto
       ,: ,
       parameter_declaration: size_t proto_len
        primitive_type: size_t
        identifier: proto_len
       ,: ,
       parameter_declaration: TF_Status* status
        type_identifier: TF_Status
        pointer_declarator: * status
         *: *
         identifier: status
       ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
  return nullptr;
#else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
  return nullptr;
#else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Server functionality is not supported on mobile")
          (: (
          string_literal: "Server functionality is not supported on mobile"
           ": "
           string_content: Server functionality is not supported on mobile
           ": "
          ): )
       ;: ;
      return_statement: return nullptr;
       return: return
       null: nullptr
        nullptr: nullptr
       ;: ;
      preproc_else: #else
  tensorflow::ServerDef server_def;
  if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }

  std::unique_ptr<tensorflow::ServerInterface> out_server;
  status->status = tensorflow::NewServer(server_def, &out_server);
  if (!status->status.ok()) return nullptr;

  return new TF_Server(std::move(out_server));
       #else: #else
       declaration: tensorflow::ServerDef server_def;
        qualified_identifier: tensorflow::ServerDef
         namespace_identifier: tensorflow
         ::: ::
         type_identifier: ServerDef
        identifier: server_def
        ;: ;
       if_statement: if (!server_def.ParseFromArray(proto, static_cast<int>(proto_len))) {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }
        if: if
        condition_clause: (!server_def.ParseFromArray(proto, static_cast<int>(proto_len)))
         (: (
         unary_expression: !server_def.ParseFromArray(proto, static_cast<int>(proto_len))
          !: !
          call_expression: server_def.ParseFromArray(proto, static_cast<int>(proto_len))
           field_expression: server_def.ParseFromArray
            identifier: server_def
            .: .
            field_identifier: ParseFromArray
           argument_list: (proto, static_cast<int>(proto_len))
            (: (
            identifier: proto
            ,: ,
            call_expression: static_cast<int>(proto_len)
             template_function: static_cast<int>
              identifier: static_cast
              template_argument_list: <int>
               <: <
               type_descriptor: int
                primitive_type: int
               >: >
             argument_list: (proto_len)
              (: (
              identifier: proto_len
              ): )
            ): )
         ): )
        compound_statement: {
    status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
    return nullptr;
  }
         {: {
         expression_statement: status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer");
          assignment_expression: status->status = InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer")
           field_expression: status->status
            identifier: status
            ->: ->
            field_identifier: status
           =: =
           call_expression: InvalidArgument(
        "Could not parse provided bytes into a ServerDef protocol buffer")
            identifier: InvalidArgument
            argument_list: (
        "Could not parse provided bytes into a ServerDef protocol buffer")
             (: (
             string_literal: "Could not parse provided bytes into a ServerDef protocol buffer"
              ": "
              string_content: Could not parse provided bytes into a ServerDef protocol buffer
              ": "
             ): )
          ;: ;
         return_statement: return nullptr;
          return: return
          null: nullptr
           nullptr: nullptr
          ;: ;
         }: }
       declaration: std::unique_ptr<tensorflow::ServerInterface> out_server;
        qualified_identifier: std::unique_ptr<tensorflow::ServerInterface>
         namespace_identifier: std
         ::: ::
         template_type: unique_ptr<tensorflow::ServerInterface>
          type_identifier: unique_ptr
          template_argument_list: <tensorflow::ServerInterface>
           <: <
           type_descriptor: tensorflow::ServerInterface
            qualified_identifier: tensorflow::ServerInterface
             namespace_identifier: tensorflow
             ::: ::
             type_identifier: ServerInterface
           >: >
        identifier: out_server
        ;: ;
       expression_statement: status->status = tensorflow::NewServer(server_def, &out_server);
        assignment_expression: status->status = tensorflow::NewServer(server_def, &out_server)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: tensorflow::NewServer(server_def, &out_server)
          qualified_identifier: tensorflow::NewServer
           namespace_identifier: tensorflow
           ::: ::
           identifier: NewServer
          argument_list: (server_def, &out_server)
           (: (
           identifier: server_def
           ,: ,
           pointer_expression: &out_server
            &: &
            identifier: out_server
           ): )
        ;: ;
       if_statement: if (!status->status.ok()) return nullptr;
        if: if
        condition_clause: (!status->status.ok())
         (: (
         unary_expression: !status->status.ok()
          !: !
          call_expression: status->status.ok()
           field_expression: status->status.ok
            field_expression: status->status
             identifier: status
             ->: ->
             field_identifier: status
            .: .
            field_identifier: ok
           argument_list: ()
            (: (
            ): )
         ): )
        return_statement: return nullptr;
         return: return
         null: nullptr
          nullptr: nullptr
         ;: ;
       return_statement: return new TF_Server(std::move(out_server));
        return: return
        new_expression: new TF_Server(std::move(out_server))
         new: new
         type_identifier: TF_Server
         argument_list: (std::move(out_server))
          (: (
          call_expression: std::move(out_server)
           qualified_identifier: std::move
            namespace_identifier: std
            ::: ::
            identifier: move
           argument_list: (out_server)
            (: (
            identifier: out_server
            ): )
          ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_ServerStart(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Start();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_ServerStart(TF_Server* server, TF_Status* status)
     identifier: TF_ServerStart
     parameter_list: (TF_Server* server, TF_Status* status)
      (: (
      parameter_declaration: TF_Server* server
       type_identifier: TF_Server
       pointer_declarator: * server
        *: *
        identifier: server
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Start();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Start();
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Server functionality is not supported on mobile")
          (: (
          string_literal: "Server functionality is not supported on mobile"
           ": "
           string_content: Server functionality is not supported on mobile
           ": "
          ): )
       ;: ;
      preproc_else: #else
  status->status = server->server->Start();
       #else: #else
       expression_statement: status->status = server->server->Start();
        assignment_expression: status->status = server->server->Start()
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: server->server->Start()
          field_expression: server->server->Start
           field_expression: server->server
            identifier: server
            ->: ->
            field_identifier: server
           ->: ->
           field_identifier: Start
          argument_list: ()
           (: (
           ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_ServerStop(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Stop();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_ServerStop(TF_Server* server, TF_Status* status)
     identifier: TF_ServerStop
     parameter_list: (TF_Server* server, TF_Status* status)
      (: (
      parameter_declaration: TF_Server* server
       type_identifier: TF_Server
       pointer_declarator: * server
        *: *
        identifier: server
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Stop();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Stop();
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Server functionality is not supported on mobile")
          (: (
          string_literal: "Server functionality is not supported on mobile"
           ": "
           string_content: Server functionality is not supported on mobile
           ": "
          ): )
       ;: ;
      preproc_else: #else
  status->status = server->server->Stop();
       #else: #else
       expression_statement: status->status = server->server->Stop();
        assignment_expression: status->status = server->server->Stop()
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: server->server->Stop()
          field_expression: server->server->Stop
           field_expression: server->server
            identifier: server
            ->: ->
            field_identifier: server
           ->: ->
           field_identifier: Stop
          argument_list: ()
           (: (
           ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_ServerJoin(TF_Server* server, TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Join();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_ServerJoin(TF_Server* server, TF_Status* status)
     identifier: TF_ServerJoin
     parameter_list: (TF_Server* server, TF_Status* status)
      (: (
      parameter_declaration: TF_Server* server
       type_identifier: TF_Server
       pointer_declarator: * server
        *: *
        identifier: server
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Join();
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
#else
  status->status = server->server->Join();
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "Server functionality is not supported on mobile")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "Server functionality is not supported on mobile")
          (: (
          string_literal: "Server functionality is not supported on mobile"
           ": "
           string_content: Server functionality is not supported on mobile
           ": "
          ): )
       ;: ;
      preproc_else: #else
  status->status = server->server->Join();
       #else: #else
       expression_statement: status->status = server->server->Join();
        assignment_expression: status->status = server->server->Join()
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: server->server->Join()
          field_expression: server->server->Join
           field_expression: server->server
            identifier: server
            ->: ->
            field_identifier: server
           ->: ->
           field_identifier: Join
          argument_list: ()
           (: (
           ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   function_definition: const char* TF_ServerTarget(TF_Server* server) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  return nullptr;
#else
  return server->target.c_str();
#endif
}
    type_qualifier: const
     const: const
    primitive_type: char
    pointer_declarator: * TF_ServerTarget(TF_Server* server)
     *: *
     function_declarator: TF_ServerTarget(TF_Server* server)
      identifier: TF_ServerTarget
      parameter_list: (TF_Server* server)
       (: (
       parameter_declaration: TF_Server* server
        type_identifier: TF_Server
        pointer_declarator: * server
         *: *
         identifier: server
       ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  return nullptr;
#else
  return server->target.c_str();
#endif
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  return nullptr;
#else
  return server->target.c_str();
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      return_statement: return nullptr;
       return: return
       null: nullptr
        nullptr: nullptr
       ;: ;
      preproc_else: #else
  return server->target.c_str();
       #else: #else
       return_statement: return server->target.c_str();
        return: return
        call_expression: server->target.c_str()
         field_expression: server->target.c_str
          field_expression: server->target
           identifier: server
           ->: ->
           field_identifier: target
          .: .
          field_identifier: c_str
         argument_list: ()
          (: (
          ): )
        ;: ;
      #endif: #endif
     }: }
   function_definition: void TF_DeleteServer(TF_Server* server) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  delete server;
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_DeleteServer(TF_Server* server)
     identifier: TF_DeleteServer
     parameter_list: (TF_Server* server)
      (: (
      parameter_declaration: TF_Server* server
       type_identifier: TF_Server
       pointer_declarator: * server
        *: *
        identifier: server
      ): )
    compound_statement: {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  delete server;
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  delete server;
#endif
      #if: #if
      binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
       unary_expression: !defined(IS_MOBILE_PLATFORM)
        !: !
        preproc_defined: defined(IS_MOBILE_PLATFORM)
         defined: defined
         (: (
         identifier: IS_MOBILE_PLATFORM
         ): )
       &&: &&
       unary_expression: !defined(IS_SLIM_BUILD)
        !: !
        preproc_defined: defined(IS_SLIM_BUILD)
         defined: defined
         (: (
         identifier: IS_SLIM_BUILD
         ): )
      
: 

      expression_statement: delete server;
       delete_expression: delete server
        delete: delete
        identifier: server
       ;: ;
      #endif: #endif
     comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_RegisterLogListener(void (*listener)(const char*)) {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  tensorflow::logging::RegisterListener(listener);
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_RegisterLogListener(void (*listener)(const char*))
     identifier: TF_RegisterLogListener
     parameter_list: (void (*listener)(const char*))
      (: (
      parameter_declaration: void (*listener)(const char*)
       primitive_type: void
       function_declarator: (*listener)(const char*)
        parenthesized_declarator: (*listener)
         (: (
         pointer_declarator: *listener
          *: *
          identifier: listener
         ): )
        parameter_list: (const char*)
         (: (
         parameter_declaration: const char*
          type_qualifier: const
           const: const
          primitive_type: char
          abstract_pointer_declarator: *
           *: *
         ): )
      ): )
    compound_statement: {
#if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  tensorflow::logging::RegisterListener(listener);
#endif  // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
  tensorflow::logging::RegisterListener(listener);
#endif
      #if: #if
      binary_expression: !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
       unary_expression: !defined(IS_MOBILE_PLATFORM)
        !: !
        preproc_defined: defined(IS_MOBILE_PLATFORM)
         defined: defined
         (: (
         identifier: IS_MOBILE_PLATFORM
         ): )
       &&: &&
       unary_expression: !defined(IS_SLIM_BUILD)
        !: !
        preproc_defined: defined(IS_SLIM_BUILD)
         defined: defined
         (: (
         identifier: IS_SLIM_BUILD
         ): )
      
: 

      expression_statement: tensorflow::logging::RegisterListener(listener);
       call_expression: tensorflow::logging::RegisterListener(listener)
        qualified_identifier: tensorflow::logging::RegisterListener
         namespace_identifier: tensorflow
         ::: ::
         qualified_identifier: logging::RegisterListener
          namespace_identifier: logging
          ::: ::
          identifier: RegisterListener
        argument_list: (listener)
         (: (
         identifier: listener
         ): )
       ;: ;
      #endif: #endif
     comment: // !defined(IS_MOBILE_PLATFORM) && !defined(IS_SLIM_BUILD)
     }: }
   function_definition: void TF_RegisterFilesystemPlugin(const char* plugin_filename,
                                 TF_Status* status) {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
#else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
    primitive_type: void
    function_declarator: TF_RegisterFilesystemPlugin(const char* plugin_filename,
                                 TF_Status* status)
     identifier: TF_RegisterFilesystemPlugin
     parameter_list: (const char* plugin_filename,
                                 TF_Status* status)
      (: (
      parameter_declaration: const char* plugin_filename
       type_qualifier: const
        const: const
       primitive_type: char
       pointer_declarator: * plugin_filename
        *: *
        identifier: plugin_filename
      ,: ,
      parameter_declaration: TF_Status* status
       type_identifier: TF_Status
       pointer_declarator: * status
        *: *
        identifier: status
      ): )
    compound_statement: {
#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
#else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
}
     {: {
     preproc_if: #if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
  status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
#else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
#endif
      #if: #if
      binary_expression: defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
       preproc_defined: defined(IS_MOBILE_PLATFORM)
        defined: defined
        (: (
        identifier: IS_MOBILE_PLATFORM
        ): )
       ||: ||
       preproc_defined: defined(IS_SLIM_BUILD)
        defined: defined
        (: (
        identifier: IS_SLIM_BUILD
        ): )
      
: 

      expression_statement: status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile");
       assignment_expression: status->status = tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile")
        field_expression: status->status
         identifier: status
         ->: ->
         field_identifier: status
        =: =
        call_expression: tensorflow::errors::Unimplemented(
      "FileSystem plugin functionality is not supported on mobile")
         qualified_identifier: tensorflow::errors::Unimplemented
          namespace_identifier: tensorflow
          ::: ::
          qualified_identifier: errors::Unimplemented
           namespace_identifier: errors
           ::: ::
           identifier: Unimplemented
         argument_list: (
      "FileSystem plugin functionality is not supported on mobile")
          (: (
          string_literal: "FileSystem plugin functionality is not supported on mobile"
           ": "
           string_content: FileSystem plugin functionality is not supported on mobile
           ": "
          ): )
       ;: ;
      preproc_else: #else
  status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
       #else: #else
       expression_statement: status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename);
        assignment_expression: status->status = tensorflow::RegisterFilesystemPlugin(plugin_filename)
         field_expression: status->status
          identifier: status
          ->: ->
          field_identifier: status
         =: =
         call_expression: tensorflow::RegisterFilesystemPlugin(plugin_filename)
          qualified_identifier: tensorflow::RegisterFilesystemPlugin
           namespace_identifier: tensorflow
           ::: ::
           identifier: RegisterFilesystemPlugin
          argument_list: (plugin_filename)
           (: (
           identifier: plugin_filename
           ): )
        ;: ;
      #endif: #endif
     comment: // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)
     }: }
   }: }
 comment: // end extern "C"
