module: # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================
"""Exposes the Python wrapper conversion to trt_graph."""

import collections
from functools import partial  # pylint: disable=g-importing-member
import os
import platform
import sys
import tempfile

import numpy as np
import six as _six

from tensorflow.core.framework import variable_pb2
from tensorflow.core.protobuf import config_pb2
from tensorflow.core.protobuf import meta_graph_pb2
from tensorflow.core.protobuf import rewriter_config_pb2
from tensorflow.python.client import session
from tensorflow.python.compiler.tensorrt import utils as trt_utils
from tensorflow.python.eager import context
from tensorflow.python.eager import wrap_function
from tensorflow.python.framework import convert_to_constants
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import errors
from tensorflow.python.framework import importer
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor
from tensorflow.python.grappler import tf_optimizer
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import gen_resource_variable_ops
from tensorflow.python.platform import tf_logging as logging
from tensorflow.python.saved_model import builder
from tensorflow.python.saved_model import load
from tensorflow.python.saved_model import loader
from tensorflow.python.saved_model import save
from tensorflow.python.saved_model import signature_constants
from tensorflow.python.saved_model import tag_constants
from tensorflow.python.trackable import asset
from tensorflow.python.trackable import autotrackable
from tensorflow.python.trackable import resource
from tensorflow.python.training import saver
from tensorflow.python.util import deprecation
from tensorflow.python.util import nest
from tensorflow.python.util.lazy_loader import LazyLoader
from tensorflow.python.util.tf_export import tf_export

# Lazily load the op, since it's not available in cpu-only builds. Importing
# this at top will cause tests that imports TF-TRT fail when they're built
# and run without CUDA/GPU.
gen_trt_ops = LazyLoader(
    "gen_trt_ops", globals(),
    "tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops")

_pywrap_py_utils = LazyLoader(
    "_pywrap_py_utils", globals(),
    "tensorflow.compiler.tf2tensorrt._pywrap_py_utils")

# Register TRT ops in python, so that when users import this module they can
# execute a TRT-converted graph without calling any of the methods in this
# module.
#
# This will call register_op_list() in
# tensorflow/python/framework/op_def_registry.py, but it doesn't register
# the op or the op kernel in C++ runtime.
try:
  gen_trt_ops.trt_engine_op  # pylint: disable=pointless-statement
except AttributeError:
  pass


def _to_bytes(s):
  """Encode s if it is a sequence of chars."""
  if isinstance(s, _six.text_type):
    return s.encode("utf-8", errors="surrogateescape")
  return s


def _to_string(s):
  """Decode s if it is a sequence of bytes."""
  if isinstance(s, _six.binary_type):
    return s.decode("utf-8")
  return s


class TrtPrecisionMode(object):
  FP32 = "FP32"
  FP16 = "FP16"
  INT8 = "INT8"

  @staticmethod
  def supported_precision_modes():
    precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
    return precisions + [p.lower() for p in precisions]


# Use a large enough number as the default max_workspace_size for TRT engines,
# so it can produce reasonable performance results with the default.
# For TRT >= 8.4, the recommendation is MAX_INT.
if (_pywrap_py_utils.is_tensorrt_enabled() and
    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)):
  # We must use `sys.maxsize - 512` to avoid overflow during casting.
  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512
else:
  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30  # 1,073,741,824

PROFILE_STRATEGY_RANGE = "Range"
PROFILE_STRATEGY_OPTIMAL = "Optimal"
PROFILE_STRATEGY_RANGE_OPTIMAL = "Range+Optimal"
PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE = "ImplicitBatchModeCompatible"


def supported_profile_strategies():
  return [
      PROFILE_STRATEGY_RANGE, PROFILE_STRATEGY_OPTIMAL,
      PROFILE_STRATEGY_RANGE_OPTIMAL,
      PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
  ]


@tf_export("experimental.tensorrt.ConversionParams", v1=[])
class TrtConversionParams(
    collections.namedtuple("TrtConversionParams", [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ])):
  """Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  """

  def __new__(cls,
              max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
              precision_mode=TrtPrecisionMode.FP32,
              minimum_segment_size=3,
              maximum_cached_engines=1,
              use_calibration=True,
              allow_build_at_runtime=True):
    return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)


DEFAULT_TRT_CONVERSION_PARAMS = TrtConversionParams()

_TRT_ENGINE_OP_NAME = "TRTEngineOp"


def _check_conversion_params(conversion_params, is_v2=False):
  """Validate the provided TrtConversionParams.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
  supported_precision_modes = TrtPrecisionMode.supported_precision_modes()
  if conversion_params.precision_mode not in supported_precision_modes:
    raise ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
  if (conversion_params.minimum_segment_size <= 0 and
      conversion_params.minimum_segment_size != -1):
    raise ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")


def _check_trt_version_compatibility():
  """Check compatibility of TensorRT version.

  Raises:
    RuntimeError: if the TensorRT library version is incompatible.
  """

  if not _pywrap_py_utils.is_tensorrt_enabled():
    logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")

    raise RuntimeError("Tensorflow has not been built with TensorRT support.")

  if platform.system() == "Windows":
    logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")

  linked_version = _pywrap_py_utils.get_linked_tensorrt_version()
  loaded_version = _pywrap_py_utils.get_loaded_tensorrt_version()

  logging.info("Linked TensorRT version: %s", str(linked_version))
  logging.info("Loaded TensorRT version: %s", str(loaded_version))

  def raise_trt_version_deprecated(version_type, trt_version):
    assert version_type in [
        "linked", "loaded"
    ], ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type

    logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))

    raise RuntimeError("Incompatible %s TensorRT versions" % version_type)

  if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("linked", linked_version)

  if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("loaded", loaded_version)

  if (loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):
    logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
    raise RuntimeError("Incompatible TensorRT major version")

  elif loaded_version != linked_version:
    logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))


def _get_tensorrt_rewriter_config(conversion_params,
                                  is_dynamic_op=None,
                                  max_batch_size=None,
                                  is_v2=False,
                                  disable_non_trt_optimizers=False,
                                  use_implicit_batch=True,
                                  profile_strategy=PROFILE_STRATEGY_RANGE):
  """Returns a RewriterConfig proto for TRT transformation.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_dynamic_op: whether to use dynamic engines.
    max_batch_size: maximum batch size for static engines.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.
    disable_non_trt_optimizers: Turn off all default Grappler optimizers.
    use_implicit_batch: Whether to use implicit batch or explicit batch.
    profile_strategy: dynamic shape optimization profile strategy.

  Returns:
    A RewriterConfig proto which sets a TensorRTOptimizer to run Grappler.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
  _check_conversion_params(conversion_params, is_v2=is_v2)
  if is_v2 and is_dynamic_op is not None and not is_dynamic_op:
    raise ValueError("is_dynamic_op is either None or True for TF2")
  if not is_v2 and is_dynamic_op is None:
    raise ValueError("is_dynamic_op can't be None for TF1")

  if (is_dynamic_op is None or is_dynamic_op) and max_batch_size is not None:
    raise ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
  if is_dynamic_op is not None and not is_dynamic_op and not isinstance(
      max_batch_size, int):
    raise ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
  rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()
  # Disable Grappler Remapper to avoid that fused OPs that may not be
  # beneficial to TF-TRT and are not supported by TF-TRT.
  rewriter_config_with_trt.remapping = False

  # Prevent folding of Const->QDQ chains.
  rewriter_config_with_trt. \
    experimental_disable_folding_quantization_emulation = (
      trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0))

  if not disable_non_trt_optimizers:
    rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])

  rewriter_config_with_trt.meta_optimizer_iterations = (
      rewriter_config_pb2.RewriterConfig.ONE)
  optimizer = rewriter_config_with_trt.custom_optimizers.add()

  if not disable_non_trt_optimizers:
    # Add a constfold optimizer to cleanup the unused Const nodes.
    rewriter_config_with_trt.custom_optimizers.add().name = "constfold"

  optimizer.name = "TensorRTOptimizer"
  optimizer.parameter_map[
      "minimum_segment_size"].i = conversion_params.minimum_segment_size
  optimizer.parameter_map["max_workspace_size_bytes"].i = (
      conversion_params.max_workspace_size_bytes)
  optimizer.parameter_map["precision_mode"].s = _to_bytes(
      conversion_params.precision_mode)
  optimizer.parameter_map[
      "maximum_cached_engines"].i = conversion_params.maximum_cached_engines
  optimizer.parameter_map[
      "use_calibration"].b = conversion_params.use_calibration
  optimizer.parameter_map["is_dynamic_op"].b = is_dynamic_op
  optimizer.parameter_map[
      "allow_build_at_runtime"].b = conversion_params.allow_build_at_runtime
  if max_batch_size is not None:
    optimizer.parameter_map["max_batch_size"].i = max_batch_size
  optimizer.parameter_map["use_implicit_batch"].b = use_implicit_batch
  # While we accept case insensitive strings from the users, we only pass the
  # strings in lower cases to TF-TRT converter.
  if not use_implicit_batch:
    optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())

  # Disabling optimizers should happen after defining the TF-TRT grappler pass
  # otherwise the template can overwrite the disablement.
  if disable_non_trt_optimizers:
    trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)

  return rewriter_config_with_trt


@deprecation.deprecated(
    None, "You shouldn't need a rewriter_config with the current TF-TRT APIs.")
def get_tensorrt_rewriter_config(conversion_params,
                                 is_dynamic_op=None,
                                 max_batch_size=None,
                                 is_v2=False,
                                 disable_non_trt_optimizers=False):
  return _get_tensorrt_rewriter_config(conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)


# Remove all scope prefixes in the node name. In TF 2.0, the same concrete
# function can be initialized multiple times with different prefixes, and
# this will result in the same TRTEngineOp being initialized multiple times
# with different cache and duplicate TRT engines.
# TODO(laigd): this may be caused by the fact that TRTEngineOp is not
# stateful, need to investigate.
# TODO(laigd): we rely on the fact that all functions are fully inlined
# before TF-TRT optimizer is called, as otherwise it may generate the same
# name when optimizing a different function graph. Fix this.
def _get_canonical_engine_name(name):
  return name.split("/")[-1]


class TrtGraphConverter(object):
  """A converter for TF-TRT transformation for TF 1.x GraphDef/SavedModels.

  To run the conversion without quantization calibration (e.g. for FP32/FP16
  precision modes):

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.FP16)
  converted_graph_def = converter.convert()
  converter.save(output_saved_model_dir)
  ```

  To run the conversion with quantization calibration:

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.INT8)
  converter.convert()

  # Run calibration 10 times.
  converted_graph_def = converter.calibrate(
      fetch_names=['output:0'],
      num_runs=10,
      feed_dict_fn=lambda: {'input:0': my_next_data()})

  converter.save(output_saved_model_dir)
  ```
  """

  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               input_graph_def=None,
               nodes_denylist=None,
               max_batch_size=1,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               is_dynamic_op=False,
               maximum_cached_engines=1,
               use_calibration=True):
    """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
    if context.executing_eagerly():
      raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")

    if input_graph_def and input_saved_model_dir:
      raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
    if not input_graph_def and not input_saved_model_dir:
      raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
    _check_trt_version_compatibility()

    self._input_graph_def = input_graph_def
    self._nodes_denylist = nodes_denylist

    self._input_saved_model_dir = input_saved_model_dir
    self._converted = False
    self._grappler_meta_graph_def = None

    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)

    # For calibration usage.
    self._calibration_graph = None
    self._calibration_data_collected = False
    self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
    if self._need_calibration and not is_dynamic_op:
      logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True

    self._is_dynamic_op = is_dynamic_op
    if is_dynamic_op:
      self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
    else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size

    self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
    _check_conversion_params(self._conversion_params)

    self._test_only_disable_non_trt_optimizers = False

  def _run_conversion(self):
    """Run Grappler's OptimizeGraph() tool to convert the graph."""
    # Create custom ConfigProto for Grappler.
    grappler_session_config = config_pb2.ConfigProto()
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)

    # Run Grappler.
    self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
    self._converted = True

  def _add_nodes_denylist(self):
    if self._nodes_denylist:
      collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))

  def _convert_graph_def(self):
    """Convert the input GraphDef."""
    graph = ops.Graph()
    with graph.as_default():
      importer.import_graph_def(self._input_graph_def, name="")
    self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
    self._add_nodes_denylist()

    self._run_conversion()

  def _collections_to_keep(self, collection_keys):
    # TODO(laigd): currently we use the collection key to filter out
    # collections that depend on variable ops, but this may miss some
    # other user-defined collections. A better way would be to use
    # CollectionDef::NodeList for the filtering.
    collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
    return [key for key in collection_keys if key not in collections_to_remove]

  def _convert_saved_model(self):
    """Convert the input SavedModel."""
    graph = ops.Graph()
    with session.Session(graph=graph) as sess:
      input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.

    self._run_conversion()

  def convert(self):
    """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
    assert not self._converted
    if self._input_graph_def:
      self._convert_graph_def()
    else:
      self._convert_saved_model()
    return self._converted_graph_def

  def calibrate(self,
                fetch_names,
                num_runs,
                feed_dict_fn=None,
                input_map_fn=None):
    """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
    assert self._converted
    assert self._need_calibration
    assert not self._calibration_data_collected

    if (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn):
      raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")

    if input_map_fn:
      for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")

    self._calibration_graph = ops.Graph()
    with self._calibration_graph.as_default():
      fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")

    calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
    if self._test_only_disable_non_trt_optimizers:
      trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)

    # Set allow_soft_placement=True to run the graph for calibration so that
    # OPs supported by TensorRT but don't have a GPU implementation are allowed
    # to execute on CPU.
    calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))

    with session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess:
      for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True

    return self._converted_graph_def

  def save(self, output_saved_model_dir):
    """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
    assert self._converted
    if self._need_calibration:
      assert self._calibration_data_collected
    if self._input_graph_def:
      raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")

    def _restore_collections(dest_graph, src_meta_graph_def, collection_keys):
      """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))

    # Write the transformed graphdef as SavedModel.
    saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
    with ops.Graph().as_default():
      importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
    # Ignore other meta graphs from the input SavedModel.
    saved_model_builder.save()

def _get_resource_handle(name, device):
  with ops.device(device):
    return gen_trt_ops.create_trt_resource_handle(resource_name=name)


def _remove_native_segments(input_func):
  """Remove native segments from the input TF-TRT Converted Function.

  Args:
    input_func: provide the concrete function with native segment nodes. The
      transformed output func will not contain any native segment nodes. All the
      TRTEngineOp references will be deleted and reset to default empty func.
  """
  input_graph_def = input_func.graph.as_graph_def()
  # Deleting the Native Segment node in each TRTEngineOp node.
  nodes_deleted = 0
  for func_id in reversed(range(len(input_graph_def.library.function))):
    f = input_graph_def.library.function[func_id]
    if "native_segment" in f.signature.name:
      nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]

  logging.info(
      "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
  )

  # Deleting the references to `<EngineName>_native_segment`s.
  # This helps TRTEngineOp constructor to not look for native segment handles
  # during construction of graph for inference.
  for node in input_graph_def.node:
    if node.op == "TRTEngineOp":
      del node.attr["segment_func"]
  for func in input_graph_def.library.function:
    for node in func.node_def:
      if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
  # Reconstruct the converted_func with the new graph
  new_func = _construct_function_from_graph_def(input_func, input_graph_def)

  return new_func


class _TRTEngineResource(resource.TrackableResource):
  """Class to track the serialized engines resource."""

  def __init__(self,
               resource_name,
               filename,
               maximum_cached_engines,
               device="GPU"):
    super(_TRTEngineResource, self).__init__(device=device)
    self._resource_name = resource_name
    # Track the serialized engine file in the SavedModel.
    self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
    self._maximum_cached_engines = maximum_cached_engines

  def _create_resource(self):
    return _get_resource_handle(self._resource_name, self._resource_device)

  def _initialize(self):
    gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)

  def _destroy_resource(self):
    handle = _get_resource_handle(self._resource_name, self._resource_device)
    with ops.device(self._resource_device):
      gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)


def _print_row(fields, positions, print_fn):
  """Prints a row."""
  line = ""
  for i, field in enumerate(fields):
    field = str(field)
    end_line_pos = positions[i]
    if i > 0:
      line = line + " "
    line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)

    if len(line) > end_line_pos:
      line = line[:(end_line_pos - 4)] + " ..."

  print_fn(line)


def _construct_function_from_graph_def(func, graph_def, frozen_func=None):
  """Rebuild function from graph_def."""
  if frozen_func is None:
    frozen_func = func

  # If a function is converted, then the TF context contains the original
  # function while the converted_graph_def contains the converted function.
  # Remove the original function from the TF context in this case.
  for f in graph_def.library.function:
    while context.context().has_function(f.signature.name):
      context.context().remove_function(f.signature.name)

  captures = {
      c[1].name.split(":")[0]: c[0]
      for c in frozen_func.graph.captures
  }
  new_func = wrap_function.function_from_graph_def(
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
  new_func.graph.structured_outputs = nest.pack_sequence_as(
      func.graph.structured_outputs, new_func.graph.structured_outputs)
  new_func._function_type = func.function_type  # pylint: disable=protected-access

  # Copy structured input signature from original function (used during
  # serialization)
  new_func.graph.structured_input_signature = (func.structured_input_signature)

  return new_func


def _apply_inlining(func):
  """Apply an inlining optimization to the function's graph definition."""
  graph_def = func.graph.as_graph_def()

  # In some cases, a secondary implementation of the function (e.g. for GPU) is
  # written to the "api_implements" attribute. (e.g. `tf.keras.layers.LSTM` in
  # TF2 produces a CuDNN-based RNN for GPU).
  # This function suppose to inline all functions calls, but "api_implements"
  # prevents this from happening. Removing the attribute solves the problem.
  # To learn more about "api_implements", see:
  #   tensorflow/core/grappler/optimizers/implementation_selector.h
  for function in graph_def.library.function:
    if "api_implements" in function.attr:
      del function.attr["api_implements"]

  meta_graph = saver.export_meta_graph(graph_def=graph_def, graph=func.graph)

  # Clear the initializer_name for the variables collections, since they are not
  # needed after saved to saved_model.
  for name in [
      "variables", "model_variables", "trainable_variables", "local_variables"
  ]:
    raw_list = []
    for raw in meta_graph.collection_def["variables"].bytes_list.value:
      variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
    meta_graph.collection_def[name].bytes_list.value[:] = raw_list

  # Add a collection 'train_op' so that Grappler knows the outputs.
  fetch_collection = meta_graph_pb2.CollectionDef()
  for array in func.inputs + func.outputs:
    fetch_collection.node_list.value.append(array.name)
  meta_graph.collection_def["train_op"].CopyFrom(fetch_collection)

  # Initialize RewriterConfig with everything disabled except function inlining.
  config = config_pb2.ConfigProto()
  rewrite_options = config.graph_options.rewrite_options
  rewrite_options.min_graph_nodes = -1  # do not skip small graphs
  rewrite_options.optimizers.append("function")

  new_graph_def = tf_optimizer.OptimizeGraph(config, meta_graph)

  return new_graph_def


def _annotate_variable_ops(func, graph_def):
  """Annotates variable operations with custom `_shape` attribute.

  This is required for the converters and shape inference. The graph
  definition is modified in-place.

  Args:
    func: Function represented by the graph definition.
    graph_def: Graph definition to be annotated in-place.

  Raises:
    RuntimeError: if some shapes cannot be annotated.
  """
  ph_shape_map = {}
  for ph, var in zip(func.graph.internal_captures, func.variables):
    ph_shape_map[ph.name] = var.shape
  # Construct a mapping of node names to nodes
  name_to_node = {node.name: node for node in graph_def.node}
  # Go through all the ReadVariableOp nodes in the graph def
  for node in graph_def.node:
    if node.op == "ReadVariableOp" or node.op == "ResourceGather":
      node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))


def _save_calibration_table(node):
  try:
    calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
    node.attr["calibration_data"].s = calibration_table.numpy()
  except (errors.UnknownError, errors.NotFoundError):
    logging.warning("Warning calibration error for %s", node.name)


def _convert_to_tensor(inp):
  try:
    if isinstance(inp, dict):
      args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
    else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
  except:
    error_msg = "Failed to convert input to tensor."
    logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
    raise RuntimeError(error_msg)

  return args, kwargs


@tf_export("experimental.tensorrt.Converter", v1=[])
class TrtGraphConverterV2(object):
  """An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  """

  def _verify_profile_strategy(self, strategy):
    supported_strategies = [s.lower() for s in supported_profile_strategies()]
    if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
    if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")

  @deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None):
    """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False

  def _need_trt_profiles(self):
    return self._use_dynamic_shape

  def _run_conversion(self, meta_graph_def):
    """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
    grappler_session_config = config_pb2.ConfigProto()
    # Always set `allow_build_at_runtime` for offline TensorRT engine building.
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
    return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")

  def _for_each_trt_node(self, graph_def, fn):
    """Helper method to manipulate all TRTEngineOps in a GraphDef."""
    for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
    for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)

  def _execute_calibration(self, calibration_input_fn):
    """Run INT8 calibration with the provided input generator function."""
    for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)

    self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)

    # Rebuild the function since calibration has changed the graph.
    self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
    self._calibrated = True

  # TODO(laigd): provide a utility function to optimize a ConcreteFunction and
  # use it here (b/124792963).
  def convert(self, calibration_input_fn=None):
    """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
    assert not self._converted

    # Creating an empty tensor to fetch queried device
    device_requested = array_ops.zeros([]).device

    if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")

    if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")

    if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
    if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")

    self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
    func = self._saved_model.signatures[self._input_saved_model_signature_key]
    if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
    frozen_graph_def = frozen_func.graph.as_graph_def()

    # Clear any prior device assignments
    logging.info("Clearing prior device assignments in loaded saved model")
    for node in frozen_graph_def.node:
      node.device = ""

    if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)

    # Add a collection 'train_op' so that Grappler knows the outputs.
    fetch_collection = meta_graph_pb2.CollectionDef()
    for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
    grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)

    # Run TRT optimizer in Grappler to convert the graph.
    self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
    self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)

    if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn

    self._converted = True

    graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
    if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )

    return self._converted_func

  def build(self, input_fn):
    """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
    if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
    if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
    if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
    if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")

    def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value

    if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func

    first_input = None
    # Run inference:
    #   Builds TRT engines if self._need_trt_profiles is False.
    #   Builds TRT optimization profiles if self._need_trt_profiles is True.
    for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)

    if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))

    # Run calibration if required, this would have been skipped in
    # the convert step
    if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)

    self._build_called_once = True

  def save(self,
           output_saved_model_dir,
           save_gpu_specific_engines=True,
           options=None):
    """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
    assert self._converted

    # 'remove_native_segments': setting this value to True removes native segments
    # associated with each TRT engine. This option can be used to reduce the size
    # of the converted model. Please note that a converted model without native
    # segments can't be used for collecting profiles, building or re-converting.
    # The reduced model can only be used for inference when no native segments
    # are required for computation. When remove_native_segments flag is set to
    # True, the converted_graph_def needs to be reduced before saved_model
    # function serialization.
    if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()

    if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
    # Serialize the TRT engines in the cache if any, and create trackable
    # resource to track them.
    engine_asset_dir = tempfile.mkdtemp()
    resource_map = {}

    def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)

    self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
    # If the graph is frozen, tracked variables are not needed by the converted model.
    trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
    trackable.trt_engine_resources = resource_map

    # Set allow_build_at_runtime=False if asked by user.
    #
    # This attribute is set here because build() needs it to be True in order to
    # build engines.
    if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func

    # Rewrite the signature map using the optimized ConcreteFunction.
    signatures = {self._input_saved_model_signature_key: self._converted_func}
    save.save(trackable, output_saved_model_dir, signatures, options=options)

  def summary(self, line_length=160, detailed=True, print_fn=None):
    """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
    if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")

    if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")

    if print_fn is None:
      print_fn = print

    # positions are percentage of `line_length`. positions[i]+1 is the starting
    # position for (i+1)th field. We also make sure that the last char printed
    # for each field is a space.
    columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]

    positions = [int(line_length * p) for _, p in columns]
    positions = np.cumsum(positions).tolist()
    headers = [h for h, _ in columns]

    _print_row(headers, positions, print_fn=print_fn)
    print_fn("=" * line_length)

    n_engines = 0
    n_ops_converted = 0
    n_ops_not_converted = 0

    graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)

    trtengineops_dict = dict()
    for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1

    for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")

    print_fn(f"\n{'='*line_length}")
    print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
    total_ops = n_ops_not_converted + n_ops_converted
    conversion_ratio = n_ops_converted / total_ops * 100
    print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")


# TODO(laigd): use TrtConversionParams here.
def create_inference_graph(
    input_graph_def,
    outputs,
    max_batch_size=1,
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32,
    minimum_segment_size=3,
    is_dynamic_op=False,
    maximum_cached_engines=1,
    input_saved_model_dir=None,
    input_saved_model_tags=None,
    input_saved_model_signature_key=None,
    output_saved_model_dir=None):
  """Python wrapper for the TRT transformation.

  Args:
    input_graph_def: a GraphDef object containing a model to be transformed. If
      set to None, the graph will be read from the SavedModel loaded from
      input_saved_model_dir.
    outputs: list of tensors or node names for the model outputs. Only used when
      input_graph_def is not None.
    max_batch_size: max size for the input batch.
    max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
      engine can use at execution time. This corresponds to the 'workspaceSize'
      parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph to
      be replaced by TRTEngineOp.
    is_dynamic_op: whether to generate dynamic TRT ops which will build the TRT
      network and engine at run time.
    maximum_cached_engines: max number of cached TRT engines in dynamic TRT ops.
      If the number of cached engines is already at max but none of them can
      serve the input, the TRTEngineOp will fall back to run the TF function
      based on which the TRTEngineOp is created.
    input_saved_model_dir: the directory to load the SavedModel which contains
      the input graph to transforms. Used only when input_graph_def is None.
    input_saved_model_tags: list of tags to load the SavedModel.
    input_saved_model_signature_key: the key of the signature to optimize the
      graph for.
    output_saved_model_dir: if not None, construct a SavedModel using the
      returned GraphDef and save it to the specified directory. This option only
      works when the input graph is loaded from a SavedModel, i.e. when
      input_saved_model_dir is specified and input_graph_def is None.

  Returns:
    A GraphDef transformed from input_graph_def (or the SavedModel graph def
    loaded from input_saved_model_dir, if input_graph_def is not present), where
    all TRT compatible subgraphs are replaced with TRTEngineOps, and a TF
    function is added for each of the subgraphs.

    If is_dynamic_op is True, each TRTEngineOp will contain a serialized
    subgraph GraphDef, which will be converted to a TRT engine at execution time
    and the TRT engine will be cached for future usage. A new TRT engine will be
    created each time when none of the cached engines match the input shapes. If
    it fails to execute the TRT engine or the number of cached engines reaches
    maximum_cached_engines, the op will fall back to call the corresponding TF
    function.

    If is_dynamic_op is False, each TRTEngineOp will contain a serialized TRT
    engine created from the corresponding subgraph. No more engines will be
    created on the fly, and the op will fall back to call the corresponding TF
    function when it fails to execute the engine.

  Raises:
    ValueError: if the combination of the parameters is invalid.
  """
  trt_converter = TrtGraphConverter(
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
  converted_graph_def = trt_converter.convert()
  if output_saved_model_dir:
    trt_converter.save(output_saved_model_dir)
  return converted_graph_def

 comment: # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 comment: #
 comment: # Licensed under the Apache License, Version 2.0 (the "License");
 comment: # you may not use this file except in compliance with the License.
 comment: # You may obtain a copy of the License at
 comment: #
 comment: #     http://www.apache.org/licenses/LICENSE-2.0
 comment: #
 comment: # Unless required by applicable law or agreed to in writing, software
 comment: # distributed under the License is distributed on an "AS IS" BASIS,
 comment: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 comment: # See the License for the specific language governing permissions and
 comment: # limitations under the License.
 comment: # =============================================================================
 expression_statement: """Exposes the Python wrapper conversion to trt_graph."""
  string: """Exposes the Python wrapper conversion to trt_graph."""
   string_start: """
   string_content: Exposes the Python wrapper conversion to trt_graph.
   string_end: """
 import_statement: import collections
  import: import
  dotted_name: collections
   identifier: collections
 import_from_statement: from functools import partial
  from: from
  dotted_name: functools
   identifier: functools
  import: import
  dotted_name: partial
   identifier: partial
 comment: # pylint: disable=g-importing-member
 import_statement: import os
  import: import
  dotted_name: os
   identifier: os
 import_statement: import platform
  import: import
  dotted_name: platform
   identifier: platform
 import_statement: import sys
  import: import
  dotted_name: sys
   identifier: sys
 import_statement: import tempfile
  import: import
  dotted_name: tempfile
   identifier: tempfile
 import_statement: import numpy as np
  import: import
  aliased_import: numpy as np
   dotted_name: numpy
    identifier: numpy
   as: as
   identifier: np
 import_statement: import six as _six
  import: import
  aliased_import: six as _six
   dotted_name: six
    identifier: six
   as: as
   identifier: _six
 import_from_statement: from tensorflow.core.framework import variable_pb2
  from: from
  dotted_name: tensorflow.core.framework
   identifier: tensorflow
   .: .
   identifier: core
   .: .
   identifier: framework
  import: import
  dotted_name: variable_pb2
   identifier: variable_pb2
 import_from_statement: from tensorflow.core.protobuf import config_pb2
  from: from
  dotted_name: tensorflow.core.protobuf
   identifier: tensorflow
   .: .
   identifier: core
   .: .
   identifier: protobuf
  import: import
  dotted_name: config_pb2
   identifier: config_pb2
 import_from_statement: from tensorflow.core.protobuf import meta_graph_pb2
  from: from
  dotted_name: tensorflow.core.protobuf
   identifier: tensorflow
   .: .
   identifier: core
   .: .
   identifier: protobuf
  import: import
  dotted_name: meta_graph_pb2
   identifier: meta_graph_pb2
 import_from_statement: from tensorflow.core.protobuf import rewriter_config_pb2
  from: from
  dotted_name: tensorflow.core.protobuf
   identifier: tensorflow
   .: .
   identifier: core
   .: .
   identifier: protobuf
  import: import
  dotted_name: rewriter_config_pb2
   identifier: rewriter_config_pb2
 import_from_statement: from tensorflow.python.client import session
  from: from
  dotted_name: tensorflow.python.client
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: client
  import: import
  dotted_name: session
   identifier: session
 import_from_statement: from tensorflow.python.compiler.tensorrt import utils as trt_utils
  from: from
  dotted_name: tensorflow.python.compiler.tensorrt
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: compiler
   .: .
   identifier: tensorrt
  import: import
  aliased_import: utils as trt_utils
   dotted_name: utils
    identifier: utils
   as: as
   identifier: trt_utils
 import_from_statement: from tensorflow.python.eager import context
  from: from
  dotted_name: tensorflow.python.eager
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: eager
  import: import
  dotted_name: context
   identifier: context
 import_from_statement: from tensorflow.python.eager import wrap_function
  from: from
  dotted_name: tensorflow.python.eager
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: eager
  import: import
  dotted_name: wrap_function
   identifier: wrap_function
 import_from_statement: from tensorflow.python.framework import convert_to_constants
  from: from
  dotted_name: tensorflow.python.framework
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: framework
  import: import
  dotted_name: convert_to_constants
   identifier: convert_to_constants
 import_from_statement: from tensorflow.python.framework import dtypes
  from: from
  dotted_name: tensorflow.python.framework
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: framework
  import: import
  dotted_name: dtypes
   identifier: dtypes
 import_from_statement: from tensorflow.python.framework import errors
  from: from
  dotted_name: tensorflow.python.framework
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: framework
  import: import
  dotted_name: errors
   identifier: errors
 import_from_statement: from tensorflow.python.framework import importer
  from: from
  dotted_name: tensorflow.python.framework
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: framework
  import: import
  dotted_name: importer
   identifier: importer
 import_from_statement: from tensorflow.python.framework import ops
  from: from
  dotted_name: tensorflow.python.framework
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: framework
  import: import
  dotted_name: ops
   identifier: ops
 import_from_statement: from tensorflow.python.framework import tensor
  from: from
  dotted_name: tensorflow.python.framework
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: framework
  import: import
  dotted_name: tensor
   identifier: tensor
 import_from_statement: from tensorflow.python.grappler import tf_optimizer
  from: from
  dotted_name: tensorflow.python.grappler
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: grappler
  import: import
  dotted_name: tf_optimizer
   identifier: tf_optimizer
 import_from_statement: from tensorflow.python.ops import array_ops
  from: from
  dotted_name: tensorflow.python.ops
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: ops
  import: import
  dotted_name: array_ops
   identifier: array_ops
 import_from_statement: from tensorflow.python.ops import gen_resource_variable_ops
  from: from
  dotted_name: tensorflow.python.ops
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: ops
  import: import
  dotted_name: gen_resource_variable_ops
   identifier: gen_resource_variable_ops
 import_from_statement: from tensorflow.python.platform import tf_logging as logging
  from: from
  dotted_name: tensorflow.python.platform
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: platform
  import: import
  aliased_import: tf_logging as logging
   dotted_name: tf_logging
    identifier: tf_logging
   as: as
   identifier: logging
 import_from_statement: from tensorflow.python.saved_model import builder
  from: from
  dotted_name: tensorflow.python.saved_model
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: saved_model
  import: import
  dotted_name: builder
   identifier: builder
 import_from_statement: from tensorflow.python.saved_model import load
  from: from
  dotted_name: tensorflow.python.saved_model
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: saved_model
  import: import
  dotted_name: load
   identifier: load
 import_from_statement: from tensorflow.python.saved_model import loader
  from: from
  dotted_name: tensorflow.python.saved_model
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: saved_model
  import: import
  dotted_name: loader
   identifier: loader
 import_from_statement: from tensorflow.python.saved_model import save
  from: from
  dotted_name: tensorflow.python.saved_model
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: saved_model
  import: import
  dotted_name: save
   identifier: save
 import_from_statement: from tensorflow.python.saved_model import signature_constants
  from: from
  dotted_name: tensorflow.python.saved_model
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: saved_model
  import: import
  dotted_name: signature_constants
   identifier: signature_constants
 import_from_statement: from tensorflow.python.saved_model import tag_constants
  from: from
  dotted_name: tensorflow.python.saved_model
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: saved_model
  import: import
  dotted_name: tag_constants
   identifier: tag_constants
 import_from_statement: from tensorflow.python.trackable import asset
  from: from
  dotted_name: tensorflow.python.trackable
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: trackable
  import: import
  dotted_name: asset
   identifier: asset
 import_from_statement: from tensorflow.python.trackable import autotrackable
  from: from
  dotted_name: tensorflow.python.trackable
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: trackable
  import: import
  dotted_name: autotrackable
   identifier: autotrackable
 import_from_statement: from tensorflow.python.trackable import resource
  from: from
  dotted_name: tensorflow.python.trackable
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: trackable
  import: import
  dotted_name: resource
   identifier: resource
 import_from_statement: from tensorflow.python.training import saver
  from: from
  dotted_name: tensorflow.python.training
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: training
  import: import
  dotted_name: saver
   identifier: saver
 import_from_statement: from tensorflow.python.util import deprecation
  from: from
  dotted_name: tensorflow.python.util
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: util
  import: import
  dotted_name: deprecation
   identifier: deprecation
 import_from_statement: from tensorflow.python.util import nest
  from: from
  dotted_name: tensorflow.python.util
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: util
  import: import
  dotted_name: nest
   identifier: nest
 import_from_statement: from tensorflow.python.util.lazy_loader import LazyLoader
  from: from
  dotted_name: tensorflow.python.util.lazy_loader
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: util
   .: .
   identifier: lazy_loader
  import: import
  dotted_name: LazyLoader
   identifier: LazyLoader
 import_from_statement: from tensorflow.python.util.tf_export import tf_export
  from: from
  dotted_name: tensorflow.python.util.tf_export
   identifier: tensorflow
   .: .
   identifier: python
   .: .
   identifier: util
   .: .
   identifier: tf_export
  import: import
  dotted_name: tf_export
   identifier: tf_export
 comment: # Lazily load the op, since it's not available in cpu-only builds. Importing
 comment: # this at top will cause tests that imports TF-TRT fail when they're built
 comment: # and run without CUDA/GPU.
 expression_statement: gen_trt_ops = LazyLoader(
    "gen_trt_ops", globals(),
    "tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops")
  assignment: gen_trt_ops = LazyLoader(
    "gen_trt_ops", globals(),
    "tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops")
   identifier: gen_trt_ops
   =: =
   call: LazyLoader(
    "gen_trt_ops", globals(),
    "tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops")
    identifier: LazyLoader
    argument_list: (
    "gen_trt_ops", globals(),
    "tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops")
     (: (
     string: "gen_trt_ops"
      string_start: "
      string_content: gen_trt_ops
      string_end: "
     ,: ,
     call: globals()
      identifier: globals
      argument_list: ()
       (: (
       ): )
     ,: ,
     string: "tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops"
      string_start: "
      string_content: tensorflow.compiler.tf2tensorrt.ops.gen_trt_ops
      string_end: "
     ): )
 expression_statement: _pywrap_py_utils = LazyLoader(
    "_pywrap_py_utils", globals(),
    "tensorflow.compiler.tf2tensorrt._pywrap_py_utils")
  assignment: _pywrap_py_utils = LazyLoader(
    "_pywrap_py_utils", globals(),
    "tensorflow.compiler.tf2tensorrt._pywrap_py_utils")
   identifier: _pywrap_py_utils
   =: =
   call: LazyLoader(
    "_pywrap_py_utils", globals(),
    "tensorflow.compiler.tf2tensorrt._pywrap_py_utils")
    identifier: LazyLoader
    argument_list: (
    "_pywrap_py_utils", globals(),
    "tensorflow.compiler.tf2tensorrt._pywrap_py_utils")
     (: (
     string: "_pywrap_py_utils"
      string_start: "
      string_content: _pywrap_py_utils
      string_end: "
     ,: ,
     call: globals()
      identifier: globals
      argument_list: ()
       (: (
       ): )
     ,: ,
     string: "tensorflow.compiler.tf2tensorrt._pywrap_py_utils"
      string_start: "
      string_content: tensorflow.compiler.tf2tensorrt._pywrap_py_utils
      string_end: "
     ): )
 comment: # Register TRT ops in python, so that when users import this module they can
 comment: # execute a TRT-converted graph without calling any of the methods in this
 comment: # module.
 comment: #
 comment: # This will call register_op_list() in
 comment: # tensorflow/python/framework/op_def_registry.py, but it doesn't register
 comment: # the op or the op kernel in C++ runtime.
 try_statement: try:
  gen_trt_ops.trt_engine_op  # pylint: disable=pointless-statement
except AttributeError:
  pass
  try: try
  :: :
  block: gen_trt_ops.trt_engine_op  # pylint: disable=pointless-statement
   expression_statement: gen_trt_ops.trt_engine_op
    attribute: gen_trt_ops.trt_engine_op
     identifier: gen_trt_ops
     .: .
     identifier: trt_engine_op
   comment: # pylint: disable=pointless-statement
  except_clause: except AttributeError:
  pass
   except: except
   identifier: AttributeError
   :: :
   block: pass
    pass_statement: pass
     pass: pass
 function_definition: def _to_bytes(s):
  """Encode s if it is a sequence of chars."""
  if isinstance(s, _six.text_type):
    return s.encode("utf-8", errors="surrogateescape")
  return s
  def: def
  identifier: _to_bytes
  parameters: (s)
   (: (
   identifier: s
   ): )
  :: :
  block: """Encode s if it is a sequence of chars."""
  if isinstance(s, _six.text_type):
    return s.encode("utf-8", errors="surrogateescape")
  return s
   expression_statement: """Encode s if it is a sequence of chars."""
    string: """Encode s if it is a sequence of chars."""
     string_start: """
     string_content: Encode s if it is a sequence of chars.
     string_end: """
   if_statement: if isinstance(s, _six.text_type):
    return s.encode("utf-8", errors="surrogateescape")
    if: if
    call: isinstance(s, _six.text_type)
     identifier: isinstance
     argument_list: (s, _six.text_type)
      (: (
      identifier: s
      ,: ,
      attribute: _six.text_type
       identifier: _six
       .: .
       identifier: text_type
      ): )
    :: :
    block: return s.encode("utf-8", errors="surrogateescape")
     return_statement: return s.encode("utf-8", errors="surrogateescape")
      return: return
      call: s.encode("utf-8", errors="surrogateescape")
       attribute: s.encode
        identifier: s
        .: .
        identifier: encode
       argument_list: ("utf-8", errors="surrogateescape")
        (: (
        string: "utf-8"
         string_start: "
         string_content: utf-8
         string_end: "
        ,: ,
        keyword_argument: errors="surrogateescape"
         identifier: errors
         =: =
         string: "surrogateescape"
          string_start: "
          string_content: surrogateescape
          string_end: "
        ): )
   return_statement: return s
    return: return
    identifier: s
 function_definition: def _to_string(s):
  """Decode s if it is a sequence of bytes."""
  if isinstance(s, _six.binary_type):
    return s.decode("utf-8")
  return s
  def: def
  identifier: _to_string
  parameters: (s)
   (: (
   identifier: s
   ): )
  :: :
  block: """Decode s if it is a sequence of bytes."""
  if isinstance(s, _six.binary_type):
    return s.decode("utf-8")
  return s
   expression_statement: """Decode s if it is a sequence of bytes."""
    string: """Decode s if it is a sequence of bytes."""
     string_start: """
     string_content: Decode s if it is a sequence of bytes.
     string_end: """
   if_statement: if isinstance(s, _six.binary_type):
    return s.decode("utf-8")
    if: if
    call: isinstance(s, _six.binary_type)
     identifier: isinstance
     argument_list: (s, _six.binary_type)
      (: (
      identifier: s
      ,: ,
      attribute: _six.binary_type
       identifier: _six
       .: .
       identifier: binary_type
      ): )
    :: :
    block: return s.decode("utf-8")
     return_statement: return s.decode("utf-8")
      return: return
      call: s.decode("utf-8")
       attribute: s.decode
        identifier: s
        .: .
        identifier: decode
       argument_list: ("utf-8")
        (: (
        string: "utf-8"
         string_start: "
         string_content: utf-8
         string_end: "
        ): )
   return_statement: return s
    return: return
    identifier: s
 class_definition: class TrtPrecisionMode(object):
  FP32 = "FP32"
  FP16 = "FP16"
  INT8 = "INT8"

  @staticmethod
  def supported_precision_modes():
    precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
    return precisions + [p.lower() for p in precisions]
  class: class
  identifier: TrtPrecisionMode
  argument_list: (object)
   (: (
   identifier: object
   ): )
  :: :
  block: FP32 = "FP32"
  FP16 = "FP16"
  INT8 = "INT8"

  @staticmethod
  def supported_precision_modes():
    precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
    return precisions + [p.lower() for p in precisions]
   expression_statement: FP32 = "FP32"
    assignment: FP32 = "FP32"
     identifier: FP32
     =: =
     string: "FP32"
      string_start: "
      string_content: FP32
      string_end: "
   expression_statement: FP16 = "FP16"
    assignment: FP16 = "FP16"
     identifier: FP16
     =: =
     string: "FP16"
      string_start: "
      string_content: FP16
      string_end: "
   expression_statement: INT8 = "INT8"
    assignment: INT8 = "INT8"
     identifier: INT8
     =: =
     string: "INT8"
      string_start: "
      string_content: INT8
      string_end: "
   decorated_definition: @staticmethod
  def supported_precision_modes():
    precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
    return precisions + [p.lower() for p in precisions]
    decorator: @staticmethod
     @: @
     identifier: staticmethod
    function_definition: def supported_precision_modes():
    precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
    return precisions + [p.lower() for p in precisions]
     def: def
     identifier: supported_precision_modes
     parameters: ()
      (: (
      ): )
     :: :
     block: precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
    return precisions + [p.lower() for p in precisions]
      expression_statement: precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
       assignment: precisions = [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
        identifier: precisions
        =: =
        list: [
        TrtPrecisionMode.FP32, TrtPrecisionMode.FP16, TrtPrecisionMode.INT8
    ]
         [: [
         attribute: TrtPrecisionMode.FP32
          identifier: TrtPrecisionMode
          .: .
          identifier: FP32
         ,: ,
         attribute: TrtPrecisionMode.FP16
          identifier: TrtPrecisionMode
          .: .
          identifier: FP16
         ,: ,
         attribute: TrtPrecisionMode.INT8
          identifier: TrtPrecisionMode
          .: .
          identifier: INT8
         ]: ]
      return_statement: return precisions + [p.lower() for p in precisions]
       return: return
       binary_operator: precisions + [p.lower() for p in precisions]
        identifier: precisions
        +: +
        list_comprehension: [p.lower() for p in precisions]
         [: [
         call: p.lower()
          attribute: p.lower
           identifier: p
           .: .
           identifier: lower
          argument_list: ()
           (: (
           ): )
         for_in_clause: for p in precisions
          for: for
          identifier: p
          in: in
          identifier: precisions
         ]: ]
 comment: # Use a large enough number as the default max_workspace_size for TRT engines,
 comment: # so it can produce reasonable performance results with the default.
 comment: # For TRT >= 8.4, the recommendation is MAX_INT.
 if_statement: if (_pywrap_py_utils.is_tensorrt_enabled() and
    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)):
  # We must use `sys.maxsize - 512` to avoid overflow during casting.
  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512
else:
  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30  # 1,073,741,824
  if: if
  parenthesized_expression: (_pywrap_py_utils.is_tensorrt_enabled() and
    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0))
   (: (
   boolean_operator: _pywrap_py_utils.is_tensorrt_enabled() and
    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)
    call: _pywrap_py_utils.is_tensorrt_enabled()
     attribute: _pywrap_py_utils.is_tensorrt_enabled
      identifier: _pywrap_py_utils
      .: .
      identifier: is_tensorrt_enabled
     argument_list: ()
      (: (
      ): )
    and: and
    call: trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)
     attribute: trt_utils.is_loaded_tensorrt_version_greater_equal
      identifier: trt_utils
      .: .
      identifier: is_loaded_tensorrt_version_greater_equal
     argument_list: (8, 4, 0)
      (: (
      integer: 8
      ,: ,
      integer: 4
      ,: ,
      integer: 0
      ): )
   ): )
  :: :
  comment: # We must use `sys.maxsize - 512` to avoid overflow during casting.
  block: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512
   expression_statement: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512
    assignment: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512
     identifier: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
     =: =
     binary_operator: sys.maxsize - 512
      attribute: sys.maxsize
       identifier: sys
       .: .
       identifier: maxsize
      -: -
      integer: 512
  else_clause: else:
  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30  # 1,073,741,824
   else: else
   :: :
   block: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30  # 1,073,741,824
    expression_statement: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30
     assignment: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30
      identifier: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
      =: =
      binary_operator: 1 << 30
       integer: 1
       <<: <<
       integer: 30
    comment: # 1,073,741,824
 expression_statement: PROFILE_STRATEGY_RANGE = "Range"
  assignment: PROFILE_STRATEGY_RANGE = "Range"
   identifier: PROFILE_STRATEGY_RANGE
   =: =
   string: "Range"
    string_start: "
    string_content: Range
    string_end: "
 expression_statement: PROFILE_STRATEGY_OPTIMAL = "Optimal"
  assignment: PROFILE_STRATEGY_OPTIMAL = "Optimal"
   identifier: PROFILE_STRATEGY_OPTIMAL
   =: =
   string: "Optimal"
    string_start: "
    string_content: Optimal
    string_end: "
 expression_statement: PROFILE_STRATEGY_RANGE_OPTIMAL = "Range+Optimal"
  assignment: PROFILE_STRATEGY_RANGE_OPTIMAL = "Range+Optimal"
   identifier: PROFILE_STRATEGY_RANGE_OPTIMAL
   =: =
   string: "Range+Optimal"
    string_start: "
    string_content: Range+Optimal
    string_end: "
 expression_statement: PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE = "ImplicitBatchModeCompatible"
  assignment: PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE = "ImplicitBatchModeCompatible"
   identifier: PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
   =: =
   string: "ImplicitBatchModeCompatible"
    string_start: "
    string_content: ImplicitBatchModeCompatible
    string_end: "
 function_definition: def supported_profile_strategies():
  return [
      PROFILE_STRATEGY_RANGE, PROFILE_STRATEGY_OPTIMAL,
      PROFILE_STRATEGY_RANGE_OPTIMAL,
      PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
  ]
  def: def
  identifier: supported_profile_strategies
  parameters: ()
   (: (
   ): )
  :: :
  block: return [
      PROFILE_STRATEGY_RANGE, PROFILE_STRATEGY_OPTIMAL,
      PROFILE_STRATEGY_RANGE_OPTIMAL,
      PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
  ]
   return_statement: return [
      PROFILE_STRATEGY_RANGE, PROFILE_STRATEGY_OPTIMAL,
      PROFILE_STRATEGY_RANGE_OPTIMAL,
      PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
  ]
    return: return
    list: [
      PROFILE_STRATEGY_RANGE, PROFILE_STRATEGY_OPTIMAL,
      PROFILE_STRATEGY_RANGE_OPTIMAL,
      PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
  ]
     [: [
     identifier: PROFILE_STRATEGY_RANGE
     ,: ,
     identifier: PROFILE_STRATEGY_OPTIMAL
     ,: ,
     identifier: PROFILE_STRATEGY_RANGE_OPTIMAL
     ,: ,
     identifier: PROFILE_STRATEGY_IMPLICIT_BATCH_MODE_COMPATIBLE
     ]: ]
 decorated_definition: @tf_export("experimental.tensorrt.ConversionParams", v1=[])
class TrtConversionParams(
    collections.namedtuple("TrtConversionParams", [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ])):
  """Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  """

  def __new__(cls,
              max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
              precision_mode=TrtPrecisionMode.FP32,
              minimum_segment_size=3,
              maximum_cached_engines=1,
              use_calibration=True,
              allow_build_at_runtime=True):
    return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
  decorator: @tf_export("experimental.tensorrt.ConversionParams", v1=[])
   @: @
   call: tf_export("experimental.tensorrt.ConversionParams", v1=[])
    identifier: tf_export
    argument_list: ("experimental.tensorrt.ConversionParams", v1=[])
     (: (
     string: "experimental.tensorrt.ConversionParams"
      string_start: "
      string_content: experimental.tensorrt.ConversionParams
      string_end: "
     ,: ,
     keyword_argument: v1=[]
      identifier: v1
      =: =
      list: []
       [: [
       ]: ]
     ): )
  class_definition: class TrtConversionParams(
    collections.namedtuple("TrtConversionParams", [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ])):
  """Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  """

  def __new__(cls,
              max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
              precision_mode=TrtPrecisionMode.FP32,
              minimum_segment_size=3,
              maximum_cached_engines=1,
              use_calibration=True,
              allow_build_at_runtime=True):
    return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
   class: class
   identifier: TrtConversionParams
   argument_list: (
    collections.namedtuple("TrtConversionParams", [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ]))
    (: (
    call: collections.namedtuple("TrtConversionParams", [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ])
     attribute: collections.namedtuple
      identifier: collections
      .: .
      identifier: namedtuple
     argument_list: ("TrtConversionParams", [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ])
      (: (
      string: "TrtConversionParams"
       string_start: "
       string_content: TrtConversionParams
       string_end: "
      ,: ,
      list: [
        "max_workspace_size_bytes", "precision_mode", "minimum_segment_size",
        "maximum_cached_engines", "use_calibration", "allow_build_at_runtime"
    ]
       [: [
       string: "max_workspace_size_bytes"
        string_start: "
        string_content: max_workspace_size_bytes
        string_end: "
       ,: ,
       string: "precision_mode"
        string_start: "
        string_content: precision_mode
        string_end: "
       ,: ,
       string: "minimum_segment_size"
        string_start: "
        string_content: minimum_segment_size
        string_end: "
       ,: ,
       string: "maximum_cached_engines"
        string_start: "
        string_content: maximum_cached_engines
        string_end: "
       ,: ,
       string: "use_calibration"
        string_start: "
        string_content: use_calibration
        string_end: "
       ,: ,
       string: "allow_build_at_runtime"
        string_start: "
        string_content: allow_build_at_runtime
        string_end: "
       ]: ]
      ): )
    ): )
   :: :
   block: """Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  """

  def __new__(cls,
              max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
              precision_mode=TrtPrecisionMode.FP32,
              minimum_segment_size=3,
              maximum_cached_engines=1,
              use_calibration=True,
              allow_build_at_runtime=True):
    return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
    expression_statement: """Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  """
     string: """Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  """
      string_start: """
      string_content: Parameters that are used for TF-TRT conversion.

  Fields:
    max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
      engine can use at execution time. This corresponds to the
      'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of the strings in
      TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph
      to be replaced by TRTEngineOp.
    maximum_cached_engines: max number of cached TRT engines for dynamic TRT
      ops. Created TRT engines for a dynamic dimension are cached. If the
      number of cached engines is already at max but none of them supports the
      input shapes, the TRTEngineOp will fall back to run the original TF
      subgraph that corresponds to the TRTEngineOp.
    use_calibration: this argument is ignored if precision_mode is not INT8.
      If set to True, a calibration graph will be created to calibrate the
      missing ranges. The calibration graph must be converted to an inference
      graph by running calibration with calibrate(). If set to False,
      quantization nodes will be expected for every tensor in the graph
      (excluding those which will be fused). If a range is missing, an error
      will occur. Please note that accuracy may be negatively affected if
      there is a mismatch between which tensors TRT quantizes and which
      tensors were trained with fake quantization.
    allow_build_at_runtime: whether to allow building TensorRT engines during
      runtime if no prebuilt TensorRT engine can be found that can handle the
      given inputs during runtime, then a new TensorRT engine is built at
      runtime if allow_build_at_runtime=True, and otherwise native TF is used.
  
      string_end: """
    function_definition: def __new__(cls,
              max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
              precision_mode=TrtPrecisionMode.FP32,
              minimum_segment_size=3,
              maximum_cached_engines=1,
              use_calibration=True,
              allow_build_at_runtime=True):
    return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
     def: def
     identifier: __new__
     parameters: (cls,
              max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
              precision_mode=TrtPrecisionMode.FP32,
              minimum_segment_size=3,
              maximum_cached_engines=1,
              use_calibration=True,
              allow_build_at_runtime=True)
      (: (
      identifier: cls
      ,: ,
      default_parameter: max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
       identifier: max_workspace_size_bytes
       =: =
       identifier: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
      ,: ,
      default_parameter: precision_mode=TrtPrecisionMode.FP32
       identifier: precision_mode
       =: =
       attribute: TrtPrecisionMode.FP32
        identifier: TrtPrecisionMode
        .: .
        identifier: FP32
      ,: ,
      default_parameter: minimum_segment_size=3
       identifier: minimum_segment_size
       =: =
       integer: 3
      ,: ,
      default_parameter: maximum_cached_engines=1
       identifier: maximum_cached_engines
       =: =
       integer: 1
      ,: ,
      default_parameter: use_calibration=True
       identifier: use_calibration
       =: =
       true: True
      ,: ,
      default_parameter: allow_build_at_runtime=True
       identifier: allow_build_at_runtime
       =: =
       true: True
      ): )
     :: :
     block: return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
      return_statement: return super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
       return: return
       call: super(TrtConversionParams,
                 cls).__new__(cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
        attribute: super(TrtConversionParams,
                 cls).__new__
         call: super(TrtConversionParams,
                 cls)
          identifier: super
          argument_list: (TrtConversionParams,
                 cls)
           (: (
           identifier: TrtConversionParams
           ,: ,
           identifier: cls
           ): )
         .: .
         identifier: __new__
        argument_list: (cls, max_workspace_size_bytes, precision_mode,
                              minimum_segment_size, maximum_cached_engines,
                              use_calibration, allow_build_at_runtime)
         (: (
         identifier: cls
         ,: ,
         identifier: max_workspace_size_bytes
         ,: ,
         identifier: precision_mode
         ,: ,
         identifier: minimum_segment_size
         ,: ,
         identifier: maximum_cached_engines
         ,: ,
         identifier: use_calibration
         ,: ,
         identifier: allow_build_at_runtime
         ): )
 expression_statement: DEFAULT_TRT_CONVERSION_PARAMS = TrtConversionParams()
  assignment: DEFAULT_TRT_CONVERSION_PARAMS = TrtConversionParams()
   identifier: DEFAULT_TRT_CONVERSION_PARAMS
   =: =
   call: TrtConversionParams()
    identifier: TrtConversionParams
    argument_list: ()
     (: (
     ): )
 expression_statement: _TRT_ENGINE_OP_NAME = "TRTEngineOp"
  assignment: _TRT_ENGINE_OP_NAME = "TRTEngineOp"
   identifier: _TRT_ENGINE_OP_NAME
   =: =
   string: "TRTEngineOp"
    string_start: "
    string_content: TRTEngineOp
    string_end: "
 function_definition: def _check_conversion_params(conversion_params, is_v2=False):
  """Validate the provided TrtConversionParams.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
  supported_precision_modes = TrtPrecisionMode.supported_precision_modes()
  if conversion_params.precision_mode not in supported_precision_modes:
    raise ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
  if (conversion_params.minimum_segment_size <= 0 and
      conversion_params.minimum_segment_size != -1):
    raise ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
  def: def
  identifier: _check_conversion_params
  parameters: (conversion_params, is_v2=False)
   (: (
   identifier: conversion_params
   ,: ,
   default_parameter: is_v2=False
    identifier: is_v2
    =: =
    false: False
   ): )
  :: :
  block: """Validate the provided TrtConversionParams.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
  supported_precision_modes = TrtPrecisionMode.supported_precision_modes()
  if conversion_params.precision_mode not in supported_precision_modes:
    raise ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
  if (conversion_params.minimum_segment_size <= 0 and
      conversion_params.minimum_segment_size != -1):
    raise ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
   expression_statement: """Validate the provided TrtConversionParams.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
    string: """Validate the provided TrtConversionParams.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
     string_start: """
     string_content: Validate the provided TrtConversionParams.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  
     string_end: """
   expression_statement: supported_precision_modes = TrtPrecisionMode.supported_precision_modes()
    assignment: supported_precision_modes = TrtPrecisionMode.supported_precision_modes()
     identifier: supported_precision_modes
     =: =
     call: TrtPrecisionMode.supported_precision_modes()
      attribute: TrtPrecisionMode.supported_precision_modes
       identifier: TrtPrecisionMode
       .: .
       identifier: supported_precision_modes
      argument_list: ()
       (: (
       ): )
   if_statement: if conversion_params.precision_mode not in supported_precision_modes:
    raise ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
    if: if
    comparison_operator: conversion_params.precision_mode not in supported_precision_modes
     attribute: conversion_params.precision_mode
      identifier: conversion_params
      .: .
      identifier: precision_mode
     not in: not in
      not: not
      in: in
     identifier: supported_precision_modes
    :: :
    block: raise ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
     raise_statement: raise ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
      raise: raise
      call: ValueError(
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
       identifier: ValueError
       argument_list: (
        ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes))
        (: (
        call: ("precision mode '{}' is not supported."
         "It should be one of {}").format(conversion_params.precision_mode,
                                          supported_precision_modes)
         attribute: ("precision mode '{}' is not supported."
         "It should be one of {}").format
          parenthesized_expression: ("precision mode '{}' is not supported."
         "It should be one of {}")
           (: (
           concatenated_string: "precision mode '{}' is not supported."
         "It should be one of {}"
            string: "precision mode '{}' is not supported."
             string_start: "
             string_content: precision mode '{}' is not supported.
             string_end: "
            string: "It should be one of {}"
             string_start: "
             string_content: It should be one of {}
             string_end: "
           ): )
          .: .
          identifier: format
         argument_list: (conversion_params.precision_mode,
                                          supported_precision_modes)
          (: (
          attribute: conversion_params.precision_mode
           identifier: conversion_params
           .: .
           identifier: precision_mode
          ,: ,
          identifier: supported_precision_modes
          ): )
        ): )
   if_statement: if (conversion_params.minimum_segment_size <= 0 and
      conversion_params.minimum_segment_size != -1):
    raise ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
    if: if
    parenthesized_expression: (conversion_params.minimum_segment_size <= 0 and
      conversion_params.minimum_segment_size != -1)
     (: (
     boolean_operator: conversion_params.minimum_segment_size <= 0 and
      conversion_params.minimum_segment_size != -1
      comparison_operator: conversion_params.minimum_segment_size <= 0
       attribute: conversion_params.minimum_segment_size
        identifier: conversion_params
        .: .
        identifier: minimum_segment_size
       <=: <=
       integer: 0
      and: and
      comparison_operator: conversion_params.minimum_segment_size != -1
       attribute: conversion_params.minimum_segment_size
        identifier: conversion_params
        .: .
        identifier: minimum_segment_size
       !=: !=
       unary_operator: -1
        -: -
        integer: 1
     ): )
    :: :
    block: raise ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
     raise_statement: raise ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
      raise: raise
      call: ValueError("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
       identifier: ValueError
       argument_list: ("minimum segment size should be positive or -1 "
                     "(to disable main graph conversion).")
        (: (
        concatenated_string: "minimum segment size should be positive or -1 "
                     "(to disable main graph conversion)."
         string: "minimum segment size should be positive or -1 "
          string_start: "
          string_content: minimum segment size should be positive or -1 
          string_end: "
         string: "(to disable main graph conversion)."
          string_start: "
          string_content: (to disable main graph conversion).
          string_end: "
        ): )
 function_definition: def _check_trt_version_compatibility():
  """Check compatibility of TensorRT version.

  Raises:
    RuntimeError: if the TensorRT library version is incompatible.
  """

  if not _pywrap_py_utils.is_tensorrt_enabled():
    logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")

    raise RuntimeError("Tensorflow has not been built with TensorRT support.")

  if platform.system() == "Windows":
    logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")

  linked_version = _pywrap_py_utils.get_linked_tensorrt_version()
  loaded_version = _pywrap_py_utils.get_loaded_tensorrt_version()

  logging.info("Linked TensorRT version: %s", str(linked_version))
  logging.info("Loaded TensorRT version: %s", str(loaded_version))

  def raise_trt_version_deprecated(version_type, trt_version):
    assert version_type in [
        "linked", "loaded"
    ], ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type

    logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))

    raise RuntimeError("Incompatible %s TensorRT versions" % version_type)

  if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("linked", linked_version)

  if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("loaded", loaded_version)

  if (loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):
    logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
    raise RuntimeError("Incompatible TensorRT major version")

  elif loaded_version != linked_version:
    logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
  def: def
  identifier: _check_trt_version_compatibility
  parameters: ()
   (: (
   ): )
  :: :
  block: """Check compatibility of TensorRT version.

  Raises:
    RuntimeError: if the TensorRT library version is incompatible.
  """

  if not _pywrap_py_utils.is_tensorrt_enabled():
    logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")

    raise RuntimeError("Tensorflow has not been built with TensorRT support.")

  if platform.system() == "Windows":
    logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")

  linked_version = _pywrap_py_utils.get_linked_tensorrt_version()
  loaded_version = _pywrap_py_utils.get_loaded_tensorrt_version()

  logging.info("Linked TensorRT version: %s", str(linked_version))
  logging.info("Loaded TensorRT version: %s", str(loaded_version))

  def raise_trt_version_deprecated(version_type, trt_version):
    assert version_type in [
        "linked", "loaded"
    ], ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type

    logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))

    raise RuntimeError("Incompatible %s TensorRT versions" % version_type)

  if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("linked", linked_version)

  if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("loaded", loaded_version)

  if (loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):
    logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
    raise RuntimeError("Incompatible TensorRT major version")

  elif loaded_version != linked_version:
    logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
   expression_statement: """Check compatibility of TensorRT version.

  Raises:
    RuntimeError: if the TensorRT library version is incompatible.
  """
    string: """Check compatibility of TensorRT version.

  Raises:
    RuntimeError: if the TensorRT library version is incompatible.
  """
     string_start: """
     string_content: Check compatibility of TensorRT version.

  Raises:
    RuntimeError: if the TensorRT library version is incompatible.
  
     string_end: """
   if_statement: if not _pywrap_py_utils.is_tensorrt_enabled():
    logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")

    raise RuntimeError("Tensorflow has not been built with TensorRT support.")
    if: if
    not_operator: not _pywrap_py_utils.is_tensorrt_enabled()
     not: not
     call: _pywrap_py_utils.is_tensorrt_enabled()
      attribute: _pywrap_py_utils.is_tensorrt_enabled
       identifier: _pywrap_py_utils
       .: .
       identifier: is_tensorrt_enabled
      argument_list: ()
       (: (
       ): )
    :: :
    block: logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")

    raise RuntimeError("Tensorflow has not been built with TensorRT support.")
     expression_statement: logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")
      call: logging.error(
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")
       attribute: logging.error
        identifier: logging
        .: .
        identifier: error
       argument_list: (
        "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate.")
        (: (
        concatenated_string: "Tensorflow needs to be built with TensorRT support enabled to allow "
        "TF-TRT to operate."
         string: "Tensorflow needs to be built with TensorRT support enabled to allow "
          string_start: "
          string_content: Tensorflow needs to be built with TensorRT support enabled to allow 
          string_end: "
         string: "TF-TRT to operate."
          string_start: "
          string_content: TF-TRT to operate.
          string_end: "
        ): )
     raise_statement: raise RuntimeError("Tensorflow has not been built with TensorRT support.")
      raise: raise
      call: RuntimeError("Tensorflow has not been built with TensorRT support.")
       identifier: RuntimeError
       argument_list: ("Tensorflow has not been built with TensorRT support.")
        (: (
        string: "Tensorflow has not been built with TensorRT support."
         string_start: "
         string_content: Tensorflow has not been built with TensorRT support.
         string_end: "
        ): )
   if_statement: if platform.system() == "Windows":
    logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")
    if: if
    comparison_operator: platform.system() == "Windows"
     call: platform.system()
      attribute: platform.system
       identifier: platform
       .: .
       identifier: system
      argument_list: ()
       (: (
       ): )
     ==: ==
     string: "Windows"
      string_start: "
      string_content: Windows
      string_end: "
    :: :
    block: logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")
     expression_statement: logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")
      call: logging.warn(
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")
       attribute: logging.warn
        identifier: logging
        .: .
        identifier: warn
       argument_list: (
        "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk.")
        (: (
        concatenated_string: "Windows support is provided experimentally. No guarantee is made "
        "regarding functionality or engineering support. Use at your own risk."
         string: "Windows support is provided experimentally. No guarantee is made "
          string_start: "
          string_content: Windows support is provided experimentally. No guarantee is made 
          string_end: "
         string: "regarding functionality or engineering support. Use at your own risk."
          string_start: "
          string_content: regarding functionality or engineering support. Use at your own risk.
          string_end: "
        ): )
   expression_statement: linked_version = _pywrap_py_utils.get_linked_tensorrt_version()
    assignment: linked_version = _pywrap_py_utils.get_linked_tensorrt_version()
     identifier: linked_version
     =: =
     call: _pywrap_py_utils.get_linked_tensorrt_version()
      attribute: _pywrap_py_utils.get_linked_tensorrt_version
       identifier: _pywrap_py_utils
       .: .
       identifier: get_linked_tensorrt_version
      argument_list: ()
       (: (
       ): )
   expression_statement: loaded_version = _pywrap_py_utils.get_loaded_tensorrt_version()
    assignment: loaded_version = _pywrap_py_utils.get_loaded_tensorrt_version()
     identifier: loaded_version
     =: =
     call: _pywrap_py_utils.get_loaded_tensorrt_version()
      attribute: _pywrap_py_utils.get_loaded_tensorrt_version
       identifier: _pywrap_py_utils
       .: .
       identifier: get_loaded_tensorrt_version
      argument_list: ()
       (: (
       ): )
   expression_statement: logging.info("Linked TensorRT version: %s", str(linked_version))
    call: logging.info("Linked TensorRT version: %s", str(linked_version))
     attribute: logging.info
      identifier: logging
      .: .
      identifier: info
     argument_list: ("Linked TensorRT version: %s", str(linked_version))
      (: (
      string: "Linked TensorRT version: %s"
       string_start: "
       string_content: Linked TensorRT version: %s
       string_end: "
      ,: ,
      call: str(linked_version)
       identifier: str
       argument_list: (linked_version)
        (: (
        identifier: linked_version
        ): )
      ): )
   expression_statement: logging.info("Loaded TensorRT version: %s", str(loaded_version))
    call: logging.info("Loaded TensorRT version: %s", str(loaded_version))
     attribute: logging.info
      identifier: logging
      .: .
      identifier: info
     argument_list: ("Loaded TensorRT version: %s", str(loaded_version))
      (: (
      string: "Loaded TensorRT version: %s"
       string_start: "
       string_content: Loaded TensorRT version: %s
       string_end: "
      ,: ,
      call: str(loaded_version)
       identifier: str
       argument_list: (loaded_version)
        (: (
        identifier: loaded_version
        ): )
      ): )
   function_definition: def raise_trt_version_deprecated(version_type, trt_version):
    assert version_type in [
        "linked", "loaded"
    ], ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type

    logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))

    raise RuntimeError("Incompatible %s TensorRT versions" % version_type)
    def: def
    identifier: raise_trt_version_deprecated
    parameters: (version_type, trt_version)
     (: (
     identifier: version_type
     ,: ,
     identifier: trt_version
     ): )
    :: :
    block: assert version_type in [
        "linked", "loaded"
    ], ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type

    logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))

    raise RuntimeError("Incompatible %s TensorRT versions" % version_type)
     assert_statement: assert version_type in [
        "linked", "loaded"
    ], ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type
      assert: assert
      comparison_operator: version_type in [
        "linked", "loaded"
    ]
       identifier: version_type
       in: in
       list: [
        "linked", "loaded"
    ]
        [: [
        string: "linked"
         string_start: "
         string_content: linked
         string_end: "
        ,: ,
        string: "loaded"
         string_start: "
         string_content: loaded
         string_end: "
        ]: ]
      ,: ,
      binary_operator: ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']") % version_type
       parenthesized_expression: ("Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']")
        (: (
        concatenated_string: "Incorrect value received for version_type: %s. Accepted: ['linked', "
        "'loaded']"
         string: "Incorrect value received for version_type: %s. Accepted: ['linked', "
          string_start: "
          string_content: Incorrect value received for version_type: %s. Accepted: ['linked', 
          string_end: "
         string: "'loaded']"
          string_start: "
          string_content: 'loaded']
          string_end: "
        ): )
       %: %
       identifier: version_type
     expression_statement: logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))
      call: logging.error(
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))
       attribute: logging.error
        identifier: logging
        .: .
        identifier: error
       argument_list: (
        "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version)))
        (: (
        call: "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format(
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version))
         attribute: "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent.".format
          concatenated_string: "The {version_type} version of TensorRT: `{trt_version}` has now "
        "been removed. Please upgrade to TensorRT 7 or more recent."
           string: "The {version_type} version of TensorRT: `{trt_version}` has now "
            string_start: "
            string_content: The {version_type} version of TensorRT: `{trt_version}` has now 
            string_end: "
           string: "been removed. Please upgrade to TensorRT 7 or more recent."
            string_start: "
            string_content: been removed. Please upgrade to TensorRT 7 or more recent.
            string_end: "
          .: .
          identifier: format
         argument_list: (
            version_type=version_type,
            trt_version=trt_utils.version_tuple_to_string(trt_version))
          (: (
          keyword_argument: version_type=version_type
           identifier: version_type
           =: =
           identifier: version_type
          ,: ,
          keyword_argument: trt_version=trt_utils.version_tuple_to_string(trt_version)
           identifier: trt_version
           =: =
           call: trt_utils.version_tuple_to_string(trt_version)
            attribute: trt_utils.version_tuple_to_string
             identifier: trt_utils
             .: .
             identifier: version_tuple_to_string
            argument_list: (trt_version)
             (: (
             identifier: trt_version
             ): )
          ): )
        ): )
     raise_statement: raise RuntimeError("Incompatible %s TensorRT versions" % version_type)
      raise: raise
      call: RuntimeError("Incompatible %s TensorRT versions" % version_type)
       identifier: RuntimeError
       argument_list: ("Incompatible %s TensorRT versions" % version_type)
        (: (
        binary_operator: "Incompatible %s TensorRT versions" % version_type
         string: "Incompatible %s TensorRT versions"
          string_start: "
          string_content: Incompatible %s TensorRT versions
          string_end: "
         %: %
         identifier: version_type
        ): )
   if_statement: if not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("linked", linked_version)
    if: if
    not_operator: not trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0)
     not: not
     call: trt_utils.is_linked_tensorrt_version_greater_equal(7, 0, 0)
      attribute: trt_utils.is_linked_tensorrt_version_greater_equal
       identifier: trt_utils
       .: .
       identifier: is_linked_tensorrt_version_greater_equal
      argument_list: (7, 0, 0)
       (: (
       integer: 7
       ,: ,
       integer: 0
       ,: ,
       integer: 0
       ): )
    :: :
    block: raise_trt_version_deprecated("linked", linked_version)
     expression_statement: raise_trt_version_deprecated("linked", linked_version)
      call: raise_trt_version_deprecated("linked", linked_version)
       identifier: raise_trt_version_deprecated
       argument_list: ("linked", linked_version)
        (: (
        string: "linked"
         string_start: "
         string_content: linked
         string_end: "
        ,: ,
        identifier: linked_version
        ): )
   if_statement: if not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0):
    raise_trt_version_deprecated("loaded", loaded_version)
    if: if
    not_operator: not trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0)
     not: not
     call: trt_utils.is_loaded_tensorrt_version_greater_equal(7, 0, 0)
      attribute: trt_utils.is_loaded_tensorrt_version_greater_equal
       identifier: trt_utils
       .: .
       identifier: is_loaded_tensorrt_version_greater_equal
      argument_list: (7, 0, 0)
       (: (
       integer: 7
       ,: ,
       integer: 0
       ,: ,
       integer: 0
       ): )
    :: :
    block: raise_trt_version_deprecated("loaded", loaded_version)
     expression_statement: raise_trt_version_deprecated("loaded", loaded_version)
      call: raise_trt_version_deprecated("loaded", loaded_version)
       identifier: raise_trt_version_deprecated
       argument_list: ("loaded", loaded_version)
        (: (
        string: "loaded"
         string_start: "
         string_content: loaded
         string_end: "
        ,: ,
        identifier: loaded_version
        ): )
   if_statement: if (loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)):
    logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
    raise RuntimeError("Incompatible TensorRT major version")

  elif loaded_version != linked_version:
    logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
    if: if
    parenthesized_expression: (loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version))
     (: (
     boolean_operator: loaded_version[0] != linked_version[0] or
      not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)
      comparison_operator: loaded_version[0] != linked_version[0]
       subscript: loaded_version[0]
        identifier: loaded_version
        [: [
        integer: 0
        ]: ]
       !=: !=
       subscript: linked_version[0]
        identifier: linked_version
        [: [
        integer: 0
        ]: ]
      or: or
      not_operator: not trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)
       not: not
       call: trt_utils.is_loaded_tensorrt_version_greater_equal(*linked_version)
        attribute: trt_utils.is_loaded_tensorrt_version_greater_equal
         identifier: trt_utils
         .: .
         identifier: is_loaded_tensorrt_version_greater_equal
        argument_list: (*linked_version)
         (: (
         list_splat: *linked_version
          *: *
          identifier: linked_version
         ): )
     ): )
    :: :
    block: logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
    raise RuntimeError("Incompatible TensorRT major version")
     expression_statement: logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
      call: logging.error(
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
       attribute: logging.error
        identifier: logging
        .: .
        identifier: error
       argument_list: (
        "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version.",
        trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
        (: (
        concatenated_string: "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
        "requirements must be met:\n"
        "\t-It is required to use the same major version of TensorRT during "
        "compilation and runtime.\n"
        "\t-TensorRT does not support forward compatibility. The loaded "
        "version has to be equal or more recent than the linked version."
         string: "Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few "
          string_start: "
          string_content: Loaded TensorRT %s but linked TensorFlow against TensorRT %s. A few 
          string_end: "
         string: "requirements must be met:\n"
          string_start: "
          string_content: requirements must be met:\n
           escape_sequence: \n
          string_end: "
         string: "\t-It is required to use the same major version of TensorRT during "
          string_start: "
          string_content: \t-It is required to use the same major version of TensorRT during 
           escape_sequence: \t
          string_end: "
         string: "compilation and runtime.\n"
          string_start: "
          string_content: compilation and runtime.\n
           escape_sequence: \n
          string_end: "
         string: "\t-TensorRT does not support forward compatibility. The loaded "
          string_start: "
          string_content: \t-TensorRT does not support forward compatibility. The loaded 
           escape_sequence: \t
          string_end: "
         string: "version has to be equal or more recent than the linked version."
          string_start: "
          string_content: version has to be equal or more recent than the linked version.
          string_end: "
        ,: ,
        call: trt_utils.version_tuple_to_string(loaded_version)
         attribute: trt_utils.version_tuple_to_string
          identifier: trt_utils
          .: .
          identifier: version_tuple_to_string
         argument_list: (loaded_version)
          (: (
          identifier: loaded_version
          ): )
        ,: ,
        call: trt_utils.version_tuple_to_string(linked_version)
         attribute: trt_utils.version_tuple_to_string
          identifier: trt_utils
          .: .
          identifier: version_tuple_to_string
         argument_list: (linked_version)
          (: (
          identifier: linked_version
          ): )
        ): )
     raise_statement: raise RuntimeError("Incompatible TensorRT major version")
      raise: raise
      call: RuntimeError("Incompatible TensorRT major version")
       identifier: RuntimeError
       argument_list: ("Incompatible TensorRT major version")
        (: (
        string: "Incompatible TensorRT major version"
         string_start: "
         string_content: Incompatible TensorRT major version
         string_end: "
        ): )
    elif_clause: elif loaded_version != linked_version:
    logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
     elif: elif
     comparison_operator: loaded_version != linked_version
      identifier: loaded_version
      !=: !=
      identifier: linked_version
     :: :
     block: logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
      expression_statement: logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
       call: logging.info(
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
        attribute: logging.info
         identifier: logging
         .: .
         identifier: info
        argument_list: (
        "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible.", trt_utils.version_tuple_to_string(loaded_version),
        trt_utils.version_tuple_to_string(linked_version))
         (: (
         concatenated_string: "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
        "supported because TensorRT minor/patch upgrades are backward "
        "compatible."
          string: "Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is "
           string_start: "
           string_content: Loaded TensorRT %s and linked TensorFlow against TensorRT %s. This is 
           string_end: "
          string: "supported because TensorRT minor/patch upgrades are backward "
           string_start: "
           string_content: supported because TensorRT minor/patch upgrades are backward 
           string_end: "
          string: "compatible."
           string_start: "
           string_content: compatible.
           string_end: "
         ,: ,
         call: trt_utils.version_tuple_to_string(loaded_version)
          attribute: trt_utils.version_tuple_to_string
           identifier: trt_utils
           .: .
           identifier: version_tuple_to_string
          argument_list: (loaded_version)
           (: (
           identifier: loaded_version
           ): )
         ,: ,
         call: trt_utils.version_tuple_to_string(linked_version)
          attribute: trt_utils.version_tuple_to_string
           identifier: trt_utils
           .: .
           identifier: version_tuple_to_string
          argument_list: (linked_version)
           (: (
           identifier: linked_version
           ): )
         ): )
 function_definition: def _get_tensorrt_rewriter_config(conversion_params,
                                  is_dynamic_op=None,
                                  max_batch_size=None,
                                  is_v2=False,
                                  disable_non_trt_optimizers=False,
                                  use_implicit_batch=True,
                                  profile_strategy=PROFILE_STRATEGY_RANGE):
  """Returns a RewriterConfig proto for TRT transformation.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_dynamic_op: whether to use dynamic engines.
    max_batch_size: maximum batch size for static engines.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.
    disable_non_trt_optimizers: Turn off all default Grappler optimizers.
    use_implicit_batch: Whether to use implicit batch or explicit batch.
    profile_strategy: dynamic shape optimization profile strategy.

  Returns:
    A RewriterConfig proto which sets a TensorRTOptimizer to run Grappler.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
  _check_conversion_params(conversion_params, is_v2=is_v2)
  if is_v2 and is_dynamic_op is not None and not is_dynamic_op:
    raise ValueError("is_dynamic_op is either None or True for TF2")
  if not is_v2 and is_dynamic_op is None:
    raise ValueError("is_dynamic_op can't be None for TF1")

  if (is_dynamic_op is None or is_dynamic_op) and max_batch_size is not None:
    raise ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
  if is_dynamic_op is not None and not is_dynamic_op and not isinstance(
      max_batch_size, int):
    raise ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
  rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()
  # Disable Grappler Remapper to avoid that fused OPs that may not be
  # beneficial to TF-TRT and are not supported by TF-TRT.
  rewriter_config_with_trt.remapping = False

  # Prevent folding of Const->QDQ chains.
  rewriter_config_with_trt. \
    experimental_disable_folding_quantization_emulation = (
      trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0))

  if not disable_non_trt_optimizers:
    rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])

  rewriter_config_with_trt.meta_optimizer_iterations = (
      rewriter_config_pb2.RewriterConfig.ONE)
  optimizer = rewriter_config_with_trt.custom_optimizers.add()

  if not disable_non_trt_optimizers:
    # Add a constfold optimizer to cleanup the unused Const nodes.
    rewriter_config_with_trt.custom_optimizers.add().name = "constfold"

  optimizer.name = "TensorRTOptimizer"
  optimizer.parameter_map[
      "minimum_segment_size"].i = conversion_params.minimum_segment_size
  optimizer.parameter_map["max_workspace_size_bytes"].i = (
      conversion_params.max_workspace_size_bytes)
  optimizer.parameter_map["precision_mode"].s = _to_bytes(
      conversion_params.precision_mode)
  optimizer.parameter_map[
      "maximum_cached_engines"].i = conversion_params.maximum_cached_engines
  optimizer.parameter_map[
      "use_calibration"].b = conversion_params.use_calibration
  optimizer.parameter_map["is_dynamic_op"].b = is_dynamic_op
  optimizer.parameter_map[
      "allow_build_at_runtime"].b = conversion_params.allow_build_at_runtime
  if max_batch_size is not None:
    optimizer.parameter_map["max_batch_size"].i = max_batch_size
  optimizer.parameter_map["use_implicit_batch"].b = use_implicit_batch
  # While we accept case insensitive strings from the users, we only pass the
  # strings in lower cases to TF-TRT converter.
  if not use_implicit_batch:
    optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())

  # Disabling optimizers should happen after defining the TF-TRT grappler pass
  # otherwise the template can overwrite the disablement.
  if disable_non_trt_optimizers:
    trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)

  return rewriter_config_with_trt
  def: def
  identifier: _get_tensorrt_rewriter_config
  parameters: (conversion_params,
                                  is_dynamic_op=None,
                                  max_batch_size=None,
                                  is_v2=False,
                                  disable_non_trt_optimizers=False,
                                  use_implicit_batch=True,
                                  profile_strategy=PROFILE_STRATEGY_RANGE)
   (: (
   identifier: conversion_params
   ,: ,
   default_parameter: is_dynamic_op=None
    identifier: is_dynamic_op
    =: =
    none: None
   ,: ,
   default_parameter: max_batch_size=None
    identifier: max_batch_size
    =: =
    none: None
   ,: ,
   default_parameter: is_v2=False
    identifier: is_v2
    =: =
    false: False
   ,: ,
   default_parameter: disable_non_trt_optimizers=False
    identifier: disable_non_trt_optimizers
    =: =
    false: False
   ,: ,
   default_parameter: use_implicit_batch=True
    identifier: use_implicit_batch
    =: =
    true: True
   ,: ,
   default_parameter: profile_strategy=PROFILE_STRATEGY_RANGE
    identifier: profile_strategy
    =: =
    identifier: PROFILE_STRATEGY_RANGE
   ): )
  :: :
  block: """Returns a RewriterConfig proto for TRT transformation.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_dynamic_op: whether to use dynamic engines.
    max_batch_size: maximum batch size for static engines.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.
    disable_non_trt_optimizers: Turn off all default Grappler optimizers.
    use_implicit_batch: Whether to use implicit batch or explicit batch.
    profile_strategy: dynamic shape optimization profile strategy.

  Returns:
    A RewriterConfig proto which sets a TensorRTOptimizer to run Grappler.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
  _check_conversion_params(conversion_params, is_v2=is_v2)
  if is_v2 and is_dynamic_op is not None and not is_dynamic_op:
    raise ValueError("is_dynamic_op is either None or True for TF2")
  if not is_v2 and is_dynamic_op is None:
    raise ValueError("is_dynamic_op can't be None for TF1")

  if (is_dynamic_op is None or is_dynamic_op) and max_batch_size is not None:
    raise ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
  if is_dynamic_op is not None and not is_dynamic_op and not isinstance(
      max_batch_size, int):
    raise ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
  rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()
  # Disable Grappler Remapper to avoid that fused OPs that may not be
  # beneficial to TF-TRT and are not supported by TF-TRT.
  rewriter_config_with_trt.remapping = False

  # Prevent folding of Const->QDQ chains.
  rewriter_config_with_trt. \
    experimental_disable_folding_quantization_emulation = (
      trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0))

  if not disable_non_trt_optimizers:
    rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])

  rewriter_config_with_trt.meta_optimizer_iterations = (
      rewriter_config_pb2.RewriterConfig.ONE)
  optimizer = rewriter_config_with_trt.custom_optimizers.add()

  if not disable_non_trt_optimizers:
    # Add a constfold optimizer to cleanup the unused Const nodes.
    rewriter_config_with_trt.custom_optimizers.add().name = "constfold"

  optimizer.name = "TensorRTOptimizer"
  optimizer.parameter_map[
      "minimum_segment_size"].i = conversion_params.minimum_segment_size
  optimizer.parameter_map["max_workspace_size_bytes"].i = (
      conversion_params.max_workspace_size_bytes)
  optimizer.parameter_map["precision_mode"].s = _to_bytes(
      conversion_params.precision_mode)
  optimizer.parameter_map[
      "maximum_cached_engines"].i = conversion_params.maximum_cached_engines
  optimizer.parameter_map[
      "use_calibration"].b = conversion_params.use_calibration
  optimizer.parameter_map["is_dynamic_op"].b = is_dynamic_op
  optimizer.parameter_map[
      "allow_build_at_runtime"].b = conversion_params.allow_build_at_runtime
  if max_batch_size is not None:
    optimizer.parameter_map["max_batch_size"].i = max_batch_size
  optimizer.parameter_map["use_implicit_batch"].b = use_implicit_batch
  # While we accept case insensitive strings from the users, we only pass the
  # strings in lower cases to TF-TRT converter.
  if not use_implicit_batch:
    optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())

  # Disabling optimizers should happen after defining the TF-TRT grappler pass
  # otherwise the template can overwrite the disablement.
  if disable_non_trt_optimizers:
    trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)

  return rewriter_config_with_trt
   expression_statement: """Returns a RewriterConfig proto for TRT transformation.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_dynamic_op: whether to use dynamic engines.
    max_batch_size: maximum batch size for static engines.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.
    disable_non_trt_optimizers: Turn off all default Grappler optimizers.
    use_implicit_batch: Whether to use implicit batch or explicit batch.
    profile_strategy: dynamic shape optimization profile strategy.

  Returns:
    A RewriterConfig proto which sets a TensorRTOptimizer to run Grappler.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
    string: """Returns a RewriterConfig proto for TRT transformation.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_dynamic_op: whether to use dynamic engines.
    max_batch_size: maximum batch size for static engines.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.
    disable_non_trt_optimizers: Turn off all default Grappler optimizers.
    use_implicit_batch: Whether to use implicit batch or explicit batch.
    profile_strategy: dynamic shape optimization profile strategy.

  Returns:
    A RewriterConfig proto which sets a TensorRTOptimizer to run Grappler.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  """
     string_start: """
     string_content: Returns a RewriterConfig proto for TRT transformation.

  Args:
    conversion_params: a TrtConversionParams instance.
    is_dynamic_op: whether to use dynamic engines.
    max_batch_size: maximum batch size for static engines.
    is_v2: whether we're getting a RewriterConfig for TF 2.0.
    disable_non_trt_optimizers: Turn off all default Grappler optimizers.
    use_implicit_batch: Whether to use implicit batch or explicit batch.
    profile_strategy: dynamic shape optimization profile strategy.

  Returns:
    A RewriterConfig proto which sets a TensorRTOptimizer to run Grappler.

  Raises:
    TypeError: if any of the parameters are of unexpected type.
    ValueError: if any of the parameters are of unexpected value.
  
     string_end: """
   expression_statement: _check_conversion_params(conversion_params, is_v2=is_v2)
    call: _check_conversion_params(conversion_params, is_v2=is_v2)
     identifier: _check_conversion_params
     argument_list: (conversion_params, is_v2=is_v2)
      (: (
      identifier: conversion_params
      ,: ,
      keyword_argument: is_v2=is_v2
       identifier: is_v2
       =: =
       identifier: is_v2
      ): )
   if_statement: if is_v2 and is_dynamic_op is not None and not is_dynamic_op:
    raise ValueError("is_dynamic_op is either None or True for TF2")
    if: if
    boolean_operator: is_v2 and is_dynamic_op is not None and not is_dynamic_op
     boolean_operator: is_v2 and is_dynamic_op is not None
      identifier: is_v2
      and: and
      comparison_operator: is_dynamic_op is not None
       identifier: is_dynamic_op
       is not: is not
        is: is
        not: not
       none: None
     and: and
     not_operator: not is_dynamic_op
      not: not
      identifier: is_dynamic_op
    :: :
    block: raise ValueError("is_dynamic_op is either None or True for TF2")
     raise_statement: raise ValueError("is_dynamic_op is either None or True for TF2")
      raise: raise
      call: ValueError("is_dynamic_op is either None or True for TF2")
       identifier: ValueError
       argument_list: ("is_dynamic_op is either None or True for TF2")
        (: (
        string: "is_dynamic_op is either None or True for TF2"
         string_start: "
         string_content: is_dynamic_op is either None or True for TF2
         string_end: "
        ): )
   if_statement: if not is_v2 and is_dynamic_op is None:
    raise ValueError("is_dynamic_op can't be None for TF1")
    if: if
    boolean_operator: not is_v2 and is_dynamic_op is None
     not_operator: not is_v2
      not: not
      identifier: is_v2
     and: and
     comparison_operator: is_dynamic_op is None
      identifier: is_dynamic_op
      is: is
      none: None
    :: :
    block: raise ValueError("is_dynamic_op can't be None for TF1")
     raise_statement: raise ValueError("is_dynamic_op can't be None for TF1")
      raise: raise
      call: ValueError("is_dynamic_op can't be None for TF1")
       identifier: ValueError
       argument_list: ("is_dynamic_op can't be None for TF1")
        (: (
        string: "is_dynamic_op can't be None for TF1"
         string_start: "
         string_content: is_dynamic_op can't be None for TF1
         string_end: "
        ): )
   if_statement: if (is_dynamic_op is None or is_dynamic_op) and max_batch_size is not None:
    raise ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
    if: if
    boolean_operator: (is_dynamic_op is None or is_dynamic_op) and max_batch_size is not None
     parenthesized_expression: (is_dynamic_op is None or is_dynamic_op)
      (: (
      boolean_operator: is_dynamic_op is None or is_dynamic_op
       comparison_operator: is_dynamic_op is None
        identifier: is_dynamic_op
        is: is
        none: None
       or: or
       identifier: is_dynamic_op
      ): )
     and: and
     comparison_operator: max_batch_size is not None
      identifier: max_batch_size
      is not: is not
       is: is
       not: not
      none: None
    :: :
    block: raise ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
     raise_statement: raise ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
      raise: raise
      call: ValueError("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
       identifier: ValueError
       argument_list: ("max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1")
        (: (
        concatenated_string: "max_batch_size has to be None for TF2"
                     " or when is_dynamic_op == True in TF1"
         string: "max_batch_size has to be None for TF2"
          string_start: "
          string_content: max_batch_size has to be None for TF2
          string_end: "
         string: " or when is_dynamic_op == True in TF1"
          string_start: "
          string_content:  or when is_dynamic_op == True in TF1
          string_end: "
        ): )
   if_statement: if is_dynamic_op is not None and not is_dynamic_op and not isinstance(
      max_batch_size, int):
    raise ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
    if: if
    boolean_operator: is_dynamic_op is not None and not is_dynamic_op and not isinstance(
      max_batch_size, int)
     boolean_operator: is_dynamic_op is not None and not is_dynamic_op
      comparison_operator: is_dynamic_op is not None
       identifier: is_dynamic_op
       is not: is not
        is: is
        not: not
       none: None
      and: and
      not_operator: not is_dynamic_op
       not: not
       identifier: is_dynamic_op
     and: and
     not_operator: not isinstance(
      max_batch_size, int)
      not: not
      call: isinstance(
      max_batch_size, int)
       identifier: isinstance
       argument_list: (
      max_batch_size, int)
        (: (
        identifier: max_batch_size
        ,: ,
        identifier: int
        ): )
    :: :
    block: raise ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
     raise_statement: raise ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
      raise: raise
      call: ValueError(
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
       identifier: ValueError
       argument_list: (
        "max_batch_size has to be an integer for is_dynamic_op==False in TF1")
        (: (
        string: "max_batch_size has to be an integer for is_dynamic_op==False in TF1"
         string_start: "
         string_content: max_batch_size has to be an integer for is_dynamic_op==False in TF1
         string_end: "
        ): )
   expression_statement: rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()
    assignment: rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()
     identifier: rewriter_config_with_trt
     =: =
     call: rewriter_config_pb2.RewriterConfig()
      attribute: rewriter_config_pb2.RewriterConfig
       identifier: rewriter_config_pb2
       .: .
       identifier: RewriterConfig
      argument_list: ()
       (: (
       ): )
   comment: # Disable Grappler Remapper to avoid that fused OPs that may not be
   comment: # beneficial to TF-TRT and are not supported by TF-TRT.
   expression_statement: rewriter_config_with_trt.remapping = False
    assignment: rewriter_config_with_trt.remapping = False
     attribute: rewriter_config_with_trt.remapping
      identifier: rewriter_config_with_trt
      .: .
      identifier: remapping
     =: =
     false: False
   comment: # Prevent folding of Const->QDQ chains.
   expression_statement: rewriter_config_with_trt. \
    experimental_disable_folding_quantization_emulation = (
      trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0))
    assignment: rewriter_config_with_trt. \
    experimental_disable_folding_quantization_emulation = (
      trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0))
     attribute: rewriter_config_with_trt. \
    experimental_disable_folding_quantization_emulation
      identifier: rewriter_config_with_trt
      .: .
      line_continuation: \

      identifier: experimental_disable_folding_quantization_emulation
     =: =
     parenthesized_expression: (
      trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0))
      (: (
      boolean_operator: trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0) or
      trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0)
       call: trt_utils.is_linked_tensorrt_version_greater_equal(8, 0, 0)
        attribute: trt_utils.is_linked_tensorrt_version_greater_equal
         identifier: trt_utils
         .: .
         identifier: is_linked_tensorrt_version_greater_equal
        argument_list: (8, 0, 0)
         (: (
         integer: 8
         ,: ,
         integer: 0
         ,: ,
         integer: 0
         ): )
       or: or
       call: trt_utils.is_loaded_tensorrt_version_greater_equal(8, 0, 0)
        attribute: trt_utils.is_loaded_tensorrt_version_greater_equal
         identifier: trt_utils
         .: .
         identifier: is_loaded_tensorrt_version_greater_equal
        argument_list: (8, 0, 0)
         (: (
         integer: 8
         ,: ,
         integer: 0
         ,: ,
         integer: 0
         ): )
      ): )
   if_statement: if not disable_non_trt_optimizers:
    rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])
    if: if
    not_operator: not disable_non_trt_optimizers
     not: not
     identifier: disable_non_trt_optimizers
    :: :
    block: rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])
     expression_statement: rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])
      call: rewriter_config_with_trt.optimizers.extend([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])
       attribute: rewriter_config_with_trt.optimizers.extend
        attribute: rewriter_config_with_trt.optimizers
         identifier: rewriter_config_with_trt
         .: .
         identifier: optimizers
        .: .
        identifier: extend
       argument_list: ([
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ])
        (: (
        list: [
        "pruning", "debug_stripper", "layout", "dependency", "constfold",
        "common_subgraph_elimination"
    ]
         [: [
         string: "pruning"
          string_start: "
          string_content: pruning
          string_end: "
         ,: ,
         string: "debug_stripper"
          string_start: "
          string_content: debug_stripper
          string_end: "
         ,: ,
         string: "layout"
          string_start: "
          string_content: layout
          string_end: "
         ,: ,
         string: "dependency"
          string_start: "
          string_content: dependency
          string_end: "
         ,: ,
         string: "constfold"
          string_start: "
          string_content: constfold
          string_end: "
         ,: ,
         string: "common_subgraph_elimination"
          string_start: "
          string_content: common_subgraph_elimination
          string_end: "
         ]: ]
        ): )
   expression_statement: rewriter_config_with_trt.meta_optimizer_iterations = (
      rewriter_config_pb2.RewriterConfig.ONE)
    assignment: rewriter_config_with_trt.meta_optimizer_iterations = (
      rewriter_config_pb2.RewriterConfig.ONE)
     attribute: rewriter_config_with_trt.meta_optimizer_iterations
      identifier: rewriter_config_with_trt
      .: .
      identifier: meta_optimizer_iterations
     =: =
     parenthesized_expression: (
      rewriter_config_pb2.RewriterConfig.ONE)
      (: (
      attribute: rewriter_config_pb2.RewriterConfig.ONE
       attribute: rewriter_config_pb2.RewriterConfig
        identifier: rewriter_config_pb2
        .: .
        identifier: RewriterConfig
       .: .
       identifier: ONE
      ): )
   expression_statement: optimizer = rewriter_config_with_trt.custom_optimizers.add()
    assignment: optimizer = rewriter_config_with_trt.custom_optimizers.add()
     identifier: optimizer
     =: =
     call: rewriter_config_with_trt.custom_optimizers.add()
      attribute: rewriter_config_with_trt.custom_optimizers.add
       attribute: rewriter_config_with_trt.custom_optimizers
        identifier: rewriter_config_with_trt
        .: .
        identifier: custom_optimizers
       .: .
       identifier: add
      argument_list: ()
       (: (
       ): )
   if_statement: if not disable_non_trt_optimizers:
    # Add a constfold optimizer to cleanup the unused Const nodes.
    rewriter_config_with_trt.custom_optimizers.add().name = "constfold"
    if: if
    not_operator: not disable_non_trt_optimizers
     not: not
     identifier: disable_non_trt_optimizers
    :: :
    comment: # Add a constfold optimizer to cleanup the unused Const nodes.
    block: rewriter_config_with_trt.custom_optimizers.add().name = "constfold"
     expression_statement: rewriter_config_with_trt.custom_optimizers.add().name = "constfold"
      assignment: rewriter_config_with_trt.custom_optimizers.add().name = "constfold"
       attribute: rewriter_config_with_trt.custom_optimizers.add().name
        call: rewriter_config_with_trt.custom_optimizers.add()
         attribute: rewriter_config_with_trt.custom_optimizers.add
          attribute: rewriter_config_with_trt.custom_optimizers
           identifier: rewriter_config_with_trt
           .: .
           identifier: custom_optimizers
          .: .
          identifier: add
         argument_list: ()
          (: (
          ): )
        .: .
        identifier: name
       =: =
       string: "constfold"
        string_start: "
        string_content: constfold
        string_end: "
   expression_statement: optimizer.name = "TensorRTOptimizer"
    assignment: optimizer.name = "TensorRTOptimizer"
     attribute: optimizer.name
      identifier: optimizer
      .: .
      identifier: name
     =: =
     string: "TensorRTOptimizer"
      string_start: "
      string_content: TensorRTOptimizer
      string_end: "
   expression_statement: optimizer.parameter_map[
      "minimum_segment_size"].i = conversion_params.minimum_segment_size
    assignment: optimizer.parameter_map[
      "minimum_segment_size"].i = conversion_params.minimum_segment_size
     attribute: optimizer.parameter_map[
      "minimum_segment_size"].i
      subscript: optimizer.parameter_map[
      "minimum_segment_size"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "minimum_segment_size"
        string_start: "
        string_content: minimum_segment_size
        string_end: "
       ]: ]
      .: .
      identifier: i
     =: =
     attribute: conversion_params.minimum_segment_size
      identifier: conversion_params
      .: .
      identifier: minimum_segment_size
   expression_statement: optimizer.parameter_map["max_workspace_size_bytes"].i = (
      conversion_params.max_workspace_size_bytes)
    assignment: optimizer.parameter_map["max_workspace_size_bytes"].i = (
      conversion_params.max_workspace_size_bytes)
     attribute: optimizer.parameter_map["max_workspace_size_bytes"].i
      subscript: optimizer.parameter_map["max_workspace_size_bytes"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "max_workspace_size_bytes"
        string_start: "
        string_content: max_workspace_size_bytes
        string_end: "
       ]: ]
      .: .
      identifier: i
     =: =
     parenthesized_expression: (
      conversion_params.max_workspace_size_bytes)
      (: (
      attribute: conversion_params.max_workspace_size_bytes
       identifier: conversion_params
       .: .
       identifier: max_workspace_size_bytes
      ): )
   expression_statement: optimizer.parameter_map["precision_mode"].s = _to_bytes(
      conversion_params.precision_mode)
    assignment: optimizer.parameter_map["precision_mode"].s = _to_bytes(
      conversion_params.precision_mode)
     attribute: optimizer.parameter_map["precision_mode"].s
      subscript: optimizer.parameter_map["precision_mode"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "precision_mode"
        string_start: "
        string_content: precision_mode
        string_end: "
       ]: ]
      .: .
      identifier: s
     =: =
     call: _to_bytes(
      conversion_params.precision_mode)
      identifier: _to_bytes
      argument_list: (
      conversion_params.precision_mode)
       (: (
       attribute: conversion_params.precision_mode
        identifier: conversion_params
        .: .
        identifier: precision_mode
       ): )
   expression_statement: optimizer.parameter_map[
      "maximum_cached_engines"].i = conversion_params.maximum_cached_engines
    assignment: optimizer.parameter_map[
      "maximum_cached_engines"].i = conversion_params.maximum_cached_engines
     attribute: optimizer.parameter_map[
      "maximum_cached_engines"].i
      subscript: optimizer.parameter_map[
      "maximum_cached_engines"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "maximum_cached_engines"
        string_start: "
        string_content: maximum_cached_engines
        string_end: "
       ]: ]
      .: .
      identifier: i
     =: =
     attribute: conversion_params.maximum_cached_engines
      identifier: conversion_params
      .: .
      identifier: maximum_cached_engines
   expression_statement: optimizer.parameter_map[
      "use_calibration"].b = conversion_params.use_calibration
    assignment: optimizer.parameter_map[
      "use_calibration"].b = conversion_params.use_calibration
     attribute: optimizer.parameter_map[
      "use_calibration"].b
      subscript: optimizer.parameter_map[
      "use_calibration"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "use_calibration"
        string_start: "
        string_content: use_calibration
        string_end: "
       ]: ]
      .: .
      identifier: b
     =: =
     attribute: conversion_params.use_calibration
      identifier: conversion_params
      .: .
      identifier: use_calibration
   expression_statement: optimizer.parameter_map["is_dynamic_op"].b = is_dynamic_op
    assignment: optimizer.parameter_map["is_dynamic_op"].b = is_dynamic_op
     attribute: optimizer.parameter_map["is_dynamic_op"].b
      subscript: optimizer.parameter_map["is_dynamic_op"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "is_dynamic_op"
        string_start: "
        string_content: is_dynamic_op
        string_end: "
       ]: ]
      .: .
      identifier: b
     =: =
     identifier: is_dynamic_op
   expression_statement: optimizer.parameter_map[
      "allow_build_at_runtime"].b = conversion_params.allow_build_at_runtime
    assignment: optimizer.parameter_map[
      "allow_build_at_runtime"].b = conversion_params.allow_build_at_runtime
     attribute: optimizer.parameter_map[
      "allow_build_at_runtime"].b
      subscript: optimizer.parameter_map[
      "allow_build_at_runtime"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "allow_build_at_runtime"
        string_start: "
        string_content: allow_build_at_runtime
        string_end: "
       ]: ]
      .: .
      identifier: b
     =: =
     attribute: conversion_params.allow_build_at_runtime
      identifier: conversion_params
      .: .
      identifier: allow_build_at_runtime
   if_statement: if max_batch_size is not None:
    optimizer.parameter_map["max_batch_size"].i = max_batch_size
    if: if
    comparison_operator: max_batch_size is not None
     identifier: max_batch_size
     is not: is not
      is: is
      not: not
     none: None
    :: :
    block: optimizer.parameter_map["max_batch_size"].i = max_batch_size
     expression_statement: optimizer.parameter_map["max_batch_size"].i = max_batch_size
      assignment: optimizer.parameter_map["max_batch_size"].i = max_batch_size
       attribute: optimizer.parameter_map["max_batch_size"].i
        subscript: optimizer.parameter_map["max_batch_size"]
         attribute: optimizer.parameter_map
          identifier: optimizer
          .: .
          identifier: parameter_map
         [: [
         string: "max_batch_size"
          string_start: "
          string_content: max_batch_size
          string_end: "
         ]: ]
        .: .
        identifier: i
       =: =
       identifier: max_batch_size
   expression_statement: optimizer.parameter_map["use_implicit_batch"].b = use_implicit_batch
    assignment: optimizer.parameter_map["use_implicit_batch"].b = use_implicit_batch
     attribute: optimizer.parameter_map["use_implicit_batch"].b
      subscript: optimizer.parameter_map["use_implicit_batch"]
       attribute: optimizer.parameter_map
        identifier: optimizer
        .: .
        identifier: parameter_map
       [: [
       string: "use_implicit_batch"
        string_start: "
        string_content: use_implicit_batch
        string_end: "
       ]: ]
      .: .
      identifier: b
     =: =
     identifier: use_implicit_batch
   comment: # While we accept case insensitive strings from the users, we only pass the
   comment: # strings in lower cases to TF-TRT converter.
   if_statement: if not use_implicit_batch:
    optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())
    if: if
    not_operator: not use_implicit_batch
     not: not
     identifier: use_implicit_batch
    :: :
    block: optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())
     expression_statement: optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())
      assignment: optimizer.parameter_map["profile_strategy"].s = _to_bytes(
        profile_strategy.lower())
       attribute: optimizer.parameter_map["profile_strategy"].s
        subscript: optimizer.parameter_map["profile_strategy"]
         attribute: optimizer.parameter_map
          identifier: optimizer
          .: .
          identifier: parameter_map
         [: [
         string: "profile_strategy"
          string_start: "
          string_content: profile_strategy
          string_end: "
         ]: ]
        .: .
        identifier: s
       =: =
       call: _to_bytes(
        profile_strategy.lower())
        identifier: _to_bytes
        argument_list: (
        profile_strategy.lower())
         (: (
         call: profile_strategy.lower()
          attribute: profile_strategy.lower
           identifier: profile_strategy
           .: .
           identifier: lower
          argument_list: ()
           (: (
           ): )
         ): )
   comment: # Disabling optimizers should happen after defining the TF-TRT grappler pass
   comment: # otherwise the template can overwrite the disablement.
   if_statement: if disable_non_trt_optimizers:
    trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)
    if: if
    identifier: disable_non_trt_optimizers
    :: :
    block: trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)
     expression_statement: trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)
      call: trt_utils.disable_non_trt_optimizers_in_rewriter_config(
        rewriter_config_with_trt)
       attribute: trt_utils.disable_non_trt_optimizers_in_rewriter_config
        identifier: trt_utils
        .: .
        identifier: disable_non_trt_optimizers_in_rewriter_config
       argument_list: (
        rewriter_config_with_trt)
        (: (
        identifier: rewriter_config_with_trt
        ): )
   return_statement: return rewriter_config_with_trt
    return: return
    identifier: rewriter_config_with_trt
 decorated_definition: @deprecation.deprecated(
    None, "You shouldn't need a rewriter_config with the current TF-TRT APIs.")
def get_tensorrt_rewriter_config(conversion_params,
                                 is_dynamic_op=None,
                                 max_batch_size=None,
                                 is_v2=False,
                                 disable_non_trt_optimizers=False):
  return _get_tensorrt_rewriter_config(conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)
  decorator: @deprecation.deprecated(
    None, "You shouldn't need a rewriter_config with the current TF-TRT APIs.")
   @: @
   call: deprecation.deprecated(
    None, "You shouldn't need a rewriter_config with the current TF-TRT APIs.")
    attribute: deprecation.deprecated
     identifier: deprecation
     .: .
     identifier: deprecated
    argument_list: (
    None, "You shouldn't need a rewriter_config with the current TF-TRT APIs.")
     (: (
     none: None
     ,: ,
     string: "You shouldn't need a rewriter_config with the current TF-TRT APIs."
      string_start: "
      string_content: You shouldn't need a rewriter_config with the current TF-TRT APIs.
      string_end: "
     ): )
  function_definition: def get_tensorrt_rewriter_config(conversion_params,
                                 is_dynamic_op=None,
                                 max_batch_size=None,
                                 is_v2=False,
                                 disable_non_trt_optimizers=False):
  return _get_tensorrt_rewriter_config(conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)
   def: def
   identifier: get_tensorrt_rewriter_config
   parameters: (conversion_params,
                                 is_dynamic_op=None,
                                 max_batch_size=None,
                                 is_v2=False,
                                 disable_non_trt_optimizers=False)
    (: (
    identifier: conversion_params
    ,: ,
    default_parameter: is_dynamic_op=None
     identifier: is_dynamic_op
     =: =
     none: None
    ,: ,
    default_parameter: max_batch_size=None
     identifier: max_batch_size
     =: =
     none: None
    ,: ,
    default_parameter: is_v2=False
     identifier: is_v2
     =: =
     false: False
    ,: ,
    default_parameter: disable_non_trt_optimizers=False
     identifier: disable_non_trt_optimizers
     =: =
     false: False
    ): )
   :: :
   block: return _get_tensorrt_rewriter_config(conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)
    return_statement: return _get_tensorrt_rewriter_config(conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)
     return: return
     call: _get_tensorrt_rewriter_config(conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)
      identifier: _get_tensorrt_rewriter_config
      argument_list: (conversion_params, is_dynamic_op,
                                       max_batch_size, is_v2,
                                       disable_non_trt_optimizers)
       (: (
       identifier: conversion_params
       ,: ,
       identifier: is_dynamic_op
       ,: ,
       identifier: max_batch_size
       ,: ,
       identifier: is_v2
       ,: ,
       identifier: disable_non_trt_optimizers
       ): )
 comment: # Remove all scope prefixes in the node name. In TF 2.0, the same concrete
 comment: # function can be initialized multiple times with different prefixes, and
 comment: # this will result in the same TRTEngineOp being initialized multiple times
 comment: # with different cache and duplicate TRT engines.
 comment: # TODO(laigd): this may be caused by the fact that TRTEngineOp is not
 comment: # stateful, need to investigate.
 comment: # TODO(laigd): we rely on the fact that all functions are fully inlined
 comment: # before TF-TRT optimizer is called, as otherwise it may generate the same
 comment: # name when optimizing a different function graph. Fix this.
 function_definition: def _get_canonical_engine_name(name):
  return name.split("/")[-1]
  def: def
  identifier: _get_canonical_engine_name
  parameters: (name)
   (: (
   identifier: name
   ): )
  :: :
  block: return name.split("/")[-1]
   return_statement: return name.split("/")[-1]
    return: return
    subscript: name.split("/")[-1]
     call: name.split("/")
      attribute: name.split
       identifier: name
       .: .
       identifier: split
      argument_list: ("/")
       (: (
       string: "/"
        string_start: "
        string_content: /
        string_end: "
       ): )
     [: [
     unary_operator: -1
      -: -
      integer: 1
     ]: ]
 class_definition: class TrtGraphConverter(object):
  """A converter for TF-TRT transformation for TF 1.x GraphDef/SavedModels.

  To run the conversion without quantization calibration (e.g. for FP32/FP16
  precision modes):

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.FP16)
  converted_graph_def = converter.convert()
  converter.save(output_saved_model_dir)
  ```

  To run the conversion with quantization calibration:

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.INT8)
  converter.convert()

  # Run calibration 10 times.
  converted_graph_def = converter.calibrate(
      fetch_names=['output:0'],
      num_runs=10,
      feed_dict_fn=lambda: {'input:0': my_next_data()})

  converter.save(output_saved_model_dir)
  ```
  """

  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               input_graph_def=None,
               nodes_denylist=None,
               max_batch_size=1,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               is_dynamic_op=False,
               maximum_cached_engines=1,
               use_calibration=True):
    """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
    if context.executing_eagerly():
      raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")

    if input_graph_def and input_saved_model_dir:
      raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
    if not input_graph_def and not input_saved_model_dir:
      raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
    _check_trt_version_compatibility()

    self._input_graph_def = input_graph_def
    self._nodes_denylist = nodes_denylist

    self._input_saved_model_dir = input_saved_model_dir
    self._converted = False
    self._grappler_meta_graph_def = None

    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)

    # For calibration usage.
    self._calibration_graph = None
    self._calibration_data_collected = False
    self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
    if self._need_calibration and not is_dynamic_op:
      logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True

    self._is_dynamic_op = is_dynamic_op
    if is_dynamic_op:
      self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
    else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size

    self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
    _check_conversion_params(self._conversion_params)

    self._test_only_disable_non_trt_optimizers = False

  def _run_conversion(self):
    """Run Grappler's OptimizeGraph() tool to convert the graph."""
    # Create custom ConfigProto for Grappler.
    grappler_session_config = config_pb2.ConfigProto()
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)

    # Run Grappler.
    self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
    self._converted = True

  def _add_nodes_denylist(self):
    if self._nodes_denylist:
      collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))

  def _convert_graph_def(self):
    """Convert the input GraphDef."""
    graph = ops.Graph()
    with graph.as_default():
      importer.import_graph_def(self._input_graph_def, name="")
    self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
    self._add_nodes_denylist()

    self._run_conversion()

  def _collections_to_keep(self, collection_keys):
    # TODO(laigd): currently we use the collection key to filter out
    # collections that depend on variable ops, but this may miss some
    # other user-defined collections. A better way would be to use
    # CollectionDef::NodeList for the filtering.
    collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
    return [key for key in collection_keys if key not in collections_to_remove]

  def _convert_saved_model(self):
    """Convert the input SavedModel."""
    graph = ops.Graph()
    with session.Session(graph=graph) as sess:
      input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.

    self._run_conversion()

  def convert(self):
    """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
    assert not self._converted
    if self._input_graph_def:
      self._convert_graph_def()
    else:
      self._convert_saved_model()
    return self._converted_graph_def

  def calibrate(self,
                fetch_names,
                num_runs,
                feed_dict_fn=None,
                input_map_fn=None):
    """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
    assert self._converted
    assert self._need_calibration
    assert not self._calibration_data_collected

    if (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn):
      raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")

    if input_map_fn:
      for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")

    self._calibration_graph = ops.Graph()
    with self._calibration_graph.as_default():
      fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")

    calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
    if self._test_only_disable_non_trt_optimizers:
      trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)

    # Set allow_soft_placement=True to run the graph for calibration so that
    # OPs supported by TensorRT but don't have a GPU implementation are allowed
    # to execute on CPU.
    calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))

    with session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess:
      for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True

    return self._converted_graph_def

  def save(self, output_saved_model_dir):
    """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
    assert self._converted
    if self._need_calibration:
      assert self._calibration_data_collected
    if self._input_graph_def:
      raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")

    def _restore_collections(dest_graph, src_meta_graph_def, collection_keys):
      """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))

    # Write the transformed graphdef as SavedModel.
    saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
    with ops.Graph().as_default():
      importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
    # Ignore other meta graphs from the input SavedModel.
    saved_model_builder.save()
  class: class
  identifier: TrtGraphConverter
  argument_list: (object)
   (: (
   identifier: object
   ): )
  :: :
  block: """A converter for TF-TRT transformation for TF 1.x GraphDef/SavedModels.

  To run the conversion without quantization calibration (e.g. for FP32/FP16
  precision modes):

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.FP16)
  converted_graph_def = converter.convert()
  converter.save(output_saved_model_dir)
  ```

  To run the conversion with quantization calibration:

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.INT8)
  converter.convert()

  # Run calibration 10 times.
  converted_graph_def = converter.calibrate(
      fetch_names=['output:0'],
      num_runs=10,
      feed_dict_fn=lambda: {'input:0': my_next_data()})

  converter.save(output_saved_model_dir)
  ```
  """

  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               input_graph_def=None,
               nodes_denylist=None,
               max_batch_size=1,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               is_dynamic_op=False,
               maximum_cached_engines=1,
               use_calibration=True):
    """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
    if context.executing_eagerly():
      raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")

    if input_graph_def and input_saved_model_dir:
      raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
    if not input_graph_def and not input_saved_model_dir:
      raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
    _check_trt_version_compatibility()

    self._input_graph_def = input_graph_def
    self._nodes_denylist = nodes_denylist

    self._input_saved_model_dir = input_saved_model_dir
    self._converted = False
    self._grappler_meta_graph_def = None

    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)

    # For calibration usage.
    self._calibration_graph = None
    self._calibration_data_collected = False
    self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
    if self._need_calibration and not is_dynamic_op:
      logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True

    self._is_dynamic_op = is_dynamic_op
    if is_dynamic_op:
      self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
    else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size

    self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
    _check_conversion_params(self._conversion_params)

    self._test_only_disable_non_trt_optimizers = False

  def _run_conversion(self):
    """Run Grappler's OptimizeGraph() tool to convert the graph."""
    # Create custom ConfigProto for Grappler.
    grappler_session_config = config_pb2.ConfigProto()
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)

    # Run Grappler.
    self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
    self._converted = True

  def _add_nodes_denylist(self):
    if self._nodes_denylist:
      collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))

  def _convert_graph_def(self):
    """Convert the input GraphDef."""
    graph = ops.Graph()
    with graph.as_default():
      importer.import_graph_def(self._input_graph_def, name="")
    self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
    self._add_nodes_denylist()

    self._run_conversion()

  def _collections_to_keep(self, collection_keys):
    # TODO(laigd): currently we use the collection key to filter out
    # collections that depend on variable ops, but this may miss some
    # other user-defined collections. A better way would be to use
    # CollectionDef::NodeList for the filtering.
    collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
    return [key for key in collection_keys if key not in collections_to_remove]

  def _convert_saved_model(self):
    """Convert the input SavedModel."""
    graph = ops.Graph()
    with session.Session(graph=graph) as sess:
      input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.

    self._run_conversion()

  def convert(self):
    """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
    assert not self._converted
    if self._input_graph_def:
      self._convert_graph_def()
    else:
      self._convert_saved_model()
    return self._converted_graph_def

  def calibrate(self,
                fetch_names,
                num_runs,
                feed_dict_fn=None,
                input_map_fn=None):
    """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
    assert self._converted
    assert self._need_calibration
    assert not self._calibration_data_collected

    if (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn):
      raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")

    if input_map_fn:
      for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")

    self._calibration_graph = ops.Graph()
    with self._calibration_graph.as_default():
      fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")

    calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
    if self._test_only_disable_non_trt_optimizers:
      trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)

    # Set allow_soft_placement=True to run the graph for calibration so that
    # OPs supported by TensorRT but don't have a GPU implementation are allowed
    # to execute on CPU.
    calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))

    with session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess:
      for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True

    return self._converted_graph_def

  def save(self, output_saved_model_dir):
    """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
    assert self._converted
    if self._need_calibration:
      assert self._calibration_data_collected
    if self._input_graph_def:
      raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")

    def _restore_collections(dest_graph, src_meta_graph_def, collection_keys):
      """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))

    # Write the transformed graphdef as SavedModel.
    saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
    with ops.Graph().as_default():
      importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
    # Ignore other meta graphs from the input SavedModel.
    saved_model_builder.save()
   expression_statement: """A converter for TF-TRT transformation for TF 1.x GraphDef/SavedModels.

  To run the conversion without quantization calibration (e.g. for FP32/FP16
  precision modes):

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.FP16)
  converted_graph_def = converter.convert()
  converter.save(output_saved_model_dir)
  ```

  To run the conversion with quantization calibration:

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.INT8)
  converter.convert()

  # Run calibration 10 times.
  converted_graph_def = converter.calibrate(
      fetch_names=['output:0'],
      num_runs=10,
      feed_dict_fn=lambda: {'input:0': my_next_data()})

  converter.save(output_saved_model_dir)
  ```
  """
    string: """A converter for TF-TRT transformation for TF 1.x GraphDef/SavedModels.

  To run the conversion without quantization calibration (e.g. for FP32/FP16
  precision modes):

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.FP16)
  converted_graph_def = converter.convert()
  converter.save(output_saved_model_dir)
  ```

  To run the conversion with quantization calibration:

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.INT8)
  converter.convert()

  # Run calibration 10 times.
  converted_graph_def = converter.calibrate(
      fetch_names=['output:0'],
      num_runs=10,
      feed_dict_fn=lambda: {'input:0': my_next_data()})

  converter.save(output_saved_model_dir)
  ```
  """
     string_start: """
     string_content: A converter for TF-TRT transformation for TF 1.x GraphDef/SavedModels.

  To run the conversion without quantization calibration (e.g. for FP32/FP16
  precision modes):

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.FP16)
  converted_graph_def = converter.convert()
  converter.save(output_saved_model_dir)
  ```

  To run the conversion with quantization calibration:

  ```python
  converter = TrtGraphConverter(
      input_saved_model_dir="my_dir",
      precision_mode=TrtPrecisionMode.INT8)
  converter.convert()

  # Run calibration 10 times.
  converted_graph_def = converter.calibrate(
      fetch_names=['output:0'],
      num_runs=10,
      feed_dict_fn=lambda: {'input:0': my_next_data()})

  converter.save(output_saved_model_dir)
  ```
  
     string_end: """
   function_definition: def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               input_graph_def=None,
               nodes_denylist=None,
               max_batch_size=1,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               is_dynamic_op=False,
               maximum_cached_engines=1,
               use_calibration=True):
    """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
    if context.executing_eagerly():
      raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")

    if input_graph_def and input_saved_model_dir:
      raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
    if not input_graph_def and not input_saved_model_dir:
      raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
    _check_trt_version_compatibility()

    self._input_graph_def = input_graph_def
    self._nodes_denylist = nodes_denylist

    self._input_saved_model_dir = input_saved_model_dir
    self._converted = False
    self._grappler_meta_graph_def = None

    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)

    # For calibration usage.
    self._calibration_graph = None
    self._calibration_data_collected = False
    self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
    if self._need_calibration and not is_dynamic_op:
      logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True

    self._is_dynamic_op = is_dynamic_op
    if is_dynamic_op:
      self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
    else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size

    self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
    _check_conversion_params(self._conversion_params)

    self._test_only_disable_non_trt_optimizers = False
    def: def
    identifier: __init__
    parameters: (self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               input_graph_def=None,
               nodes_denylist=None,
               max_batch_size=1,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               is_dynamic_op=False,
               maximum_cached_engines=1,
               use_calibration=True)
     (: (
     identifier: self
     ,: ,
     default_parameter: input_saved_model_dir=None
      identifier: input_saved_model_dir
      =: =
      none: None
     ,: ,
     default_parameter: input_saved_model_tags=None
      identifier: input_saved_model_tags
      =: =
      none: None
     ,: ,
     default_parameter: input_saved_model_signature_key=None
      identifier: input_saved_model_signature_key
      =: =
      none: None
     ,: ,
     default_parameter: input_graph_def=None
      identifier: input_graph_def
      =: =
      none: None
     ,: ,
     default_parameter: nodes_denylist=None
      identifier: nodes_denylist
      =: =
      none: None
     ,: ,
     default_parameter: max_batch_size=1
      identifier: max_batch_size
      =: =
      integer: 1
     ,: ,
     default_parameter: max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
      identifier: max_workspace_size_bytes
      =: =
      identifier: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
     ,: ,
     default_parameter: precision_mode=TrtPrecisionMode.FP32
      identifier: precision_mode
      =: =
      attribute: TrtPrecisionMode.FP32
       identifier: TrtPrecisionMode
       .: .
       identifier: FP32
     ,: ,
     default_parameter: minimum_segment_size=3
      identifier: minimum_segment_size
      =: =
      integer: 3
     ,: ,
     default_parameter: is_dynamic_op=False
      identifier: is_dynamic_op
      =: =
      false: False
     ,: ,
     default_parameter: maximum_cached_engines=1
      identifier: maximum_cached_engines
      =: =
      integer: 1
     ,: ,
     default_parameter: use_calibration=True
      identifier: use_calibration
      =: =
      true: True
     ): )
    :: :
    block: """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
    if context.executing_eagerly():
      raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")

    if input_graph_def and input_saved_model_dir:
      raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
    if not input_graph_def and not input_saved_model_dir:
      raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
    _check_trt_version_compatibility()

    self._input_graph_def = input_graph_def
    self._nodes_denylist = nodes_denylist

    self._input_saved_model_dir = input_saved_model_dir
    self._converted = False
    self._grappler_meta_graph_def = None

    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)

    # For calibration usage.
    self._calibration_graph = None
    self._calibration_data_collected = False
    self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
    if self._need_calibration and not is_dynamic_op:
      logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True

    self._is_dynamic_op = is_dynamic_op
    if is_dynamic_op:
      self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
    else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size

    self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
    _check_conversion_params(self._conversion_params)

    self._test_only_disable_non_trt_optimizers = False
     expression_statement: """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
      string: """Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    """
       string_start: """
       string_content: Initializes the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Used only when input_graph_def is None.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      input_graph_def: a GraphDef object containing a model to be transformed.
        If set to None, the graph will be read from the SavedModel loaded from
        input_saved_model_dir.
      nodes_denylist: list of node names to prevent the converter from touching.
      max_batch_size: max size for the input batch.
      max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      is_dynamic_op: whether to generate dynamic TRT ops which will build the
        TRT network and engine at run time.
      maximum_cached_engines: max number of cached TRT engines in dynamic TRT
        ops. If the number of cached engines is already at max but none of them
        can serve the input, the TRTEngineOp will fall back to run the TF
        function based on which the TRTEngineOp is created.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.

    Raises:
      ValueError: if the combination of the parameters is invalid.
      RuntimeError: if this class is used in TF 2.0.
    
       string_end: """
     if_statement: if context.executing_eagerly():
      raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")
      if: if
      call: context.executing_eagerly()
       attribute: context.executing_eagerly
        identifier: context
        .: .
        identifier: executing_eagerly
       argument_list: ()
        (: (
        ): )
      :: :
      block: raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")
       raise_statement: raise RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")
        raise: raise
        call: RuntimeError(
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")
         identifier: RuntimeError
         argument_list: (
          "Please use tf.experimental.tensorrt.Converter in TF 2.0.")
          (: (
          string: "Please use tf.experimental.tensorrt.Converter in TF 2.0."
           string_start: "
           string_content: Please use tf.experimental.tensorrt.Converter in TF 2.0.
           string_end: "
          ): )
     if_statement: if input_graph_def and input_saved_model_dir:
      raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
      if: if
      boolean_operator: input_graph_def and input_saved_model_dir
       identifier: input_graph_def
       and: and
       identifier: input_saved_model_dir
      :: :
      block: raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
       raise_statement: raise ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
        raise: raise
        call: ValueError(
          "Can only specify one of input_graph_def and input_saved_model_dir")
         identifier: ValueError
         argument_list: (
          "Can only specify one of input_graph_def and input_saved_model_dir")
          (: (
          string: "Can only specify one of input_graph_def and input_saved_model_dir"
           string_start: "
           string_content: Can only specify one of input_graph_def and input_saved_model_dir
           string_end: "
          ): )
     if_statement: if not input_graph_def and not input_saved_model_dir:
      raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
      if: if
      boolean_operator: not input_graph_def and not input_saved_model_dir
       not_operator: not input_graph_def
        not: not
        identifier: input_graph_def
       and: and
       not_operator: not input_saved_model_dir
        not: not
        identifier: input_saved_model_dir
      :: :
      block: raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
       raise_statement: raise ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
        raise: raise
        call: ValueError("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
         identifier: ValueError
         argument_list: ("Must specify one of input_graph_def and "
                       "input_saved_model_dir")
          (: (
          concatenated_string: "Must specify one of input_graph_def and "
                       "input_saved_model_dir"
           string: "Must specify one of input_graph_def and "
            string_start: "
            string_content: Must specify one of input_graph_def and 
            string_end: "
           string: "input_saved_model_dir"
            string_start: "
            string_content: input_saved_model_dir
            string_end: "
          ): )
     expression_statement: _check_trt_version_compatibility()
      call: _check_trt_version_compatibility()
       identifier: _check_trt_version_compatibility
       argument_list: ()
        (: (
        ): )
     expression_statement: self._input_graph_def = input_graph_def
      assignment: self._input_graph_def = input_graph_def
       attribute: self._input_graph_def
        identifier: self
        .: .
        identifier: _input_graph_def
       =: =
       identifier: input_graph_def
     expression_statement: self._nodes_denylist = nodes_denylist
      assignment: self._nodes_denylist = nodes_denylist
       attribute: self._nodes_denylist
        identifier: self
        .: .
        identifier: _nodes_denylist
       =: =
       identifier: nodes_denylist
     expression_statement: self._input_saved_model_dir = input_saved_model_dir
      assignment: self._input_saved_model_dir = input_saved_model_dir
       attribute: self._input_saved_model_dir
        identifier: self
        .: .
        identifier: _input_saved_model_dir
       =: =
       identifier: input_saved_model_dir
     expression_statement: self._converted = False
      assignment: self._converted = False
       attribute: self._converted
        identifier: self
        .: .
        identifier: _converted
       =: =
       false: False
     expression_statement: self._grappler_meta_graph_def = None
      assignment: self._grappler_meta_graph_def = None
       attribute: self._grappler_meta_graph_def
        identifier: self
        .: .
        identifier: _grappler_meta_graph_def
       =: =
       none: None
     expression_statement: self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
      assignment: self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
       attribute: self._input_saved_model_tags
        identifier: self
        .: .
        identifier: _input_saved_model_tags
       =: =
       parenthesized_expression: (
        input_saved_model_tags or [tag_constants.SERVING])
        (: (
        boolean_operator: input_saved_model_tags or [tag_constants.SERVING]
         identifier: input_saved_model_tags
         or: or
         list: [tag_constants.SERVING]
          [: [
          attribute: tag_constants.SERVING
           identifier: tag_constants
           .: .
           identifier: SERVING
          ]: ]
        ): )
     expression_statement: self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
      assignment: self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
       attribute: self._input_saved_model_signature_key
        identifier: self
        .: .
        identifier: _input_saved_model_signature_key
       =: =
       parenthesized_expression: (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
        (: (
        boolean_operator: input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
         identifier: input_saved_model_signature_key
         or: or
         attribute: signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
          identifier: signature_constants
          .: .
          identifier: DEFAULT_SERVING_SIGNATURE_DEF_KEY
        ): )
     comment: # For calibration usage.
     expression_statement: self._calibration_graph = None
      assignment: self._calibration_graph = None
       attribute: self._calibration_graph
        identifier: self
        .: .
        identifier: _calibration_graph
       =: =
       none: None
     expression_statement: self._calibration_data_collected = False
      assignment: self._calibration_data_collected = False
       attribute: self._calibration_data_collected
        identifier: self
        .: .
        identifier: _calibration_data_collected
       =: =
       false: False
     expression_statement: self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
      assignment: self._need_calibration = (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
       attribute: self._need_calibration
        identifier: self
        .: .
        identifier: _need_calibration
       =: =
       parenthesized_expression: (
        ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration)
        (: (
        boolean_operator: ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())) and use_calibration
         parenthesized_expression: ((precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower()))
          (: (
          boolean_operator: (precision_mode == TrtPrecisionMode.INT8) or
         (precision_mode == TrtPrecisionMode.INT8.lower())
           parenthesized_expression: (precision_mode == TrtPrecisionMode.INT8)
            (: (
            comparison_operator: precision_mode == TrtPrecisionMode.INT8
             identifier: precision_mode
             ==: ==
             attribute: TrtPrecisionMode.INT8
              identifier: TrtPrecisionMode
              .: .
              identifier: INT8
            ): )
           or: or
           parenthesized_expression: (precision_mode == TrtPrecisionMode.INT8.lower())
            (: (
            comparison_operator: precision_mode == TrtPrecisionMode.INT8.lower()
             identifier: precision_mode
             ==: ==
             call: TrtPrecisionMode.INT8.lower()
              attribute: TrtPrecisionMode.INT8.lower
               attribute: TrtPrecisionMode.INT8
                identifier: TrtPrecisionMode
                .: .
                identifier: INT8
               .: .
               identifier: lower
              argument_list: ()
               (: (
               ): )
            ): )
          ): )
         and: and
         identifier: use_calibration
        ): )
     if_statement: if self._need_calibration and not is_dynamic_op:
      logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True
      if: if
      boolean_operator: self._need_calibration and not is_dynamic_op
       attribute: self._need_calibration
        identifier: self
        .: .
        identifier: _need_calibration
       and: and
       not_operator: not is_dynamic_op
        not: not
        identifier: is_dynamic_op
      :: :
      block: logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
      is_dynamic_op = True
       expression_statement: logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
        call: logging.warn(
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
         attribute: logging.warn
          identifier: logging
          .: .
          identifier: warn
         argument_list: (
          "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter.")
          (: (
          concatenated_string: "INT8 precision mode with calibration is supported with "
          "dynamic TRT ops only. Disregarding is_dynamic_op parameter."
           string: "INT8 precision mode with calibration is supported with "
            string_start: "
            string_content: INT8 precision mode with calibration is supported with 
            string_end: "
           string: "dynamic TRT ops only. Disregarding is_dynamic_op parameter."
            string_start: "
            string_content: dynamic TRT ops only. Disregarding is_dynamic_op parameter.
            string_end: "
          ): )
       expression_statement: is_dynamic_op = True
        assignment: is_dynamic_op = True
         identifier: is_dynamic_op
         =: =
         true: True
     expression_statement: self._is_dynamic_op = is_dynamic_op
      assignment: self._is_dynamic_op = is_dynamic_op
       attribute: self._is_dynamic_op
        identifier: self
        .: .
        identifier: _is_dynamic_op
       =: =
       identifier: is_dynamic_op
     if_statement: if is_dynamic_op:
      self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
    else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size
      if: if
      identifier: is_dynamic_op
      :: :
      block: self._max_batch_size = None
      if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
       expression_statement: self._max_batch_size = None
        assignment: self._max_batch_size = None
         attribute: self._max_batch_size
          identifier: self
          .: .
          identifier: _max_batch_size
         =: =
         none: None
       if_statement: if max_batch_size is not None:
        logging.warn("When is_dynamic_op==True max_batch_size should be None")
        if: if
        comparison_operator: max_batch_size is not None
         identifier: max_batch_size
         is not: is not
          is: is
          not: not
         none: None
        :: :
        block: logging.warn("When is_dynamic_op==True max_batch_size should be None")
         expression_statement: logging.warn("When is_dynamic_op==True max_batch_size should be None")
          call: logging.warn("When is_dynamic_op==True max_batch_size should be None")
           attribute: logging.warn
            identifier: logging
            .: .
            identifier: warn
           argument_list: ("When is_dynamic_op==True max_batch_size should be None")
            (: (
            string: "When is_dynamic_op==True max_batch_size should be None"
             string_start: "
             string_content: When is_dynamic_op==True max_batch_size should be None
             string_end: "
            ): )
      else_clause: else:
      if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size
       else: else
       :: :
       block: if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
      self._max_batch_size = max_batch_size
        if_statement: if not isinstance(max_batch_size, int):
        raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
         if: if
         not_operator: not isinstance(max_batch_size, int)
          not: not
          call: isinstance(max_batch_size, int)
           identifier: isinstance
           argument_list: (max_batch_size, int)
            (: (
            identifier: max_batch_size
            ,: ,
            identifier: int
            ): )
         :: :
         block: raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
          raise_statement: raise ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
           raise: raise
           call: ValueError("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
            identifier: ValueError
            argument_list: ("When is_dynamic_op==False max_batch_size should be "
                         "an integer")
             (: (
             concatenated_string: "When is_dynamic_op==False max_batch_size should be "
                         "an integer"
              string: "When is_dynamic_op==False max_batch_size should be "
               string_start: "
               string_content: When is_dynamic_op==False max_batch_size should be 
               string_end: "
              string: "an integer"
               string_start: "
               string_content: an integer
               string_end: "
             ): )
        expression_statement: self._max_batch_size = max_batch_size
         assignment: self._max_batch_size = max_batch_size
          attribute: self._max_batch_size
           identifier: self
           .: .
           identifier: _max_batch_size
          =: =
          identifier: max_batch_size
     expression_statement: self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
      assignment: self._conversion_params = TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
       attribute: self._conversion_params
        identifier: self
        .: .
        identifier: _conversion_params
       =: =
       call: TrtConversionParams(
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
        identifier: TrtConversionParams
        argument_list: (
        max_workspace_size_bytes=max_workspace_size_bytes,
        precision_mode=precision_mode,
        minimum_segment_size=minimum_segment_size,
        maximum_cached_engines=maximum_cached_engines,
        use_calibration=use_calibration,
        allow_build_at_runtime=True)
         (: (
         keyword_argument: max_workspace_size_bytes=max_workspace_size_bytes
          identifier: max_workspace_size_bytes
          =: =
          identifier: max_workspace_size_bytes
         ,: ,
         keyword_argument: precision_mode=precision_mode
          identifier: precision_mode
          =: =
          identifier: precision_mode
         ,: ,
         keyword_argument: minimum_segment_size=minimum_segment_size
          identifier: minimum_segment_size
          =: =
          identifier: minimum_segment_size
         ,: ,
         keyword_argument: maximum_cached_engines=maximum_cached_engines
          identifier: maximum_cached_engines
          =: =
          identifier: maximum_cached_engines
         ,: ,
         keyword_argument: use_calibration=use_calibration
          identifier: use_calibration
          =: =
          identifier: use_calibration
         ,: ,
         keyword_argument: allow_build_at_runtime=True
          identifier: allow_build_at_runtime
          =: =
          true: True
         ): )
     expression_statement: _check_conversion_params(self._conversion_params)
      call: _check_conversion_params(self._conversion_params)
       identifier: _check_conversion_params
       argument_list: (self._conversion_params)
        (: (
        attribute: self._conversion_params
         identifier: self
         .: .
         identifier: _conversion_params
        ): )
     expression_statement: self._test_only_disable_non_trt_optimizers = False
      assignment: self._test_only_disable_non_trt_optimizers = False
       attribute: self._test_only_disable_non_trt_optimizers
        identifier: self
        .: .
        identifier: _test_only_disable_non_trt_optimizers
       =: =
       false: False
   function_definition: def _run_conversion(self):
    """Run Grappler's OptimizeGraph() tool to convert the graph."""
    # Create custom ConfigProto for Grappler.
    grappler_session_config = config_pb2.ConfigProto()
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)

    # Run Grappler.
    self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
    self._converted = True
    def: def
    identifier: _run_conversion
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: """Run Grappler's OptimizeGraph() tool to convert the graph."""
    # Create custom ConfigProto for Grappler.
    grappler_session_config = config_pb2.ConfigProto()
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)

    # Run Grappler.
    self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
    self._converted = True
     expression_statement: """Run Grappler's OptimizeGraph() tool to convert the graph."""
      string: """Run Grappler's OptimizeGraph() tool to convert the graph."""
       string_start: """
       string_content: Run Grappler's OptimizeGraph() tool to convert the graph.
       string_end: """
     comment: # Create custom ConfigProto for Grappler.
     expression_statement: grappler_session_config = config_pb2.ConfigProto()
      assignment: grappler_session_config = config_pb2.ConfigProto()
       identifier: grappler_session_config
       =: =
       call: config_pb2.ConfigProto()
        attribute: config_pb2.ConfigProto
         identifier: config_pb2
         .: .
         identifier: ConfigProto
        argument_list: ()
         (: (
         ): )
     expression_statement: custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
      assignment: custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
       identifier: custom_rewriter_config
       =: =
       call: _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
        identifier: _get_tensorrt_rewriter_config
        argument_list: (
        conversion_params=self._conversion_params,
        is_dynamic_op=self._is_dynamic_op,
        max_batch_size=self._max_batch_size,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=True)
         (: (
         keyword_argument: conversion_params=self._conversion_params
          identifier: conversion_params
          =: =
          attribute: self._conversion_params
           identifier: self
           .: .
           identifier: _conversion_params
         ,: ,
         keyword_argument: is_dynamic_op=self._is_dynamic_op
          identifier: is_dynamic_op
          =: =
          attribute: self._is_dynamic_op
           identifier: self
           .: .
           identifier: _is_dynamic_op
         ,: ,
         keyword_argument: max_batch_size=self._max_batch_size
          identifier: max_batch_size
          =: =
          attribute: self._max_batch_size
           identifier: self
           .: .
           identifier: _max_batch_size
         ,: ,
         keyword_argument: disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers
          identifier: disable_non_trt_optimizers
          =: =
          attribute: self._test_only_disable_non_trt_optimizers
           identifier: self
           .: .
           identifier: _test_only_disable_non_trt_optimizers
         ,: ,
         keyword_argument: use_implicit_batch=True
          identifier: use_implicit_batch
          =: =
          true: True
         ): )
     expression_statement: grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
      call: grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
       attribute: grappler_session_config.graph_options.rewrite_options.CopyFrom
        attribute: grappler_session_config.graph_options.rewrite_options
         attribute: grappler_session_config.graph_options
          identifier: grappler_session_config
          .: .
          identifier: graph_options
         .: .
         identifier: rewrite_options
        .: .
        identifier: CopyFrom
       argument_list: (
        custom_rewriter_config)
        (: (
        identifier: custom_rewriter_config
        ): )
     comment: # Run Grappler.
     expression_statement: self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
      assignment: self._converted_graph_def = tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
       attribute: self._converted_graph_def
        identifier: self
        .: .
        identifier: _converted_graph_def
       =: =
       call: tf_optimizer.OptimizeGraph(
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
        attribute: tf_optimizer.OptimizeGraph
         identifier: tf_optimizer
         .: .
         identifier: OptimizeGraph
        argument_list: (
        grappler_session_config,
        self._grappler_meta_graph_def,
        graph_id=b"tf_graph")
         (: (
         identifier: grappler_session_config
         ,: ,
         attribute: self._grappler_meta_graph_def
          identifier: self
          .: .
          identifier: _grappler_meta_graph_def
         ,: ,
         keyword_argument: graph_id=b"tf_graph"
          identifier: graph_id
          =: =
          string: b"tf_graph"
           string_start: b"
           string_content: tf_graph
           string_end: "
         ): )
     expression_statement: self._converted = True
      assignment: self._converted = True
       attribute: self._converted
        identifier: self
        .: .
        identifier: _converted
       =: =
       true: True
   function_definition: def _add_nodes_denylist(self):
    if self._nodes_denylist:
      collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
    def: def
    identifier: _add_nodes_denylist
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: if self._nodes_denylist:
      collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
     if_statement: if self._nodes_denylist:
      collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
      if: if
      attribute: self._nodes_denylist
       identifier: self
       .: .
       identifier: _nodes_denylist
      :: :
      block: collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
      denylist = collection_def.node_list.value
      for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
       expression_statement: collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
        assignment: collection_def = self._grappler_meta_graph_def.collection_def["train_op"]
         identifier: collection_def
         =: =
         subscript: self._grappler_meta_graph_def.collection_def["train_op"]
          attribute: self._grappler_meta_graph_def.collection_def
           attribute: self._grappler_meta_graph_def
            identifier: self
            .: .
            identifier: _grappler_meta_graph_def
           .: .
           identifier: collection_def
          [: [
          string: "train_op"
           string_start: "
           string_content: train_op
           string_end: "
          ]: ]
       expression_statement: denylist = collection_def.node_list.value
        assignment: denylist = collection_def.node_list.value
         identifier: denylist
         =: =
         attribute: collection_def.node_list.value
          attribute: collection_def.node_list
           identifier: collection_def
           .: .
           identifier: node_list
          .: .
          identifier: value
       for_statement: for i in self._nodes_denylist:
        if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
        for: for
        identifier: i
        in: in
        attribute: self._nodes_denylist
         identifier: self
         .: .
         identifier: _nodes_denylist
        :: :
        block: if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
         if_statement: if isinstance(i, tensor.Tensor):
          denylist.append(_to_bytes(i.name))
        else:
          denylist.append(_to_bytes(i))
          if: if
          call: isinstance(i, tensor.Tensor)
           identifier: isinstance
           argument_list: (i, tensor.Tensor)
            (: (
            identifier: i
            ,: ,
            attribute: tensor.Tensor
             identifier: tensor
             .: .
             identifier: Tensor
            ): )
          :: :
          block: denylist.append(_to_bytes(i.name))
           expression_statement: denylist.append(_to_bytes(i.name))
            call: denylist.append(_to_bytes(i.name))
             attribute: denylist.append
              identifier: denylist
              .: .
              identifier: append
             argument_list: (_to_bytes(i.name))
              (: (
              call: _to_bytes(i.name)
               identifier: _to_bytes
               argument_list: (i.name)
                (: (
                attribute: i.name
                 identifier: i
                 .: .
                 identifier: name
                ): )
              ): )
          else_clause: else:
          denylist.append(_to_bytes(i))
           else: else
           :: :
           block: denylist.append(_to_bytes(i))
            expression_statement: denylist.append(_to_bytes(i))
             call: denylist.append(_to_bytes(i))
              attribute: denylist.append
               identifier: denylist
               .: .
               identifier: append
              argument_list: (_to_bytes(i))
               (: (
               call: _to_bytes(i)
                identifier: _to_bytes
                argument_list: (i)
                 (: (
                 identifier: i
                 ): )
               ): )
   function_definition: def _convert_graph_def(self):
    """Convert the input GraphDef."""
    graph = ops.Graph()
    with graph.as_default():
      importer.import_graph_def(self._input_graph_def, name="")
    self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
    self._add_nodes_denylist()

    self._run_conversion()
    def: def
    identifier: _convert_graph_def
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: """Convert the input GraphDef."""
    graph = ops.Graph()
    with graph.as_default():
      importer.import_graph_def(self._input_graph_def, name="")
    self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
    self._add_nodes_denylist()

    self._run_conversion()
     expression_statement: """Convert the input GraphDef."""
      string: """Convert the input GraphDef."""
       string_start: """
       string_content: Convert the input GraphDef.
       string_end: """
     expression_statement: graph = ops.Graph()
      assignment: graph = ops.Graph()
       identifier: graph
       =: =
       call: ops.Graph()
        attribute: ops.Graph
         identifier: ops
         .: .
         identifier: Graph
        argument_list: ()
         (: (
         ): )
     with_statement: with graph.as_default():
      importer.import_graph_def(self._input_graph_def, name="")
      with: with
      with_clause: graph.as_default()
       with_item: graph.as_default()
        call: graph.as_default()
         attribute: graph.as_default
          identifier: graph
          .: .
          identifier: as_default
         argument_list: ()
          (: (
          ): )
      :: :
      block: importer.import_graph_def(self._input_graph_def, name="")
       expression_statement: importer.import_graph_def(self._input_graph_def, name="")
        call: importer.import_graph_def(self._input_graph_def, name="")
         attribute: importer.import_graph_def
          identifier: importer
          .: .
          identifier: import_graph_def
         argument_list: (self._input_graph_def, name="")
          (: (
          attribute: self._input_graph_def
           identifier: self
           .: .
           identifier: _input_graph_def
          ,: ,
          keyword_argument: name=""
           identifier: name
           =: =
           string: ""
            string_start: "
            string_end: "
          ): )
     expression_statement: self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
      assignment: self._grappler_meta_graph_def = saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
       attribute: self._grappler_meta_graph_def
        identifier: self
        .: .
        identifier: _grappler_meta_graph_def
       =: =
       call: saver.export_meta_graph(
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
        attribute: saver.export_meta_graph
         identifier: saver
         .: .
         identifier: export_meta_graph
        argument_list: (
        graph_def=graph.as_graph_def(add_shapes=True), graph=graph)
         (: (
         keyword_argument: graph_def=graph.as_graph_def(add_shapes=True)
          identifier: graph_def
          =: =
          call: graph.as_graph_def(add_shapes=True)
           attribute: graph.as_graph_def
            identifier: graph
            .: .
            identifier: as_graph_def
           argument_list: (add_shapes=True)
            (: (
            keyword_argument: add_shapes=True
             identifier: add_shapes
             =: =
             true: True
            ): )
         ,: ,
         keyword_argument: graph=graph
          identifier: graph
          =: =
          identifier: graph
         ): )
     expression_statement: self._add_nodes_denylist()
      call: self._add_nodes_denylist()
       attribute: self._add_nodes_denylist
        identifier: self
        .: .
        identifier: _add_nodes_denylist
       argument_list: ()
        (: (
        ): )
     expression_statement: self._run_conversion()
      call: self._run_conversion()
       attribute: self._run_conversion
        identifier: self
        .: .
        identifier: _run_conversion
       argument_list: ()
        (: (
        ): )
   function_definition: def _collections_to_keep(self, collection_keys):
    # TODO(laigd): currently we use the collection key to filter out
    # collections that depend on variable ops, but this may miss some
    # other user-defined collections. A better way would be to use
    # CollectionDef::NodeList for the filtering.
    collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
    return [key for key in collection_keys if key not in collections_to_remove]
    def: def
    identifier: _collections_to_keep
    parameters: (self, collection_keys)
     (: (
     identifier: self
     ,: ,
     identifier: collection_keys
     ): )
    :: :
    comment: # TODO(laigd): currently we use the collection key to filter out
    comment: # collections that depend on variable ops, but this may miss some
    comment: # other user-defined collections. A better way would be to use
    comment: # CollectionDef::NodeList for the filtering.
    block: collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
    return [key for key in collection_keys if key not in collections_to_remove]
     expression_statement: collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
      assignment: collections_to_remove = (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
       identifier: collections_to_remove
       =: =
       parenthesized_expression: (
        ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ])
        (: (
        binary_operator: ops.GraphKeys._VARIABLE_COLLECTIONS + [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ]
         attribute: ops.GraphKeys._VARIABLE_COLLECTIONS
          attribute: ops.GraphKeys
           identifier: ops
           .: .
           identifier: GraphKeys
          .: .
          identifier: _VARIABLE_COLLECTIONS
         +: +
         list: [
            ops.GraphKeys.TRAIN_OP, ops.GraphKeys.WHILE_CONTEXT,
            ops.GraphKeys.COND_CONTEXT
        ]
          [: [
          attribute: ops.GraphKeys.TRAIN_OP
           attribute: ops.GraphKeys
            identifier: ops
            .: .
            identifier: GraphKeys
           .: .
           identifier: TRAIN_OP
          ,: ,
          attribute: ops.GraphKeys.WHILE_CONTEXT
           attribute: ops.GraphKeys
            identifier: ops
            .: .
            identifier: GraphKeys
           .: .
           identifier: WHILE_CONTEXT
          ,: ,
          attribute: ops.GraphKeys.COND_CONTEXT
           attribute: ops.GraphKeys
            identifier: ops
            .: .
            identifier: GraphKeys
           .: .
           identifier: COND_CONTEXT
          ]: ]
        ): )
     return_statement: return [key for key in collection_keys if key not in collections_to_remove]
      return: return
      list_comprehension: [key for key in collection_keys if key not in collections_to_remove]
       [: [
       identifier: key
       for_in_clause: for key in collection_keys
        for: for
        identifier: key
        in: in
        identifier: collection_keys
       if_clause: if key not in collections_to_remove
        if: if
        comparison_operator: key not in collections_to_remove
         identifier: key
         not in: not in
          not: not
          in: in
         identifier: collections_to_remove
       ]: ]
   function_definition: def _convert_saved_model(self):
    """Convert the input SavedModel."""
    graph = ops.Graph()
    with session.Session(graph=graph) as sess:
      input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.

    self._run_conversion()
    def: def
    identifier: _convert_saved_model
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: """Convert the input SavedModel."""
    graph = ops.Graph()
    with session.Session(graph=graph) as sess:
      input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.

    self._run_conversion()
     expression_statement: """Convert the input SavedModel."""
      string: """Convert the input SavedModel."""
       string_start: """
       string_content: Convert the input SavedModel.
       string_end: """
     expression_statement: graph = ops.Graph()
      assignment: graph = ops.Graph()
       identifier: graph
       =: =
       call: ops.Graph()
        attribute: ops.Graph
         identifier: ops
         .: .
         identifier: Graph
        argument_list: ()
         (: (
         ): )
     with_statement: with session.Session(graph=graph) as sess:
      input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.
      with: with
      with_clause: session.Session(graph=graph) as sess
       with_item: session.Session(graph=graph) as sess
        as_pattern: session.Session(graph=graph) as sess
         call: session.Session(graph=graph)
          attribute: session.Session
           identifier: session
           .: .
           identifier: Session
          argument_list: (graph=graph)
           (: (
           keyword_argument: graph=graph
            identifier: graph
            =: =
            identifier: graph
           ): )
         as: as
         as_pattern_target: sess
          identifier: sess
      :: :
      block: input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
      input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]

      def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}

      # Get input and outputs from all SignatureDef.
      output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))

      # Preserve nodes in collection
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])

      # Freeze the variables in the SavedModel graph and copy the frozen
      # graph over.
      frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
      self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
      self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)

      # Copy the collections that are not variables.
      for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])

      self._add_nodes_denylist()

      # Copy other information.
      self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
      self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
      # TODO(laigd): maybe add back AssetFileDef.
       expression_statement: input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
        assignment: input_meta_graph_def = loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
         identifier: input_meta_graph_def
         =: =
         call: loader.load(sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
          attribute: loader.load
           identifier: loader
           .: .
           identifier: load
          argument_list: (sess, self._input_saved_model_tags,
                                         self._input_saved_model_dir)
           (: (
           identifier: sess
           ,: ,
           attribute: self._input_saved_model_tags
            identifier: self
            .: .
            identifier: _input_saved_model_tags
           ,: ,
           attribute: self._input_saved_model_dir
            identifier: self
            .: .
            identifier: _input_saved_model_dir
           ): )
       expression_statement: input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]
        assignment: input_signature_def = input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]
         identifier: input_signature_def
         =: =
         subscript: input_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]
          attribute: input_meta_graph_def.signature_def
           identifier: input_meta_graph_def
           .: .
           identifier: signature_def
          [: [
          attribute: self._input_saved_model_signature_key
           identifier: self
           .: .
           identifier: _input_saved_model_signature_key
          ]: ]
       function_definition: def _gather_names(tensor_info):
        """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}
        def: def
        identifier: _gather_names
        parameters: (tensor_info)
         (: (
         identifier: tensor_info
         ): )
        :: :
        block: """Get the node names from a TensorInfo."""
        return {tensor_info[key].name.split(":")[0] for key in tensor_info}
         expression_statement: """Get the node names from a TensorInfo."""
          string: """Get the node names from a TensorInfo."""
           string_start: """
           string_content: Get the node names from a TensorInfo.
           string_end: """
         return_statement: return {tensor_info[key].name.split(":")[0] for key in tensor_info}
          return: return
          set_comprehension: {tensor_info[key].name.split(":")[0] for key in tensor_info}
           {: {
           subscript: tensor_info[key].name.split(":")[0]
            call: tensor_info[key].name.split(":")
             attribute: tensor_info[key].name.split
              attribute: tensor_info[key].name
               subscript: tensor_info[key]
                identifier: tensor_info
                [: [
                identifier: key
                ]: ]
               .: .
               identifier: name
              .: .
              identifier: split
             argument_list: (":")
              (: (
              string: ":"
               string_start: "
               string_content: :
               string_end: "
              ): )
            [: [
            integer: 0
            ]: ]
           for_in_clause: for key in tensor_info
            for: for
            identifier: key
            in: in
            identifier: tensor_info
           }: }
       comment: # Get input and outputs from all SignatureDef.
       expression_statement: output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))
        assignment: output_node_names = _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))
         identifier: output_node_names
         =: =
         call: _gather_names(input_signature_def.inputs).union(
          _gather_names(input_signature_def.outputs))
          attribute: _gather_names(input_signature_def.inputs).union
           call: _gather_names(input_signature_def.inputs)
            identifier: _gather_names
            argument_list: (input_signature_def.inputs)
             (: (
             attribute: input_signature_def.inputs
              identifier: input_signature_def
              .: .
              identifier: inputs
             ): )
           .: .
           identifier: union
          argument_list: (
          _gather_names(input_signature_def.outputs))
           (: (
           call: _gather_names(input_signature_def.outputs)
            identifier: _gather_names
            argument_list: (input_signature_def.outputs)
             (: (
             attribute: input_signature_def.outputs
              identifier: input_signature_def
              .: .
              identifier: outputs
             ): )
           ): )
       comment: # Preserve nodes in collection
       for_statement: for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])
        for: for
        identifier: collection_key
        in: in
        call: self._collections_to_keep(
          input_meta_graph_def.collection_def)
         attribute: self._collections_to_keep
          identifier: self
          .: .
          identifier: _collections_to_keep
         argument_list: (
          input_meta_graph_def.collection_def)
          (: (
          attribute: input_meta_graph_def.collection_def
           identifier: input_meta_graph_def
           .: .
           identifier: collection_def
          ): )
        :: :
        block: for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])
         for_statement: for op in sess.graph.get_collection(collection_key):
          if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])
          for: for
          identifier: op
          in: in
          call: sess.graph.get_collection(collection_key)
           attribute: sess.graph.get_collection
            attribute: sess.graph
             identifier: sess
             .: .
             identifier: graph
            .: .
            identifier: get_collection
           argument_list: (collection_key)
            (: (
            identifier: collection_key
            ): )
          :: :
          block: if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])
           if_statement: if isinstance(op, ops.Operation):
            output_node_names.add(op.name.split(":")[0])
            if: if
            call: isinstance(op, ops.Operation)
             identifier: isinstance
             argument_list: (op, ops.Operation)
              (: (
              identifier: op
              ,: ,
              attribute: ops.Operation
               identifier: ops
               .: .
               identifier: Operation
              ): )
            :: :
            block: output_node_names.add(op.name.split(":")[0])
             expression_statement: output_node_names.add(op.name.split(":")[0])
              call: output_node_names.add(op.name.split(":")[0])
               attribute: output_node_names.add
                identifier: output_node_names
                .: .
                identifier: add
               argument_list: (op.name.split(":")[0])
                (: (
                subscript: op.name.split(":")[0]
                 call: op.name.split(":")
                  attribute: op.name.split
                   attribute: op.name
                    identifier: op
                    .: .
                    identifier: name
                   .: .
                   identifier: split
                  argument_list: (":")
                   (: (
                   string: ":"
                    string_start: "
                    string_content: :
                    string_end: "
                   ): )
                 [: [
                 integer: 0
                 ]: ]
                ): )
       comment: # Freeze the variables in the SavedModel graph and copy the frozen
       comment: # graph over.
       expression_statement: frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
        assignment: frozen_graph_def = convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
         identifier: frozen_graph_def
         =: =
         call: convert_to_constants.convert_variables_to_constants(
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
          attribute: convert_to_constants.convert_variables_to_constants
           identifier: convert_to_constants
           .: .
           identifier: convert_variables_to_constants
          argument_list: (
          sess, sess.graph.as_graph_def(add_shapes=True),
          list(output_node_names))
           (: (
           identifier: sess
           ,: ,
           call: sess.graph.as_graph_def(add_shapes=True)
            attribute: sess.graph.as_graph_def
             attribute: sess.graph
              identifier: sess
              .: .
              identifier: graph
             .: .
             identifier: as_graph_def
            argument_list: (add_shapes=True)
             (: (
             keyword_argument: add_shapes=True
              identifier: add_shapes
              =: =
              true: True
             ): )
           ,: ,
           call: list(output_node_names)
            identifier: list
            argument_list: (output_node_names)
             (: (
             identifier: output_node_names
             ): )
           ): )
       expression_statement: self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
        assignment: self._grappler_meta_graph_def = meta_graph_pb2.MetaGraphDef()
         attribute: self._grappler_meta_graph_def
          identifier: self
          .: .
          identifier: _grappler_meta_graph_def
         =: =
         call: meta_graph_pb2.MetaGraphDef()
          attribute: meta_graph_pb2.MetaGraphDef
           identifier: meta_graph_pb2
           .: .
           identifier: MetaGraphDef
          argument_list: ()
           (: (
           ): )
       expression_statement: self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)
        call: self._grappler_meta_graph_def.graph_def.CopyFrom(frozen_graph_def)
         attribute: self._grappler_meta_graph_def.graph_def.CopyFrom
          attribute: self._grappler_meta_graph_def.graph_def
           attribute: self._grappler_meta_graph_def
            identifier: self
            .: .
            identifier: _grappler_meta_graph_def
           .: .
           identifier: graph_def
          .: .
          identifier: CopyFrom
         argument_list: (frozen_graph_def)
          (: (
          identifier: frozen_graph_def
          ): )
       comment: # Copy the collections that are not variables.
       for_statement: for collection_key in self._collections_to_keep(
          input_meta_graph_def.collection_def):
        self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])
        for: for
        identifier: collection_key
        in: in
        call: self._collections_to_keep(
          input_meta_graph_def.collection_def)
         attribute: self._collections_to_keep
          identifier: self
          .: .
          identifier: _collections_to_keep
         argument_list: (
          input_meta_graph_def.collection_def)
          (: (
          attribute: input_meta_graph_def.collection_def
           identifier: input_meta_graph_def
           .: .
           identifier: collection_def
          ): )
        :: :
        block: self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])
         expression_statement: self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])
          call: self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom(
            input_meta_graph_def.collection_def[collection_key])
           attribute: self._grappler_meta_graph_def.collection_def[collection_key].CopyFrom
            subscript: self._grappler_meta_graph_def.collection_def[collection_key]
             attribute: self._grappler_meta_graph_def.collection_def
              attribute: self._grappler_meta_graph_def
               identifier: self
               .: .
               identifier: _grappler_meta_graph_def
              .: .
              identifier: collection_def
             [: [
             identifier: collection_key
             ]: ]
            .: .
            identifier: CopyFrom
           argument_list: (
            input_meta_graph_def.collection_def[collection_key])
            (: (
            subscript: input_meta_graph_def.collection_def[collection_key]
             attribute: input_meta_graph_def.collection_def
              identifier: input_meta_graph_def
              .: .
              identifier: collection_def
             [: [
             identifier: collection_key
             ]: ]
            ): )
       expression_statement: self._add_nodes_denylist()
        call: self._add_nodes_denylist()
         attribute: self._add_nodes_denylist
          identifier: self
          .: .
          identifier: _add_nodes_denylist
         argument_list: ()
          (: (
          ): )
       comment: # Copy other information.
       expression_statement: self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
        call: self._grappler_meta_graph_def.meta_info_def.CopyFrom(
          input_meta_graph_def.meta_info_def)
         attribute: self._grappler_meta_graph_def.meta_info_def.CopyFrom
          attribute: self._grappler_meta_graph_def.meta_info_def
           attribute: self._grappler_meta_graph_def
            identifier: self
            .: .
            identifier: _grappler_meta_graph_def
           .: .
           identifier: meta_info_def
          .: .
          identifier: CopyFrom
         argument_list: (
          input_meta_graph_def.meta_info_def)
          (: (
          attribute: input_meta_graph_def.meta_info_def
           identifier: input_meta_graph_def
           .: .
           identifier: meta_info_def
          ): )
       expression_statement: self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
        call: self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom(input_signature_def)
         attribute: self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key].CopyFrom
          subscript: self._grappler_meta_graph_def.signature_def[
          self._input_saved_model_signature_key]
           attribute: self._grappler_meta_graph_def.signature_def
            attribute: self._grappler_meta_graph_def
             identifier: self
             .: .
             identifier: _grappler_meta_graph_def
            .: .
            identifier: signature_def
           [: [
           attribute: self._input_saved_model_signature_key
            identifier: self
            .: .
            identifier: _input_saved_model_signature_key
           ]: ]
          .: .
          identifier: CopyFrom
         argument_list: (input_signature_def)
          (: (
          identifier: input_signature_def
          ): )
       comment: # TODO(laigd): maybe add back AssetFileDef.
     expression_statement: self._run_conversion()
      call: self._run_conversion()
       attribute: self._run_conversion
        identifier: self
        .: .
        identifier: _run_conversion
       argument_list: ()
        (: (
        ): )
   function_definition: def convert(self):
    """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
    assert not self._converted
    if self._input_graph_def:
      self._convert_graph_def()
    else:
      self._convert_saved_model()
    return self._converted_graph_def
    def: def
    identifier: convert
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
    assert not self._converted
    if self._input_graph_def:
      self._convert_graph_def()
    else:
      self._convert_saved_model()
    return self._converted_graph_def
     expression_statement: """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
      string: """Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    """
       string_start: """
       string_content: Run the TF-TRT conversion.

    Returns:
      The converted GraphDef for TF 1.x.
    
       string_end: """
     assert_statement: assert not self._converted
      assert: assert
      not_operator: not self._converted
       not: not
       attribute: self._converted
        identifier: self
        .: .
        identifier: _converted
     if_statement: if self._input_graph_def:
      self._convert_graph_def()
    else:
      self._convert_saved_model()
      if: if
      attribute: self._input_graph_def
       identifier: self
       .: .
       identifier: _input_graph_def
      :: :
      block: self._convert_graph_def()
       expression_statement: self._convert_graph_def()
        call: self._convert_graph_def()
         attribute: self._convert_graph_def
          identifier: self
          .: .
          identifier: _convert_graph_def
         argument_list: ()
          (: (
          ): )
      else_clause: else:
      self._convert_saved_model()
       else: else
       :: :
       block: self._convert_saved_model()
        expression_statement: self._convert_saved_model()
         call: self._convert_saved_model()
          attribute: self._convert_saved_model
           identifier: self
           .: .
           identifier: _convert_saved_model
          argument_list: ()
           (: (
           ): )
     return_statement: return self._converted_graph_def
      return: return
      attribute: self._converted_graph_def
       identifier: self
       .: .
       identifier: _converted_graph_def
   function_definition: def calibrate(self,
                fetch_names,
                num_runs,
                feed_dict_fn=None,
                input_map_fn=None):
    """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
    assert self._converted
    assert self._need_calibration
    assert not self._calibration_data_collected

    if (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn):
      raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")

    if input_map_fn:
      for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")

    self._calibration_graph = ops.Graph()
    with self._calibration_graph.as_default():
      fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")

    calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
    if self._test_only_disable_non_trt_optimizers:
      trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)

    # Set allow_soft_placement=True to run the graph for calibration so that
    # OPs supported by TensorRT but don't have a GPU implementation are allowed
    # to execute on CPU.
    calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))

    with session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess:
      for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True

    return self._converted_graph_def
    def: def
    identifier: calibrate
    parameters: (self,
                fetch_names,
                num_runs,
                feed_dict_fn=None,
                input_map_fn=None)
     (: (
     identifier: self
     ,: ,
     identifier: fetch_names
     ,: ,
     identifier: num_runs
     ,: ,
     default_parameter: feed_dict_fn=None
      identifier: feed_dict_fn
      =: =
      none: None
     ,: ,
     default_parameter: input_map_fn=None
      identifier: input_map_fn
      =: =
      none: None
     ): )
    :: :
    block: """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
    assert self._converted
    assert self._need_calibration
    assert not self._calibration_data_collected

    if (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn):
      raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")

    if input_map_fn:
      for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")

    self._calibration_graph = ops.Graph()
    with self._calibration_graph.as_default():
      fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")

    calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
    if self._test_only_disable_non_trt_optimizers:
      trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)

    # Set allow_soft_placement=True to run the graph for calibration so that
    # OPs supported by TensorRT but don't have a GPU implementation are allowed
    # to execute on CPU.
    calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))

    with session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess:
      for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True

    return self._converted_graph_def
     expression_statement: """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
      string: """Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    """
       string_start: """
       string_content: Run the calibration and return the calibrated GraphDef.

    Args:
      fetch_names: a list of output tensor name to fetch during calibration.
      num_runs: number of runs of the graph during calibration.
      feed_dict_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to values (e.g. Python list,
        numpy arrays, etc). One and only one of `feed_dict_fn` and
        `input_map_fn` should be specified.
      input_map_fn: a function that returns a dictionary mapping input names (as
        strings) in the GraphDef to be calibrated to Tensor objects. The values
        of the named input tensors in the GraphDef to be calibrated will be
        re-mapped to the respective `Tensor` values during calibration. One and
        only one of `feed_dict_fn` and `input_map_fn` should be specified.

    Raises:
      ValueError: if the input combination is invalid.
      RuntimeError: if this method is called in eager mode.

    Returns:
      The GraphDef after the calibration.
    
       string_end: """
     assert_statement: assert self._converted
      assert: assert
      attribute: self._converted
       identifier: self
       .: .
       identifier: _converted
     assert_statement: assert self._need_calibration
      assert: assert
      attribute: self._need_calibration
       identifier: self
       .: .
       identifier: _need_calibration
     assert_statement: assert not self._calibration_data_collected
      assert: assert
      not_operator: not self._calibration_data_collected
       not: not
       attribute: self._calibration_data_collected
        identifier: self
        .: .
        identifier: _calibration_data_collected
     if_statement: if (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn):
      raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")
      if: if
      boolean_operator: (feed_dict_fn and input_map_fn) or (not feed_dict_fn and
                                           not input_map_fn)
       parenthesized_expression: (feed_dict_fn and input_map_fn)
        (: (
        boolean_operator: feed_dict_fn and input_map_fn
         identifier: feed_dict_fn
         and: and
         identifier: input_map_fn
        ): )
       or: or
       parenthesized_expression: (not feed_dict_fn and
                                           not input_map_fn)
        (: (
        boolean_operator: not feed_dict_fn and
                                           not input_map_fn
         not_operator: not feed_dict_fn
          not: not
          identifier: feed_dict_fn
         and: and
         not_operator: not input_map_fn
          not: not
          identifier: input_map_fn
        ): )
      :: :
      block: raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")
       raise_statement: raise ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")
        raise: raise
        call: ValueError(
          "Should specify one and only one of feed_dict_fn and input_map_fn.")
         identifier: ValueError
         argument_list: (
          "Should specify one and only one of feed_dict_fn and input_map_fn.")
          (: (
          string: "Should specify one and only one of feed_dict_fn and input_map_fn."
           string_start: "
           string_content: Should specify one and only one of feed_dict_fn and input_map_fn.
           string_end: "
          ): )
     if_statement: if input_map_fn:
      for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")
      if: if
      identifier: input_map_fn
      :: :
      block: for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")
       for_statement: for k, v in input_map_fn().items():
        if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")
        for: for
        pattern_list: k, v
         identifier: k
         ,: ,
         identifier: v
        in: in
        call: input_map_fn().items()
         attribute: input_map_fn().items
          call: input_map_fn()
           identifier: input_map_fn
           argument_list: ()
            (: (
            ): )
          .: .
          identifier: items
         argument_list: ()
          (: (
          ): )
        :: :
        block: if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
        if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")
         if_statement: if not isinstance(k, str):
          raise ValueError("Keys of input_map_fn must be of type str")
          if: if
          not_operator: not isinstance(k, str)
           not: not
           call: isinstance(k, str)
            identifier: isinstance
            argument_list: (k, str)
             (: (
             identifier: k
             ,: ,
             identifier: str
             ): )
          :: :
          block: raise ValueError("Keys of input_map_fn must be of type str")
           raise_statement: raise ValueError("Keys of input_map_fn must be of type str")
            raise: raise
            call: ValueError("Keys of input_map_fn must be of type str")
             identifier: ValueError
             argument_list: ("Keys of input_map_fn must be of type str")
              (: (
              string: "Keys of input_map_fn must be of type str"
               string_start: "
               string_content: Keys of input_map_fn must be of type str
               string_end: "
              ): )
         if_statement: if not isinstance(v, tensor.Tensor):
          raise ValueError("Values of input_map_fn must be of type tf.Tensor")
          if: if
          not_operator: not isinstance(v, tensor.Tensor)
           not: not
           call: isinstance(v, tensor.Tensor)
            identifier: isinstance
            argument_list: (v, tensor.Tensor)
             (: (
             identifier: v
             ,: ,
             attribute: tensor.Tensor
              identifier: tensor
              .: .
              identifier: Tensor
             ): )
          :: :
          block: raise ValueError("Values of input_map_fn must be of type tf.Tensor")
           raise_statement: raise ValueError("Values of input_map_fn must be of type tf.Tensor")
            raise: raise
            call: ValueError("Values of input_map_fn must be of type tf.Tensor")
             identifier: ValueError
             argument_list: ("Values of input_map_fn must be of type tf.Tensor")
              (: (
              string: "Values of input_map_fn must be of type tf.Tensor"
               string_start: "
               string_content: Values of input_map_fn must be of type tf.Tensor
               string_end: "
              ): )
     expression_statement: self._calibration_graph = ops.Graph()
      assignment: self._calibration_graph = ops.Graph()
       attribute: self._calibration_graph
        identifier: self
        .: .
        identifier: _calibration_graph
       =: =
       call: ops.Graph()
        attribute: ops.Graph
         identifier: ops
         .: .
         identifier: Graph
        argument_list: ()
         (: (
         ): )
     with_statement: with self._calibration_graph.as_default():
      fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")
      with: with
      with_clause: self._calibration_graph.as_default()
       with_item: self._calibration_graph.as_default()
        call: self._calibration_graph.as_default()
         attribute: self._calibration_graph.as_default
          attribute: self._calibration_graph
           identifier: self
           .: .
           identifier: _calibration_graph
          .: .
          identifier: as_default
         argument_list: ()
          (: (
          ): )
      :: :
      block: fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")
       expression_statement: fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")
        assignment: fetches = importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")
         identifier: fetches
         =: =
         call: importer.import_graph_def(
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")
          attribute: importer.import_graph_def
           identifier: importer
           .: .
           identifier: import_graph_def
          argument_list: (
          self._converted_graph_def,
          input_map=input_map_fn() if input_map_fn else None,
          return_elements=fetch_names,
          name="")
           (: (
           attribute: self._converted_graph_def
            identifier: self
            .: .
            identifier: _converted_graph_def
           ,: ,
           keyword_argument: input_map=input_map_fn() if input_map_fn else None
            identifier: input_map
            =: =
            conditional_expression: input_map_fn() if input_map_fn else None
             call: input_map_fn()
              identifier: input_map_fn
              argument_list: ()
               (: (
               ): )
             if: if
             identifier: input_map_fn
             else: else
             none: None
           ,: ,
           keyword_argument: return_elements=fetch_names
            identifier: return_elements
            =: =
            identifier: fetch_names
           ,: ,
           keyword_argument: name=""
            identifier: name
            =: =
            string: ""
             string_start: "
             string_end: "
           ): )
     expression_statement: calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
      assignment: calibrate_rewriter_cfg = rewriter_config_pb2.RewriterConfig()
       identifier: calibrate_rewriter_cfg
       =: =
       call: rewriter_config_pb2.RewriterConfig()
        attribute: rewriter_config_pb2.RewriterConfig
         identifier: rewriter_config_pb2
         .: .
         identifier: RewriterConfig
        argument_list: ()
         (: (
         ): )
     if_statement: if self._test_only_disable_non_trt_optimizers:
      trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)
      if: if
      attribute: self._test_only_disable_non_trt_optimizers
       identifier: self
       .: .
       identifier: _test_only_disable_non_trt_optimizers
      :: :
      block: trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)
       expression_statement: trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)
        call: trt_utils.disable_non_trt_optimizers_in_rewriter_config(
          calibrate_rewriter_cfg)
         attribute: trt_utils.disable_non_trt_optimizers_in_rewriter_config
          identifier: trt_utils
          .: .
          identifier: disable_non_trt_optimizers_in_rewriter_config
         argument_list: (
          calibrate_rewriter_cfg)
          (: (
          identifier: calibrate_rewriter_cfg
          ): )
     comment: # Set allow_soft_placement=True to run the graph for calibration so that
     comment: # OPs supported by TensorRT but don't have a GPU implementation are allowed
     comment: # to execute on CPU.
     expression_statement: calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))
      assignment: calibrate_config = config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))
       identifier: calibrate_config
       =: =
       call: config_pb2.ConfigProto(
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))
        attribute: config_pb2.ConfigProto
         identifier: config_pb2
         .: .
         identifier: ConfigProto
        argument_list: (
        allow_soft_placement=True,
        graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg))
         (: (
         keyword_argument: allow_soft_placement=True
          identifier: allow_soft_placement
          =: =
          true: True
         ,: ,
         keyword_argument: graph_options=config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg)
          identifier: graph_options
          =: =
          call: config_pb2.GraphOptions(
            rewrite_options=calibrate_rewriter_cfg)
           attribute: config_pb2.GraphOptions
            identifier: config_pb2
            .: .
            identifier: GraphOptions
           argument_list: (
            rewrite_options=calibrate_rewriter_cfg)
            (: (
            keyword_argument: rewrite_options=calibrate_rewriter_cfg
             identifier: rewrite_options
             =: =
             identifier: calibrate_rewriter_cfg
            ): )
         ): )
     with_statement: with session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess:
      for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True
      with: with
      with_clause: session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess
       with_item: session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess
        as_pattern: session.Session(
        graph=self._calibration_graph,
        config=calibrate_config) as calibration_sess
         call: session.Session(
        graph=self._calibration_graph,
        config=calibrate_config)
          attribute: session.Session
           identifier: session
           .: .
           identifier: Session
          argument_list: (
        graph=self._calibration_graph,
        config=calibrate_config)
           (: (
           keyword_argument: graph=self._calibration_graph
            identifier: graph
            =: =
            attribute: self._calibration_graph
             identifier: self
             .: .
             identifier: _calibration_graph
           ,: ,
           keyword_argument: config=calibrate_config
            identifier: config
            =: =
            identifier: calibrate_config
           ): )
         as: as
         as_pattern_target: calibration_sess
          identifier: calibration_sess
      :: :
      block: for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)

      # Maps device name to the corresponding get_calibration_data.
      #
      # TODO(laigd): a better way would be to use calibration_sess to list
      # all the devices, add one get_calibration_data for each device, and
      # fetch each such op for every resource until its found. This can work
      # even when the device of the TRTEngineOp is empty or not fully specified.
      device_to_get_resource_op_map = {}

      with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result

      self._calibration_data_collected = True
       for_statement: for _ in range(num_runs):
        calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)
        for: for
        identifier: _
        in: in
        call: range(num_runs)
         identifier: range
         argument_list: (num_runs)
          (: (
          identifier: num_runs
          ): )
        :: :
        block: calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)
         expression_statement: calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)
          call: calibration_sess.run(
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)
           attribute: calibration_sess.run
            identifier: calibration_sess
            .: .
            identifier: run
           argument_list: (
            fetches, feed_dict=feed_dict_fn() if feed_dict_fn else None)
            (: (
            identifier: fetches
            ,: ,
            keyword_argument: feed_dict=feed_dict_fn() if feed_dict_fn else None
             identifier: feed_dict
             =: =
             conditional_expression: feed_dict_fn() if feed_dict_fn else None
              call: feed_dict_fn()
               identifier: feed_dict_fn
               argument_list: ()
                (: (
                ): )
              if: if
              identifier: feed_dict_fn
              else: else
              none: None
            ): )
       comment: # Maps device name to the corresponding get_calibration_data.
       comment: #
       comment: # TODO(laigd): a better way would be to use calibration_sess to list
       comment: # all the devices, add one get_calibration_data for each device, and
       comment: # fetch each such op for every resource until its found. This can work
       comment: # even when the device of the TRTEngineOp is empty or not fully specified.
       expression_statement: device_to_get_resource_op_map = {}
        assignment: device_to_get_resource_op_map = {}
         identifier: device_to_get_resource_op_map
         =: =
         dictionary: {}
          {: {
          }: }
       with_statement: with self._calibration_graph.as_default():
        resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result
        with: with
        with_clause: self._calibration_graph.as_default()
         with_item: self._calibration_graph.as_default()
          call: self._calibration_graph.as_default()
           attribute: self._calibration_graph.as_default
            attribute: self._calibration_graph
             identifier: self
             .: .
             identifier: _calibration_graph
            .: .
            identifier: as_default
           argument_list: ()
            (: (
            ): )
        :: :
        block: resource_name_input = array_ops.placeholder(dtypes.string)

        for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result
         expression_statement: resource_name_input = array_ops.placeholder(dtypes.string)
          assignment: resource_name_input = array_ops.placeholder(dtypes.string)
           identifier: resource_name_input
           =: =
           call: array_ops.placeholder(dtypes.string)
            attribute: array_ops.placeholder
             identifier: array_ops
             .: .
             identifier: placeholder
            argument_list: (dtypes.string)
             (: (
             attribute: dtypes.string
              identifier: dtypes
              .: .
              identifier: string
             ): )
         for_statement: for node in self._converted_graph_def.node:
          if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result
          for: for
          identifier: node
          in: in
          attribute: self._converted_graph_def.node
           attribute: self._converted_graph_def
            identifier: self
            .: .
            identifier: _converted_graph_def
           .: .
           identifier: node
          :: :
          block: if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result
           if_statement: if node.op == _TRT_ENGINE_OP_NAME:
            # Adds the get_calibration_data op for the device if not done
            # before. We only add one such op for each device.
            # TODO(laigd): What if the device is empty?????
            if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result
            if: if
            comparison_operator: node.op == _TRT_ENGINE_OP_NAME
             attribute: node.op
              identifier: node
              .: .
              identifier: op
             ==: ==
             identifier: _TRT_ENGINE_OP_NAME
            :: :
            comment: # Adds the get_calibration_data op for the device if not done
            comment: # before. We only add one such op for each device.
            comment: # TODO(laigd): What if the device is empty?????
            block: if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)

            # Get the calibration resource.
            calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
            node.attr["calibration_data"].s = calibration_result
             if_statement: if node.device not in device_to_get_resource_op_map:
              with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)
              if: if
              comparison_operator: node.device not in device_to_get_resource_op_map
               attribute: node.device
                identifier: node
                .: .
                identifier: device
               not in: not in
                not: not
                in: in
               identifier: device_to_get_resource_op_map
              :: :
              block: with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
              device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)
               with_statement: with self._calibration_graph.device(node.device):
                serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
                with: with
                with_clause: self._calibration_graph.device(node.device)
                 with_item: self._calibration_graph.device(node.device)
                  call: self._calibration_graph.device(node.device)
                   attribute: self._calibration_graph.device
                    attribute: self._calibration_graph
                     identifier: self
                     .: .
                     identifier: _calibration_graph
                    .: .
                    identifier: device
                   argument_list: (node.device)
                    (: (
                    attribute: node.device
                     identifier: node
                     .: .
                     identifier: device
                    ): )
                :: :
                block: serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
                 expression_statement: serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
                  assignment: serialized_resources_output = (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
                   identifier: serialized_resources_output
                   =: =
                   parenthesized_expression: (
                    gen_trt_ops.get_calibration_data_op(resource_name_input))
                    (: (
                    call: gen_trt_ops.get_calibration_data_op(resource_name_input)
                     attribute: gen_trt_ops.get_calibration_data_op
                      identifier: gen_trt_ops
                      .: .
                      identifier: get_calibration_data_op
                     argument_list: (resource_name_input)
                      (: (
                      identifier: resource_name_input
                      ): )
                    ): )
               expression_statement: device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)
                assignment: device_to_get_resource_op_map[node.device] = (
                  serialized_resources_output)
                 subscript: device_to_get_resource_op_map[node.device]
                  identifier: device_to_get_resource_op_map
                  [: [
                  attribute: node.device
                   identifier: node
                   .: .
                   identifier: device
                  ]: ]
                 =: =
                 parenthesized_expression: (
                  serialized_resources_output)
                  (: (
                  identifier: serialized_resources_output
                  ): )
             comment: # Get the calibration resource.
             expression_statement: calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
              assignment: calibration_result = calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
               identifier: calibration_result
               =: =
               call: calibration_sess.run(
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
                attribute: calibration_sess.run
                 identifier: calibration_sess
                 .: .
                 identifier: run
                argument_list: (
                device_to_get_resource_op_map[node.device],
                feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                })
                 (: (
                 subscript: device_to_get_resource_op_map[node.device]
                  identifier: device_to_get_resource_op_map
                  [: [
                  attribute: node.device
                   identifier: node
                   .: .
                   identifier: device
                  ]: ]
                 ,: ,
                 keyword_argument: feed_dict={
                    resource_name_input: _get_canonical_engine_name(node.name)
                }
                  identifier: feed_dict
                  =: =
                  dictionary: {
                    resource_name_input: _get_canonical_engine_name(node.name)
                }
                   {: {
                   pair: resource_name_input: _get_canonical_engine_name(node.name)
                    identifier: resource_name_input
                    :: :
                    call: _get_canonical_engine_name(node.name)
                     identifier: _get_canonical_engine_name
                     argument_list: (node.name)
                      (: (
                      attribute: node.name
                       identifier: node
                       .: .
                       identifier: name
                      ): )
                   }: }
                 ): )
             expression_statement: node.attr["calibration_data"].s = calibration_result
              assignment: node.attr["calibration_data"].s = calibration_result
               attribute: node.attr["calibration_data"].s
                subscript: node.attr["calibration_data"]
                 attribute: node.attr
                  identifier: node
                  .: .
                  identifier: attr
                 [: [
                 string: "calibration_data"
                  string_start: "
                  string_content: calibration_data
                  string_end: "
                 ]: ]
                .: .
                identifier: s
               =: =
               identifier: calibration_result
       expression_statement: self._calibration_data_collected = True
        assignment: self._calibration_data_collected = True
         attribute: self._calibration_data_collected
          identifier: self
          .: .
          identifier: _calibration_data_collected
         =: =
         true: True
     return_statement: return self._converted_graph_def
      return: return
      attribute: self._converted_graph_def
       identifier: self
       .: .
       identifier: _converted_graph_def
   function_definition: def save(self, output_saved_model_dir):
    """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
    assert self._converted
    if self._need_calibration:
      assert self._calibration_data_collected
    if self._input_graph_def:
      raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")

    def _restore_collections(dest_graph, src_meta_graph_def, collection_keys):
      """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))

    # Write the transformed graphdef as SavedModel.
    saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
    with ops.Graph().as_default():
      importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
    # Ignore other meta graphs from the input SavedModel.
    saved_model_builder.save()
    def: def
    identifier: save
    parameters: (self, output_saved_model_dir)
     (: (
     identifier: self
     ,: ,
     identifier: output_saved_model_dir
     ): )
    :: :
    block: """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
    assert self._converted
    if self._need_calibration:
      assert self._calibration_data_collected
    if self._input_graph_def:
      raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")

    def _restore_collections(dest_graph, src_meta_graph_def, collection_keys):
      """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))

    # Write the transformed graphdef as SavedModel.
    saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
    with ops.Graph().as_default():
      importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
    # Ignore other meta graphs from the input SavedModel.
    saved_model_builder.save()
     expression_statement: """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
      string: """Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    """
       string_start: """
       string_content: Save the converted graph as a SavedModel.

    Args:
      output_saved_model_dir: construct a SavedModel using the converted
        GraphDef and save it to the specified directory. This option only works
        when the input graph is loaded from a SavedModel, i.e. when
        input_saved_model_dir is specified and input_graph_def is None in
        __init__().

    Raises:
      ValueError: if the input to the converter is a GraphDef instead of a
      SavedModel.
    
       string_end: """
     assert_statement: assert self._converted
      assert: assert
      attribute: self._converted
       identifier: self
       .: .
       identifier: _converted
     if_statement: if self._need_calibration:
      assert self._calibration_data_collected
      if: if
      attribute: self._need_calibration
       identifier: self
       .: .
       identifier: _need_calibration
      :: :
      block: assert self._calibration_data_collected
       assert_statement: assert self._calibration_data_collected
        assert: assert
        attribute: self._calibration_data_collected
         identifier: self
         .: .
         identifier: _calibration_data_collected
     if_statement: if self._input_graph_def:
      raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")
      if: if
      attribute: self._input_graph_def
       identifier: self
       .: .
       identifier: _input_graph_def
      :: :
      block: raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")
       raise_statement: raise ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")
        raise: raise
        call: ValueError(
          "Not able to save to a SavedModel since input is a GraphDef")
         identifier: ValueError
         argument_list: (
          "Not able to save to a SavedModel since input is a GraphDef")
          (: (
          string: "Not able to save to a SavedModel since input is a GraphDef"
           string_start: "
           string_content: Not able to save to a SavedModel since input is a GraphDef
           string_end: "
          ): )
     function_definition: def _restore_collections(dest_graph, src_meta_graph_def, collection_keys):
      """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
      def: def
      identifier: _restore_collections
      parameters: (dest_graph, src_meta_graph_def, collection_keys)
       (: (
       identifier: dest_graph
       ,: ,
       identifier: src_meta_graph_def
       ,: ,
       identifier: collection_keys
       ): )
      :: :
      block: """Restores collections that we need to keep."""
      scope = ""
      for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
       expression_statement: """Restores collections that we need to keep."""
        string: """Restores collections that we need to keep."""
         string_start: """
         string_content: Restores collections that we need to keep.
         string_end: """
       expression_statement: scope = ""
        assignment: scope = ""
         identifier: scope
         =: =
         string: ""
          string_start: "
          string_end: "
       for_statement: for key in collection_keys:
        collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
        for: for
        identifier: key
        in: in
        identifier: collection_keys
        :: :
        block: collection_def = src_meta_graph_def.collection_def[key]
        kind = collection_def.WhichOneof("kind")
        if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
        from_proto = ops.get_from_proto_function(key)
        if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
         expression_statement: collection_def = src_meta_graph_def.collection_def[key]
          assignment: collection_def = src_meta_graph_def.collection_def[key]
           identifier: collection_def
           =: =
           subscript: src_meta_graph_def.collection_def[key]
            attribute: src_meta_graph_def.collection_def
             identifier: src_meta_graph_def
             .: .
             identifier: collection_def
            [: [
            identifier: key
            ]: ]
         expression_statement: kind = collection_def.WhichOneof("kind")
          assignment: kind = collection_def.WhichOneof("kind")
           identifier: kind
           =: =
           call: collection_def.WhichOneof("kind")
            attribute: collection_def.WhichOneof
             identifier: collection_def
             .: .
             identifier: WhichOneof
            argument_list: ("kind")
             (: (
             string: "kind"
              string_start: "
              string_content: kind
              string_end: "
             ): )
         if_statement: if kind is None:
          logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
          if: if
          comparison_operator: kind is None
           identifier: kind
           is: is
           none: None
          :: :
          block: logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
          continue
           expression_statement: logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
            call: logging.error(
              "Cannot identify data type for collection %s. Skipping.", key)
             attribute: logging.error
              identifier: logging
              .: .
              identifier: error
             argument_list: (
              "Cannot identify data type for collection %s. Skipping.", key)
              (: (
              string: "Cannot identify data type for collection %s. Skipping."
               string_start: "
               string_content: Cannot identify data type for collection %s. Skipping.
               string_end: "
              ,: ,
              identifier: key
              ): )
           continue_statement: continue
            continue: continue
         expression_statement: from_proto = ops.get_from_proto_function(key)
          assignment: from_proto = ops.get_from_proto_function(key)
           identifier: from_proto
           =: =
           call: ops.get_from_proto_function(key)
            attribute: ops.get_from_proto_function
             identifier: ops
             .: .
             identifier: get_from_proto_function
            argument_list: (key)
             (: (
             identifier: key
             ): )
         if_statement: if from_proto and kind == "bytes_list":
          proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
        else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
          if: if
          boolean_operator: from_proto and kind == "bytes_list"
           identifier: from_proto
           and: and
           comparison_operator: kind == "bytes_list"
            identifier: kind
            ==: ==
            string: "bytes_list"
             string_start: "
             string_content: bytes_list
             string_end: "
          :: :
          block: proto_type = ops.get_collection_proto_type(key)
          # It is assumed that there are no Variables Keys in collections
          for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
           expression_statement: proto_type = ops.get_collection_proto_type(key)
            assignment: proto_type = ops.get_collection_proto_type(key)
             identifier: proto_type
             =: =
             call: ops.get_collection_proto_type(key)
              attribute: ops.get_collection_proto_type
               identifier: ops
               .: .
               identifier: get_collection_proto_type
              argument_list: (key)
               (: (
               identifier: key
               ): )
           comment: # It is assumed that there are no Variables Keys in collections
           for_statement: for value in collection_def.bytes_list.value:
            proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
            for: for
            identifier: value
            in: in
            attribute: collection_def.bytes_list.value
             attribute: collection_def.bytes_list
              identifier: collection_def
              .: .
              identifier: bytes_list
             .: .
             identifier: value
            :: :
            block: proto = proto_type()
            proto.ParseFromString(value)
            try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
            dest_graph.add_to_collection(key, new_value)
             expression_statement: proto = proto_type()
              assignment: proto = proto_type()
               identifier: proto
               =: =
               call: proto_type()
                identifier: proto_type
                argument_list: ()
                 (: (
                 ): )
             expression_statement: proto.ParseFromString(value)
              call: proto.ParseFromString(value)
               attribute: proto.ParseFromString
                identifier: proto
                .: .
                identifier: ParseFromString
               argument_list: (value)
                (: (
                identifier: value
                ): )
             try_statement: try:
              new_value = from_proto(proto, import_scope=scope)
            except:
              continue
              try: try
              :: :
              block: new_value = from_proto(proto, import_scope=scope)
               expression_statement: new_value = from_proto(proto, import_scope=scope)
                assignment: new_value = from_proto(proto, import_scope=scope)
                 identifier: new_value
                 =: =
                 call: from_proto(proto, import_scope=scope)
                  identifier: from_proto
                  argument_list: (proto, import_scope=scope)
                   (: (
                   identifier: proto
                   ,: ,
                   keyword_argument: import_scope=scope
                    identifier: import_scope
                    =: =
                    identifier: scope
                   ): )
              except_clause: except:
              continue
               except: except
               :: :
               block: continue
                continue_statement: continue
                 continue: continue
             expression_statement: dest_graph.add_to_collection(key, new_value)
              call: dest_graph.add_to_collection(key, new_value)
               attribute: dest_graph.add_to_collection
                identifier: dest_graph
                .: .
                identifier: add_to_collection
               argument_list: (key, new_value)
                (: (
                identifier: key
                ,: ,
                identifier: new_value
                ): )
          else_clause: else:
          field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
           else: else
           :: :
           block: field = getattr(collection_def, kind)
          if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
            expression_statement: field = getattr(collection_def, kind)
             assignment: field = getattr(collection_def, kind)
              identifier: field
              =: =
              call: getattr(collection_def, kind)
               identifier: getattr
               argument_list: (collection_def, kind)
                (: (
                identifier: collection_def
                ,: ,
                identifier: kind
                ): )
            if_statement: if kind == "node_list":
            for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
          elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
          else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
             if: if
             comparison_operator: kind == "node_list"
              identifier: kind
              ==: ==
              string: "node_list"
               string_start: "
               string_content: node_list
               string_end: "
             :: :
             block: for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
              for_statement: for value in field.value:
              name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
               for: for
               identifier: value
               in: in
               attribute: field.value
                identifier: field
                .: .
                identifier: value
               :: :
               block: name = ops.prepend_name_scope(value, scope)
              # Since the graph has been optimized, the node may no longer
              # exists
              try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
              dest_graph.add_to_collection(key, col_op)
                expression_statement: name = ops.prepend_name_scope(value, scope)
                 assignment: name = ops.prepend_name_scope(value, scope)
                  identifier: name
                  =: =
                  call: ops.prepend_name_scope(value, scope)
                   attribute: ops.prepend_name_scope
                    identifier: ops
                    .: .
                    identifier: prepend_name_scope
                   argument_list: (value, scope)
                    (: (
                    identifier: value
                    ,: ,
                    identifier: scope
                    ): )
                comment: # Since the graph has been optimized, the node may no longer
                comment: # exists
                try_statement: try:
                col_op = dest_graph.as_graph_element(name)
              except (TypeError, ValueError, KeyError):
                continue
                 try: try
                 :: :
                 block: col_op = dest_graph.as_graph_element(name)
                  expression_statement: col_op = dest_graph.as_graph_element(name)
                   assignment: col_op = dest_graph.as_graph_element(name)
                    identifier: col_op
                    =: =
                    call: dest_graph.as_graph_element(name)
                     attribute: dest_graph.as_graph_element
                      identifier: dest_graph
                      .: .
                      identifier: as_graph_element
                     argument_list: (name)
                      (: (
                      identifier: name
                      ): )
                 except_clause: except (TypeError, ValueError, KeyError):
                continue
                  except: except
                  tuple: (TypeError, ValueError, KeyError)
                   (: (
                   identifier: TypeError
                   ,: ,
                   identifier: ValueError
                   ,: ,
                   identifier: KeyError
                   ): )
                  :: :
                  block: continue
                   continue_statement: continue
                    continue: continue
                expression_statement: dest_graph.add_to_collection(key, col_op)
                 call: dest_graph.add_to_collection(key, col_op)
                  attribute: dest_graph.add_to_collection
                   identifier: dest_graph
                   .: .
                   identifier: add_to_collection
                  argument_list: (key, col_op)
                   (: (
                   identifier: key
                   ,: ,
                   identifier: col_op
                   ): )
             elif_clause: elif kind == "int64_list":
            # NOTE(opensource): This force conversion is to work around the
            # fact that Python2 distinguishes between int and long, while
            # Python3 has only int.
            for value in field.value:
              dest_graph.add_to_collection(key, int(value))
              elif: elif
              comparison_operator: kind == "int64_list"
               identifier: kind
               ==: ==
               string: "int64_list"
                string_start: "
                string_content: int64_list
                string_end: "
              :: :
              comment: # NOTE(opensource): This force conversion is to work around the
              comment: # fact that Python2 distinguishes between int and long, while
              comment: # Python3 has only int.
              block: for value in field.value:
              dest_graph.add_to_collection(key, int(value))
               for_statement: for value in field.value:
              dest_graph.add_to_collection(key, int(value))
                for: for
                identifier: value
                in: in
                attribute: field.value
                 identifier: field
                 .: .
                 identifier: value
                :: :
                block: dest_graph.add_to_collection(key, int(value))
                 expression_statement: dest_graph.add_to_collection(key, int(value))
                  call: dest_graph.add_to_collection(key, int(value))
                   attribute: dest_graph.add_to_collection
                    identifier: dest_graph
                    .: .
                    identifier: add_to_collection
                   argument_list: (key, int(value))
                    (: (
                    identifier: key
                    ,: ,
                    call: int(value)
                     identifier: int
                     argument_list: (value)
                      (: (
                      identifier: value
                      ): )
                    ): )
             else_clause: else:
            for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
              else: else
              :: :
              block: for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
               for_statement: for value in field.value:
              dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
                for: for
                identifier: value
                in: in
                attribute: field.value
                 identifier: field
                 .: .
                 identifier: value
                :: :
                block: dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
                 expression_statement: dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
                  call: dest_graph.add_to_collection(key,
                                           ops.prepend_name_scope(value, scope))
                   attribute: dest_graph.add_to_collection
                    identifier: dest_graph
                    .: .
                    identifier: add_to_collection
                   argument_list: (key,
                                           ops.prepend_name_scope(value, scope))
                    (: (
                    identifier: key
                    ,: ,
                    call: ops.prepend_name_scope(value, scope)
                     attribute: ops.prepend_name_scope
                      identifier: ops
                      .: .
                      identifier: prepend_name_scope
                     argument_list: (value, scope)
                      (: (
                      identifier: value
                      ,: ,
                      identifier: scope
                      ): )
                    ): )
     comment: # Write the transformed graphdef as SavedModel.
     expression_statement: saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
      assignment: saved_model_builder = builder.SavedModelBuilder(output_saved_model_dir)
       identifier: saved_model_builder
       =: =
       call: builder.SavedModelBuilder(output_saved_model_dir)
        attribute: builder.SavedModelBuilder
         identifier: builder
         .: .
         identifier: SavedModelBuilder
        argument_list: (output_saved_model_dir)
         (: (
         identifier: output_saved_model_dir
         ): )
     with_statement: with ops.Graph().as_default():
      importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
      with: with
      with_clause: ops.Graph().as_default()
       with_item: ops.Graph().as_default()
        call: ops.Graph().as_default()
         attribute: ops.Graph().as_default
          call: ops.Graph()
           attribute: ops.Graph
            identifier: ops
            .: .
            identifier: Graph
           argument_list: ()
            (: (
            ): )
          .: .
          identifier: as_default
         argument_list: ()
          (: (
          ): )
      :: :
      block: importer.import_graph_def(self._converted_graph_def, name="")
      _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
      # We don't use any specific converter here.
      with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
       expression_statement: importer.import_graph_def(self._converted_graph_def, name="")
        call: importer.import_graph_def(self._converted_graph_def, name="")
         attribute: importer.import_graph_def
          identifier: importer
          .: .
          identifier: import_graph_def
         argument_list: (self._converted_graph_def, name="")
          (: (
          attribute: self._converted_graph_def
           identifier: self
           .: .
           identifier: _converted_graph_def
          ,: ,
          keyword_argument: name=""
           identifier: name
           =: =
           string: ""
            string_start: "
            string_end: "
          ): )
       expression_statement: _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
        call: _restore_collections(
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
         identifier: _restore_collections
         argument_list: (
          ops.get_default_graph(), self._grappler_meta_graph_def,
          self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def))
          (: (
          call: ops.get_default_graph()
           attribute: ops.get_default_graph
            identifier: ops
            .: .
            identifier: get_default_graph
           argument_list: ()
            (: (
            ): )
          ,: ,
          attribute: self._grappler_meta_graph_def
           identifier: self
           .: .
           identifier: _grappler_meta_graph_def
          ,: ,
          call: self._collections_to_keep(
              self._grappler_meta_graph_def.collection_def)
           attribute: self._collections_to_keep
            identifier: self
            .: .
            identifier: _collections_to_keep
           argument_list: (
              self._grappler_meta_graph_def.collection_def)
            (: (
            attribute: self._grappler_meta_graph_def.collection_def
             attribute: self._grappler_meta_graph_def
              identifier: self
              .: .
              identifier: _grappler_meta_graph_def
             .: .
             identifier: collection_def
            ): )
          ): )
       comment: # We don't use any specific converter here.
       with_statement: with session.Session() as sess:
        saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
        with: with
        with_clause: session.Session() as sess
         with_item: session.Session() as sess
          as_pattern: session.Session() as sess
           call: session.Session()
            attribute: session.Session
             identifier: session
             .: .
             identifier: Session
            argument_list: ()
             (: (
             ): )
           as: as
           as_pattern_target: sess
            identifier: sess
        :: :
        block: saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
         expression_statement: saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
          call: saved_model_builder.add_meta_graph_and_variables(
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
           attribute: saved_model_builder.add_meta_graph_and_variables
            identifier: saved_model_builder
            .: .
            identifier: add_meta_graph_and_variables
           argument_list: (
            sess,
            self._input_saved_model_tags,
            signature_def_map=self._grappler_meta_graph_def.signature_def)
            (: (
            identifier: sess
            ,: ,
            attribute: self._input_saved_model_tags
             identifier: self
             .: .
             identifier: _input_saved_model_tags
            ,: ,
            keyword_argument: signature_def_map=self._grappler_meta_graph_def.signature_def
             identifier: signature_def_map
             =: =
             attribute: self._grappler_meta_graph_def.signature_def
              attribute: self._grappler_meta_graph_def
               identifier: self
               .: .
               identifier: _grappler_meta_graph_def
              .: .
              identifier: signature_def
            ): )
     comment: # Ignore other meta graphs from the input SavedModel.
     expression_statement: saved_model_builder.save()
      call: saved_model_builder.save()
       attribute: saved_model_builder.save
        identifier: saved_model_builder
        .: .
        identifier: save
       argument_list: ()
        (: (
        ): )
 function_definition: def _get_resource_handle(name, device):
  with ops.device(device):
    return gen_trt_ops.create_trt_resource_handle(resource_name=name)
  def: def
  identifier: _get_resource_handle
  parameters: (name, device)
   (: (
   identifier: name
   ,: ,
   identifier: device
   ): )
  :: :
  block: with ops.device(device):
    return gen_trt_ops.create_trt_resource_handle(resource_name=name)
   with_statement: with ops.device(device):
    return gen_trt_ops.create_trt_resource_handle(resource_name=name)
    with: with
    with_clause: ops.device(device)
     with_item: ops.device(device)
      call: ops.device(device)
       attribute: ops.device
        identifier: ops
        .: .
        identifier: device
       argument_list: (device)
        (: (
        identifier: device
        ): )
    :: :
    block: return gen_trt_ops.create_trt_resource_handle(resource_name=name)
     return_statement: return gen_trt_ops.create_trt_resource_handle(resource_name=name)
      return: return
      call: gen_trt_ops.create_trt_resource_handle(resource_name=name)
       attribute: gen_trt_ops.create_trt_resource_handle
        identifier: gen_trt_ops
        .: .
        identifier: create_trt_resource_handle
       argument_list: (resource_name=name)
        (: (
        keyword_argument: resource_name=name
         identifier: resource_name
         =: =
         identifier: name
        ): )
 function_definition: def _remove_native_segments(input_func):
  """Remove native segments from the input TF-TRT Converted Function.

  Args:
    input_func: provide the concrete function with native segment nodes. The
      transformed output func will not contain any native segment nodes. All the
      TRTEngineOp references will be deleted and reset to default empty func.
  """
  input_graph_def = input_func.graph.as_graph_def()
  # Deleting the Native Segment node in each TRTEngineOp node.
  nodes_deleted = 0
  for func_id in reversed(range(len(input_graph_def.library.function))):
    f = input_graph_def.library.function[func_id]
    if "native_segment" in f.signature.name:
      nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]

  logging.info(
      "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
  )

  # Deleting the references to `<EngineName>_native_segment`s.
  # This helps TRTEngineOp constructor to not look for native segment handles
  # during construction of graph for inference.
  for node in input_graph_def.node:
    if node.op == "TRTEngineOp":
      del node.attr["segment_func"]
  for func in input_graph_def.library.function:
    for node in func.node_def:
      if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
  # Reconstruct the converted_func with the new graph
  new_func = _construct_function_from_graph_def(input_func, input_graph_def)

  return new_func
  def: def
  identifier: _remove_native_segments
  parameters: (input_func)
   (: (
   identifier: input_func
   ): )
  :: :
  block: """Remove native segments from the input TF-TRT Converted Function.

  Args:
    input_func: provide the concrete function with native segment nodes. The
      transformed output func will not contain any native segment nodes. All the
      TRTEngineOp references will be deleted and reset to default empty func.
  """
  input_graph_def = input_func.graph.as_graph_def()
  # Deleting the Native Segment node in each TRTEngineOp node.
  nodes_deleted = 0
  for func_id in reversed(range(len(input_graph_def.library.function))):
    f = input_graph_def.library.function[func_id]
    if "native_segment" in f.signature.name:
      nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]

  logging.info(
      "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
  )

  # Deleting the references to `<EngineName>_native_segment`s.
  # This helps TRTEngineOp constructor to not look for native segment handles
  # during construction of graph for inference.
  for node in input_graph_def.node:
    if node.op == "TRTEngineOp":
      del node.attr["segment_func"]
  for func in input_graph_def.library.function:
    for node in func.node_def:
      if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
  # Reconstruct the converted_func with the new graph
  new_func = _construct_function_from_graph_def(input_func, input_graph_def)

  return new_func
   expression_statement: """Remove native segments from the input TF-TRT Converted Function.

  Args:
    input_func: provide the concrete function with native segment nodes. The
      transformed output func will not contain any native segment nodes. All the
      TRTEngineOp references will be deleted and reset to default empty func.
  """
    string: """Remove native segments from the input TF-TRT Converted Function.

  Args:
    input_func: provide the concrete function with native segment nodes. The
      transformed output func will not contain any native segment nodes. All the
      TRTEngineOp references will be deleted and reset to default empty func.
  """
     string_start: """
     string_content: Remove native segments from the input TF-TRT Converted Function.

  Args:
    input_func: provide the concrete function with native segment nodes. The
      transformed output func will not contain any native segment nodes. All the
      TRTEngineOp references will be deleted and reset to default empty func.
  
     string_end: """
   expression_statement: input_graph_def = input_func.graph.as_graph_def()
    assignment: input_graph_def = input_func.graph.as_graph_def()
     identifier: input_graph_def
     =: =
     call: input_func.graph.as_graph_def()
      attribute: input_func.graph.as_graph_def
       attribute: input_func.graph
        identifier: input_func
        .: .
        identifier: graph
       .: .
       identifier: as_graph_def
      argument_list: ()
       (: (
       ): )
   comment: # Deleting the Native Segment node in each TRTEngineOp node.
   expression_statement: nodes_deleted = 0
    assignment: nodes_deleted = 0
     identifier: nodes_deleted
     =: =
     integer: 0
   for_statement: for func_id in reversed(range(len(input_graph_def.library.function))):
    f = input_graph_def.library.function[func_id]
    if "native_segment" in f.signature.name:
      nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]
    for: for
    identifier: func_id
    in: in
    call: reversed(range(len(input_graph_def.library.function)))
     identifier: reversed
     argument_list: (range(len(input_graph_def.library.function)))
      (: (
      call: range(len(input_graph_def.library.function))
       identifier: range
       argument_list: (len(input_graph_def.library.function))
        (: (
        call: len(input_graph_def.library.function)
         identifier: len
         argument_list: (input_graph_def.library.function)
          (: (
          attribute: input_graph_def.library.function
           attribute: input_graph_def.library
            identifier: input_graph_def
            .: .
            identifier: library
           .: .
           identifier: function
          ): )
        ): )
      ): )
    :: :
    block: f = input_graph_def.library.function[func_id]
    if "native_segment" in f.signature.name:
      nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]
     expression_statement: f = input_graph_def.library.function[func_id]
      assignment: f = input_graph_def.library.function[func_id]
       identifier: f
       =: =
       subscript: input_graph_def.library.function[func_id]
        attribute: input_graph_def.library.function
         attribute: input_graph_def.library
          identifier: input_graph_def
          .: .
          identifier: library
         .: .
         identifier: function
        [: [
        identifier: func_id
        ]: ]
     if_statement: if "native_segment" in f.signature.name:
      nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]
      if: if
      comparison_operator: "native_segment" in f.signature.name
       string: "native_segment"
        string_start: "
        string_content: native_segment
        string_end: "
       in: in
       attribute: f.signature.name
        attribute: f.signature
         identifier: f
         .: .
         identifier: signature
        .: .
        identifier: name
      :: :
      block: nodes_deleted += 1
      while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
      del input_graph_def.library.function[func_id]
       expression_statement: nodes_deleted += 1
        augmented_assignment: nodes_deleted += 1
         identifier: nodes_deleted
         +=: +=
         integer: 1
       while_statement: while context.context().has_function(f.signature.name):
        context.context().remove_function(f.signature.name)
        while: while
        call: context.context().has_function(f.signature.name)
         attribute: context.context().has_function
          call: context.context()
           attribute: context.context
            identifier: context
            .: .
            identifier: context
           argument_list: ()
            (: (
            ): )
          .: .
          identifier: has_function
         argument_list: (f.signature.name)
          (: (
          attribute: f.signature.name
           attribute: f.signature
            identifier: f
            .: .
            identifier: signature
           .: .
           identifier: name
          ): )
        :: :
        block: context.context().remove_function(f.signature.name)
         expression_statement: context.context().remove_function(f.signature.name)
          call: context.context().remove_function(f.signature.name)
           attribute: context.context().remove_function
            call: context.context()
             attribute: context.context
              identifier: context
              .: .
              identifier: context
             argument_list: ()
              (: (
              ): )
            .: .
            identifier: remove_function
           argument_list: (f.signature.name)
            (: (
            attribute: f.signature.name
             attribute: f.signature
              identifier: f
              .: .
              identifier: signature
             .: .
             identifier: name
            ): )
       delete_statement: del input_graph_def.library.function[func_id]
        del: del
        subscript: input_graph_def.library.function[func_id]
         attribute: input_graph_def.library.function
          attribute: input_graph_def.library
           identifier: input_graph_def
           .: .
           identifier: library
          .: .
          identifier: function
         [: [
         identifier: func_id
         ]: ]
   expression_statement: logging.info(
      "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
  )
    call: logging.info(
      "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
  )
     attribute: logging.info
      identifier: logging
      .: .
      identifier: info
     argument_list: (
      "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
  )
      (: (
      concatenated_string: "Found and deleted native segments from "
      f"{nodes_deleted} TRTEngineOp nodes."
       string: "Found and deleted native segments from "
        string_start: "
        string_content: Found and deleted native segments from 
        string_end: "
       string: f"{nodes_deleted} TRTEngineOp nodes."
        string_start: f"
        interpolation: {nodes_deleted}
         {: {
         identifier: nodes_deleted
         }: }
        string_content:  TRTEngineOp nodes.
        string_end: "
      ): )
   comment: # Deleting the references to `<EngineName>_native_segment`s.
   comment: # This helps TRTEngineOp constructor to not look for native segment handles
   comment: # during construction of graph for inference.
   for_statement: for node in input_graph_def.node:
    if node.op == "TRTEngineOp":
      del node.attr["segment_func"]
    for: for
    identifier: node
    in: in
    attribute: input_graph_def.node
     identifier: input_graph_def
     .: .
     identifier: node
    :: :
    block: if node.op == "TRTEngineOp":
      del node.attr["segment_func"]
     if_statement: if node.op == "TRTEngineOp":
      del node.attr["segment_func"]
      if: if
      comparison_operator: node.op == "TRTEngineOp"
       attribute: node.op
        identifier: node
        .: .
        identifier: op
       ==: ==
       string: "TRTEngineOp"
        string_start: "
        string_content: TRTEngineOp
        string_end: "
      :: :
      block: del node.attr["segment_func"]
       delete_statement: del node.attr["segment_func"]
        del: del
        subscript: node.attr["segment_func"]
         attribute: node.attr
          identifier: node
          .: .
          identifier: attr
         [: [
         string: "segment_func"
          string_start: "
          string_content: segment_func
          string_end: "
         ]: ]
   for_statement: for func in input_graph_def.library.function:
    for node in func.node_def:
      if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
    for: for
    identifier: func
    in: in
    attribute: input_graph_def.library.function
     attribute: input_graph_def.library
      identifier: input_graph_def
      .: .
      identifier: library
     .: .
     identifier: function
    :: :
    block: for node in func.node_def:
      if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
     for_statement: for node in func.node_def:
      if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
      for: for
      identifier: node
      in: in
      attribute: func.node_def
       identifier: func
       .: .
       identifier: node_def
      :: :
      block: if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
       if_statement: if node.op == "TRTEngineOp":
        del node.attr["segment_func"]
        if: if
        comparison_operator: node.op == "TRTEngineOp"
         attribute: node.op
          identifier: node
          .: .
          identifier: op
         ==: ==
         string: "TRTEngineOp"
          string_start: "
          string_content: TRTEngineOp
          string_end: "
        :: :
        block: del node.attr["segment_func"]
         delete_statement: del node.attr["segment_func"]
          del: del
          subscript: node.attr["segment_func"]
           attribute: node.attr
            identifier: node
            .: .
            identifier: attr
           [: [
           string: "segment_func"
            string_start: "
            string_content: segment_func
            string_end: "
           ]: ]
   comment: # Reconstruct the converted_func with the new graph
   expression_statement: new_func = _construct_function_from_graph_def(input_func, input_graph_def)
    assignment: new_func = _construct_function_from_graph_def(input_func, input_graph_def)
     identifier: new_func
     =: =
     call: _construct_function_from_graph_def(input_func, input_graph_def)
      identifier: _construct_function_from_graph_def
      argument_list: (input_func, input_graph_def)
       (: (
       identifier: input_func
       ,: ,
       identifier: input_graph_def
       ): )
   return_statement: return new_func
    return: return
    identifier: new_func
 class_definition: class _TRTEngineResource(resource.TrackableResource):
  """Class to track the serialized engines resource."""

  def __init__(self,
               resource_name,
               filename,
               maximum_cached_engines,
               device="GPU"):
    super(_TRTEngineResource, self).__init__(device=device)
    self._resource_name = resource_name
    # Track the serialized engine file in the SavedModel.
    self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
    self._maximum_cached_engines = maximum_cached_engines

  def _create_resource(self):
    return _get_resource_handle(self._resource_name, self._resource_device)

  def _initialize(self):
    gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)

  def _destroy_resource(self):
    handle = _get_resource_handle(self._resource_name, self._resource_device)
    with ops.device(self._resource_device):
      gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
  class: class
  identifier: _TRTEngineResource
  argument_list: (resource.TrackableResource)
   (: (
   attribute: resource.TrackableResource
    identifier: resource
    .: .
    identifier: TrackableResource
   ): )
  :: :
  block: """Class to track the serialized engines resource."""

  def __init__(self,
               resource_name,
               filename,
               maximum_cached_engines,
               device="GPU"):
    super(_TRTEngineResource, self).__init__(device=device)
    self._resource_name = resource_name
    # Track the serialized engine file in the SavedModel.
    self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
    self._maximum_cached_engines = maximum_cached_engines

  def _create_resource(self):
    return _get_resource_handle(self._resource_name, self._resource_device)

  def _initialize(self):
    gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)

  def _destroy_resource(self):
    handle = _get_resource_handle(self._resource_name, self._resource_device)
    with ops.device(self._resource_device):
      gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
   expression_statement: """Class to track the serialized engines resource."""
    string: """Class to track the serialized engines resource."""
     string_start: """
     string_content: Class to track the serialized engines resource.
     string_end: """
   function_definition: def __init__(self,
               resource_name,
               filename,
               maximum_cached_engines,
               device="GPU"):
    super(_TRTEngineResource, self).__init__(device=device)
    self._resource_name = resource_name
    # Track the serialized engine file in the SavedModel.
    self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
    self._maximum_cached_engines = maximum_cached_engines
    def: def
    identifier: __init__
    parameters: (self,
               resource_name,
               filename,
               maximum_cached_engines,
               device="GPU")
     (: (
     identifier: self
     ,: ,
     identifier: resource_name
     ,: ,
     identifier: filename
     ,: ,
     identifier: maximum_cached_engines
     ,: ,
     default_parameter: device="GPU"
      identifier: device
      =: =
      string: "GPU"
       string_start: "
       string_content: GPU
       string_end: "
     ): )
    :: :
    block: super(_TRTEngineResource, self).__init__(device=device)
    self._resource_name = resource_name
    # Track the serialized engine file in the SavedModel.
    self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
    self._maximum_cached_engines = maximum_cached_engines
     expression_statement: super(_TRTEngineResource, self).__init__(device=device)
      call: super(_TRTEngineResource, self).__init__(device=device)
       attribute: super(_TRTEngineResource, self).__init__
        call: super(_TRTEngineResource, self)
         identifier: super
         argument_list: (_TRTEngineResource, self)
          (: (
          identifier: _TRTEngineResource
          ,: ,
          identifier: self
          ): )
        .: .
        identifier: __init__
       argument_list: (device=device)
        (: (
        keyword_argument: device=device
         identifier: device
         =: =
         identifier: device
        ): )
     expression_statement: self._resource_name = resource_name
      assignment: self._resource_name = resource_name
       attribute: self._resource_name
        identifier: self
        .: .
        identifier: _resource_name
       =: =
       identifier: resource_name
     comment: # Track the serialized engine file in the SavedModel.
     expression_statement: self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
      assignment: self._filename = self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
       attribute: self._filename
        identifier: self
        .: .
        identifier: _filename
       =: =
       call: self._track_trackable(
        asset.Asset(filename), "_serialized_trt_resource_filename")
        attribute: self._track_trackable
         identifier: self
         .: .
         identifier: _track_trackable
        argument_list: (
        asset.Asset(filename), "_serialized_trt_resource_filename")
         (: (
         call: asset.Asset(filename)
          attribute: asset.Asset
           identifier: asset
           .: .
           identifier: Asset
          argument_list: (filename)
           (: (
           identifier: filename
           ): )
         ,: ,
         string: "_serialized_trt_resource_filename"
          string_start: "
          string_content: _serialized_trt_resource_filename
          string_end: "
         ): )
     expression_statement: self._maximum_cached_engines = maximum_cached_engines
      assignment: self._maximum_cached_engines = maximum_cached_engines
       attribute: self._maximum_cached_engines
        identifier: self
        .: .
        identifier: _maximum_cached_engines
       =: =
       identifier: maximum_cached_engines
   function_definition: def _create_resource(self):
    return _get_resource_handle(self._resource_name, self._resource_device)
    def: def
    identifier: _create_resource
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: return _get_resource_handle(self._resource_name, self._resource_device)
     return_statement: return _get_resource_handle(self._resource_name, self._resource_device)
      return: return
      call: _get_resource_handle(self._resource_name, self._resource_device)
       identifier: _get_resource_handle
       argument_list: (self._resource_name, self._resource_device)
        (: (
        attribute: self._resource_name
         identifier: self
         .: .
         identifier: _resource_name
        ,: ,
        attribute: self._resource_device
         identifier: self
         .: .
         identifier: _resource_device
        ): )
   function_definition: def _initialize(self):
    gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)
    def: def
    identifier: _initialize
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)
     expression_statement: gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)
      call: gen_trt_ops.initialize_trt_resource(
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)
       attribute: gen_trt_ops.initialize_trt_resource
        identifier: gen_trt_ops
        .: .
        identifier: initialize_trt_resource
       argument_list: (
        self.resource_handle,
        self._filename,
        max_cached_engines_count=self._maximum_cached_engines)
        (: (
        attribute: self.resource_handle
         identifier: self
         .: .
         identifier: resource_handle
        ,: ,
        attribute: self._filename
         identifier: self
         .: .
         identifier: _filename
        ,: ,
        keyword_argument: max_cached_engines_count=self._maximum_cached_engines
         identifier: max_cached_engines_count
         =: =
         attribute: self._maximum_cached_engines
          identifier: self
          .: .
          identifier: _maximum_cached_engines
        ): )
   function_definition: def _destroy_resource(self):
    handle = _get_resource_handle(self._resource_name, self._resource_device)
    with ops.device(self._resource_device):
      gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
    def: def
    identifier: _destroy_resource
    parameters: (self)
     (: (
     identifier: self
     ): )
    :: :
    block: handle = _get_resource_handle(self._resource_name, self._resource_device)
    with ops.device(self._resource_device):
      gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
     expression_statement: handle = _get_resource_handle(self._resource_name, self._resource_device)
      assignment: handle = _get_resource_handle(self._resource_name, self._resource_device)
       identifier: handle
       =: =
       call: _get_resource_handle(self._resource_name, self._resource_device)
        identifier: _get_resource_handle
        argument_list: (self._resource_name, self._resource_device)
         (: (
         attribute: self._resource_name
          identifier: self
          .: .
          identifier: _resource_name
         ,: ,
         attribute: self._resource_device
          identifier: self
          .: .
          identifier: _resource_device
         ): )
     with_statement: with ops.device(self._resource_device):
      gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
      with: with
      with_clause: ops.device(self._resource_device)
       with_item: ops.device(self._resource_device)
        call: ops.device(self._resource_device)
         attribute: ops.device
          identifier: ops
          .: .
          identifier: device
         argument_list: (self._resource_device)
          (: (
          attribute: self._resource_device
           identifier: self
           .: .
           identifier: _resource_device
          ): )
      :: :
      block: gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
       expression_statement: gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
        call: gen_resource_variable_ops.destroy_resource_op(
          handle, ignore_lookup_error=True)
         attribute: gen_resource_variable_ops.destroy_resource_op
          identifier: gen_resource_variable_ops
          .: .
          identifier: destroy_resource_op
         argument_list: (
          handle, ignore_lookup_error=True)
          (: (
          identifier: handle
          ,: ,
          keyword_argument: ignore_lookup_error=True
           identifier: ignore_lookup_error
           =: =
           true: True
          ): )
 function_definition: def _print_row(fields, positions, print_fn):
  """Prints a row."""
  line = ""
  for i, field in enumerate(fields):
    field = str(field)
    end_line_pos = positions[i]
    if i > 0:
      line = line + " "
    line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)

    if len(line) > end_line_pos:
      line = line[:(end_line_pos - 4)] + " ..."

  print_fn(line)
  def: def
  identifier: _print_row
  parameters: (fields, positions, print_fn)
   (: (
   identifier: fields
   ,: ,
   identifier: positions
   ,: ,
   identifier: print_fn
   ): )
  :: :
  block: """Prints a row."""
  line = ""
  for i, field in enumerate(fields):
    field = str(field)
    end_line_pos = positions[i]
    if i > 0:
      line = line + " "
    line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)

    if len(line) > end_line_pos:
      line = line[:(end_line_pos - 4)] + " ..."

  print_fn(line)
   expression_statement: """Prints a row."""
    string: """Prints a row."""
     string_start: """
     string_content: Prints a row.
     string_end: """
   expression_statement: line = ""
    assignment: line = ""
     identifier: line
     =: =
     string: ""
      string_start: "
      string_end: "
   for_statement: for i, field in enumerate(fields):
    field = str(field)
    end_line_pos = positions[i]
    if i > 0:
      line = line + " "
    line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)

    if len(line) > end_line_pos:
      line = line[:(end_line_pos - 4)] + " ..."
    for: for
    pattern_list: i, field
     identifier: i
     ,: ,
     identifier: field
    in: in
    call: enumerate(fields)
     identifier: enumerate
     argument_list: (fields)
      (: (
      identifier: fields
      ): )
    :: :
    block: field = str(field)
    end_line_pos = positions[i]
    if i > 0:
      line = line + " "
    line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)

    if len(line) > end_line_pos:
      line = line[:(end_line_pos - 4)] + " ..."
     expression_statement: field = str(field)
      assignment: field = str(field)
       identifier: field
       =: =
       call: str(field)
        identifier: str
        argument_list: (field)
         (: (
         identifier: field
         ): )
     expression_statement: end_line_pos = positions[i]
      assignment: end_line_pos = positions[i]
       identifier: end_line_pos
       =: =
       subscript: positions[i]
        identifier: positions
        [: [
        identifier: i
        ]: ]
     if_statement: if i > 0:
      line = line + " "
      if: if
      comparison_operator: i > 0
       identifier: i
       >: >
       integer: 0
      :: :
      block: line = line + " "
       expression_statement: line = line + " "
        assignment: line = line + " "
         identifier: line
         =: =
         binary_operator: line + " "
          identifier: line
          +: +
          string: " "
           string_start: "
           string_content:  
           string_end: "
     expression_statement: line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)
      assignment: line = "{0:{min_length}}".format(line + field, min_length=end_line_pos)
       identifier: line
       =: =
       call: "{0:{min_length}}".format(line + field, min_length=end_line_pos)
        attribute: "{0:{min_length}}".format
         string: "{0:{min_length}}"
          string_start: "
          string_content: {0:{min_length}}
          string_end: "
         .: .
         identifier: format
        argument_list: (line + field, min_length=end_line_pos)
         (: (
         binary_operator: line + field
          identifier: line
          +: +
          identifier: field
         ,: ,
         keyword_argument: min_length=end_line_pos
          identifier: min_length
          =: =
          identifier: end_line_pos
         ): )
     if_statement: if len(line) > end_line_pos:
      line = line[:(end_line_pos - 4)] + " ..."
      if: if
      comparison_operator: len(line) > end_line_pos
       call: len(line)
        identifier: len
        argument_list: (line)
         (: (
         identifier: line
         ): )
       >: >
       identifier: end_line_pos
      :: :
      block: line = line[:(end_line_pos - 4)] + " ..."
       expression_statement: line = line[:(end_line_pos - 4)] + " ..."
        assignment: line = line[:(end_line_pos - 4)] + " ..."
         identifier: line
         =: =
         binary_operator: line[:(end_line_pos - 4)] + " ..."
          subscript: line[:(end_line_pos - 4)]
           identifier: line
           [: [
           slice: :(end_line_pos - 4)
            :: :
            parenthesized_expression: (end_line_pos - 4)
             (: (
             binary_operator: end_line_pos - 4
              identifier: end_line_pos
              -: -
              integer: 4
             ): )
           ]: ]
          +: +
          string: " ..."
           string_start: "
           string_content:  ...
           string_end: "
   expression_statement: print_fn(line)
    call: print_fn(line)
     identifier: print_fn
     argument_list: (line)
      (: (
      identifier: line
      ): )
 function_definition: def _construct_function_from_graph_def(func, graph_def, frozen_func=None):
  """Rebuild function from graph_def."""
  if frozen_func is None:
    frozen_func = func

  # If a function is converted, then the TF context contains the original
  # function while the converted_graph_def contains the converted function.
  # Remove the original function from the TF context in this case.
  for f in graph_def.library.function:
    while context.context().has_function(f.signature.name):
      context.context().remove_function(f.signature.name)

  captures = {
      c[1].name.split(":")[0]: c[0]
      for c in frozen_func.graph.captures
  }
  new_func = wrap_function.function_from_graph_def(
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
  new_func.graph.structured_outputs = nest.pack_sequence_as(
      func.graph.structured_outputs, new_func.graph.structured_outputs)
  new_func._function_type = func.function_type  # pylint: disable=protected-access

  # Copy structured input signature from original function (used during
  # serialization)
  new_func.graph.structured_input_signature = (func.structured_input_signature)

  return new_func
  def: def
  identifier: _construct_function_from_graph_def
  parameters: (func, graph_def, frozen_func=None)
   (: (
   identifier: func
   ,: ,
   identifier: graph_def
   ,: ,
   default_parameter: frozen_func=None
    identifier: frozen_func
    =: =
    none: None
   ): )
  :: :
  block: """Rebuild function from graph_def."""
  if frozen_func is None:
    frozen_func = func

  # If a function is converted, then the TF context contains the original
  # function while the converted_graph_def contains the converted function.
  # Remove the original function from the TF context in this case.
  for f in graph_def.library.function:
    while context.context().has_function(f.signature.name):
      context.context().remove_function(f.signature.name)

  captures = {
      c[1].name.split(":")[0]: c[0]
      for c in frozen_func.graph.captures
  }
  new_func = wrap_function.function_from_graph_def(
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
  new_func.graph.structured_outputs = nest.pack_sequence_as(
      func.graph.structured_outputs, new_func.graph.structured_outputs)
  new_func._function_type = func.function_type  # pylint: disable=protected-access

  # Copy structured input signature from original function (used during
  # serialization)
  new_func.graph.structured_input_signature = (func.structured_input_signature)

  return new_func
   expression_statement: """Rebuild function from graph_def."""
    string: """Rebuild function from graph_def."""
     string_start: """
     string_content: Rebuild function from graph_def.
     string_end: """
   if_statement: if frozen_func is None:
    frozen_func = func
    if: if
    comparison_operator: frozen_func is None
     identifier: frozen_func
     is: is
     none: None
    :: :
    block: frozen_func = func
     expression_statement: frozen_func = func
      assignment: frozen_func = func
       identifier: frozen_func
       =: =
       identifier: func
   comment: # If a function is converted, then the TF context contains the original
   comment: # function while the converted_graph_def contains the converted function.
   comment: # Remove the original function from the TF context in this case.
   for_statement: for f in graph_def.library.function:
    while context.context().has_function(f.signature.name):
      context.context().remove_function(f.signature.name)
    for: for
    identifier: f
    in: in
    attribute: graph_def.library.function
     attribute: graph_def.library
      identifier: graph_def
      .: .
      identifier: library
     .: .
     identifier: function
    :: :
    block: while context.context().has_function(f.signature.name):
      context.context().remove_function(f.signature.name)
     while_statement: while context.context().has_function(f.signature.name):
      context.context().remove_function(f.signature.name)
      while: while
      call: context.context().has_function(f.signature.name)
       attribute: context.context().has_function
        call: context.context()
         attribute: context.context
          identifier: context
          .: .
          identifier: context
         argument_list: ()
          (: (
          ): )
        .: .
        identifier: has_function
       argument_list: (f.signature.name)
        (: (
        attribute: f.signature.name
         attribute: f.signature
          identifier: f
          .: .
          identifier: signature
         .: .
         identifier: name
        ): )
      :: :
      block: context.context().remove_function(f.signature.name)
       expression_statement: context.context().remove_function(f.signature.name)
        call: context.context().remove_function(f.signature.name)
         attribute: context.context().remove_function
          call: context.context()
           attribute: context.context
            identifier: context
            .: .
            identifier: context
           argument_list: ()
            (: (
            ): )
          .: .
          identifier: remove_function
         argument_list: (f.signature.name)
          (: (
          attribute: f.signature.name
           attribute: f.signature
            identifier: f
            .: .
            identifier: signature
           .: .
           identifier: name
          ): )
   expression_statement: captures = {
      c[1].name.split(":")[0]: c[0]
      for c in frozen_func.graph.captures
  }
    assignment: captures = {
      c[1].name.split(":")[0]: c[0]
      for c in frozen_func.graph.captures
  }
     identifier: captures
     =: =
     dictionary_comprehension: {
      c[1].name.split(":")[0]: c[0]
      for c in frozen_func.graph.captures
  }
      {: {
      pair: c[1].name.split(":")[0]: c[0]
       subscript: c[1].name.split(":")[0]
        call: c[1].name.split(":")
         attribute: c[1].name.split
          attribute: c[1].name
           subscript: c[1]
            identifier: c
            [: [
            integer: 1
            ]: ]
           .: .
           identifier: name
          .: .
          identifier: split
         argument_list: (":")
          (: (
          string: ":"
           string_start: "
           string_content: :
           string_end: "
          ): )
        [: [
        integer: 0
        ]: ]
       :: :
       subscript: c[0]
        identifier: c
        [: [
        integer: 0
        ]: ]
      for_in_clause: for c in frozen_func.graph.captures
       for: for
       identifier: c
       in: in
       attribute: frozen_func.graph.captures
        attribute: frozen_func.graph
         identifier: frozen_func
         .: .
         identifier: graph
        .: .
        identifier: captures
      }: }
   expression_statement: new_func = wrap_function.function_from_graph_def(
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
    assignment: new_func = wrap_function.function_from_graph_def(
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
     identifier: new_func
     =: =
     call: wrap_function.function_from_graph_def(
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
      attribute: wrap_function.function_from_graph_def
       identifier: wrap_function
       .: .
       identifier: function_from_graph_def
      argument_list: (
      graph_def, [tensor.name for tensor in frozen_func.inputs],
      [tensor.name for tensor in frozen_func.outputs], captures)
       (: (
       identifier: graph_def
       ,: ,
       list_comprehension: [tensor.name for tensor in frozen_func.inputs]
        [: [
        attribute: tensor.name
         identifier: tensor
         .: .
         identifier: name
        for_in_clause: for tensor in frozen_func.inputs
         for: for
         identifier: tensor
         in: in
         attribute: frozen_func.inputs
          identifier: frozen_func
          .: .
          identifier: inputs
        ]: ]
       ,: ,
       list_comprehension: [tensor.name for tensor in frozen_func.outputs]
        [: [
        attribute: tensor.name
         identifier: tensor
         .: .
         identifier: name
        for_in_clause: for tensor in frozen_func.outputs
         for: for
         identifier: tensor
         in: in
         attribute: frozen_func.outputs
          identifier: frozen_func
          .: .
          identifier: outputs
        ]: ]
       ,: ,
       identifier: captures
       ): )
   expression_statement: new_func.graph.structured_outputs = nest.pack_sequence_as(
      func.graph.structured_outputs, new_func.graph.structured_outputs)
    assignment: new_func.graph.structured_outputs = nest.pack_sequence_as(
      func.graph.structured_outputs, new_func.graph.structured_outputs)
     attribute: new_func.graph.structured_outputs
      attribute: new_func.graph
       identifier: new_func
       .: .
       identifier: graph
      .: .
      identifier: structured_outputs
     =: =
     call: nest.pack_sequence_as(
      func.graph.structured_outputs, new_func.graph.structured_outputs)
      attribute: nest.pack_sequence_as
       identifier: nest
       .: .
       identifier: pack_sequence_as
      argument_list: (
      func.graph.structured_outputs, new_func.graph.structured_outputs)
       (: (
       attribute: func.graph.structured_outputs
        attribute: func.graph
         identifier: func
         .: .
         identifier: graph
        .: .
        identifier: structured_outputs
       ,: ,
       attribute: new_func.graph.structured_outputs
        attribute: new_func.graph
         identifier: new_func
         .: .
         identifier: graph
        .: .
        identifier: structured_outputs
       ): )
   expression_statement: new_func._function_type = func.function_type
    assignment: new_func._function_type = func.function_type
     attribute: new_func._function_type
      identifier: new_func
      .: .
      identifier: _function_type
     =: =
     attribute: func.function_type
      identifier: func
      .: .
      identifier: function_type
   comment: # pylint: disable=protected-access
   comment: # Copy structured input signature from original function (used during
   comment: # serialization)
   expression_statement: new_func.graph.structured_input_signature = (func.structured_input_signature)
    assignment: new_func.graph.structured_input_signature = (func.structured_input_signature)
     attribute: new_func.graph.structured_input_signature
      attribute: new_func.graph
       identifier: new_func
       .: .
       identifier: graph
      .: .
      identifier: structured_input_signature
     =: =
     parenthesized_expression: (func.structured_input_signature)
      (: (
      attribute: func.structured_input_signature
       identifier: func
       .: .
       identifier: structured_input_signature
      ): )
   return_statement: return new_func
    return: return
    identifier: new_func
 function_definition: def _apply_inlining(func):
  """Apply an inlining optimization to the function's graph definition."""
  graph_def = func.graph.as_graph_def()

  # In some cases, a secondary implementation of the function (e.g. for GPU) is
  # written to the "api_implements" attribute. (e.g. `tf.keras.layers.LSTM` in
  # TF2 produces a CuDNN-based RNN for GPU).
  # This function suppose to inline all functions calls, but "api_implements"
  # prevents this from happening. Removing the attribute solves the problem.
  # To learn more about "api_implements", see:
  #   tensorflow/core/grappler/optimizers/implementation_selector.h
  for function in graph_def.library.function:
    if "api_implements" in function.attr:
      del function.attr["api_implements"]

  meta_graph = saver.export_meta_graph(graph_def=graph_def, graph=func.graph)

  # Clear the initializer_name for the variables collections, since they are not
  # needed after saved to saved_model.
  for name in [
      "variables", "model_variables", "trainable_variables", "local_variables"
  ]:
    raw_list = []
    for raw in meta_graph.collection_def["variables"].bytes_list.value:
      variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
    meta_graph.collection_def[name].bytes_list.value[:] = raw_list

  # Add a collection 'train_op' so that Grappler knows the outputs.
  fetch_collection = meta_graph_pb2.CollectionDef()
  for array in func.inputs + func.outputs:
    fetch_collection.node_list.value.append(array.name)
  meta_graph.collection_def["train_op"].CopyFrom(fetch_collection)

  # Initialize RewriterConfig with everything disabled except function inlining.
  config = config_pb2.ConfigProto()
  rewrite_options = config.graph_options.rewrite_options
  rewrite_options.min_graph_nodes = -1  # do not skip small graphs
  rewrite_options.optimizers.append("function")

  new_graph_def = tf_optimizer.OptimizeGraph(config, meta_graph)

  return new_graph_def
  def: def
  identifier: _apply_inlining
  parameters: (func)
   (: (
   identifier: func
   ): )
  :: :
  block: """Apply an inlining optimization to the function's graph definition."""
  graph_def = func.graph.as_graph_def()

  # In some cases, a secondary implementation of the function (e.g. for GPU) is
  # written to the "api_implements" attribute. (e.g. `tf.keras.layers.LSTM` in
  # TF2 produces a CuDNN-based RNN for GPU).
  # This function suppose to inline all functions calls, but "api_implements"
  # prevents this from happening. Removing the attribute solves the problem.
  # To learn more about "api_implements", see:
  #   tensorflow/core/grappler/optimizers/implementation_selector.h
  for function in graph_def.library.function:
    if "api_implements" in function.attr:
      del function.attr["api_implements"]

  meta_graph = saver.export_meta_graph(graph_def=graph_def, graph=func.graph)

  # Clear the initializer_name for the variables collections, since they are not
  # needed after saved to saved_model.
  for name in [
      "variables", "model_variables", "trainable_variables", "local_variables"
  ]:
    raw_list = []
    for raw in meta_graph.collection_def["variables"].bytes_list.value:
      variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
    meta_graph.collection_def[name].bytes_list.value[:] = raw_list

  # Add a collection 'train_op' so that Grappler knows the outputs.
  fetch_collection = meta_graph_pb2.CollectionDef()
  for array in func.inputs + func.outputs:
    fetch_collection.node_list.value.append(array.name)
  meta_graph.collection_def["train_op"].CopyFrom(fetch_collection)

  # Initialize RewriterConfig with everything disabled except function inlining.
  config = config_pb2.ConfigProto()
  rewrite_options = config.graph_options.rewrite_options
  rewrite_options.min_graph_nodes = -1  # do not skip small graphs
  rewrite_options.optimizers.append("function")

  new_graph_def = tf_optimizer.OptimizeGraph(config, meta_graph)

  return new_graph_def
   expression_statement: """Apply an inlining optimization to the function's graph definition."""
    string: """Apply an inlining optimization to the function's graph definition."""
     string_start: """
     string_content: Apply an inlining optimization to the function's graph definition.
     string_end: """
   expression_statement: graph_def = func.graph.as_graph_def()
    assignment: graph_def = func.graph.as_graph_def()
     identifier: graph_def
     =: =
     call: func.graph.as_graph_def()
      attribute: func.graph.as_graph_def
       attribute: func.graph
        identifier: func
        .: .
        identifier: graph
       .: .
       identifier: as_graph_def
      argument_list: ()
       (: (
       ): )
   comment: # In some cases, a secondary implementation of the function (e.g. for GPU) is
   comment: # written to the "api_implements" attribute. (e.g. `tf.keras.layers.LSTM` in
   comment: # TF2 produces a CuDNN-based RNN for GPU).
   comment: # This function suppose to inline all functions calls, but "api_implements"
   comment: # prevents this from happening. Removing the attribute solves the problem.
   comment: # To learn more about "api_implements", see:
   comment: #   tensorflow/core/grappler/optimizers/implementation_selector.h
   for_statement: for function in graph_def.library.function:
    if "api_implements" in function.attr:
      del function.attr["api_implements"]
    for: for
    identifier: function
    in: in
    attribute: graph_def.library.function
     attribute: graph_def.library
      identifier: graph_def
      .: .
      identifier: library
     .: .
     identifier: function
    :: :
    block: if "api_implements" in function.attr:
      del function.attr["api_implements"]
     if_statement: if "api_implements" in function.attr:
      del function.attr["api_implements"]
      if: if
      comparison_operator: "api_implements" in function.attr
       string: "api_implements"
        string_start: "
        string_content: api_implements
        string_end: "
       in: in
       attribute: function.attr
        identifier: function
        .: .
        identifier: attr
      :: :
      block: del function.attr["api_implements"]
       delete_statement: del function.attr["api_implements"]
        del: del
        subscript: function.attr["api_implements"]
         attribute: function.attr
          identifier: function
          .: .
          identifier: attr
         [: [
         string: "api_implements"
          string_start: "
          string_content: api_implements
          string_end: "
         ]: ]
   expression_statement: meta_graph = saver.export_meta_graph(graph_def=graph_def, graph=func.graph)
    assignment: meta_graph = saver.export_meta_graph(graph_def=graph_def, graph=func.graph)
     identifier: meta_graph
     =: =
     call: saver.export_meta_graph(graph_def=graph_def, graph=func.graph)
      attribute: saver.export_meta_graph
       identifier: saver
       .: .
       identifier: export_meta_graph
      argument_list: (graph_def=graph_def, graph=func.graph)
       (: (
       keyword_argument: graph_def=graph_def
        identifier: graph_def
        =: =
        identifier: graph_def
       ,: ,
       keyword_argument: graph=func.graph
        identifier: graph
        =: =
        attribute: func.graph
         identifier: func
         .: .
         identifier: graph
       ): )
   comment: # Clear the initializer_name for the variables collections, since they are not
   comment: # needed after saved to saved_model.
   for_statement: for name in [
      "variables", "model_variables", "trainable_variables", "local_variables"
  ]:
    raw_list = []
    for raw in meta_graph.collection_def["variables"].bytes_list.value:
      variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
    meta_graph.collection_def[name].bytes_list.value[:] = raw_list
    for: for
    identifier: name
    in: in
    list: [
      "variables", "model_variables", "trainable_variables", "local_variables"
  ]
     [: [
     string: "variables"
      string_start: "
      string_content: variables
      string_end: "
     ,: ,
     string: "model_variables"
      string_start: "
      string_content: model_variables
      string_end: "
     ,: ,
     string: "trainable_variables"
      string_start: "
      string_content: trainable_variables
      string_end: "
     ,: ,
     string: "local_variables"
      string_start: "
      string_content: local_variables
      string_end: "
     ]: ]
    :: :
    block: raw_list = []
    for raw in meta_graph.collection_def["variables"].bytes_list.value:
      variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
    meta_graph.collection_def[name].bytes_list.value[:] = raw_list
     expression_statement: raw_list = []
      assignment: raw_list = []
       identifier: raw_list
       =: =
       list: []
        [: [
        ]: ]
     for_statement: for raw in meta_graph.collection_def["variables"].bytes_list.value:
      variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
      for: for
      identifier: raw
      in: in
      attribute: meta_graph.collection_def["variables"].bytes_list.value
       attribute: meta_graph.collection_def["variables"].bytes_list
        subscript: meta_graph.collection_def["variables"]
         attribute: meta_graph.collection_def
          identifier: meta_graph
          .: .
          identifier: collection_def
         [: [
         string: "variables"
          string_start: "
          string_content: variables
          string_end: "
         ]: ]
        .: .
        identifier: bytes_list
       .: .
       identifier: value
      :: :
      block: variable = variable_pb2.VariableDef()
      variable.ParseFromString(raw)
      variable.ClearField("initializer_name")
      raw_list.append(variable.SerializeToString())
       expression_statement: variable = variable_pb2.VariableDef()
        assignment: variable = variable_pb2.VariableDef()
         identifier: variable
         =: =
         call: variable_pb2.VariableDef()
          attribute: variable_pb2.VariableDef
           identifier: variable_pb2
           .: .
           identifier: VariableDef
          argument_list: ()
           (: (
           ): )
       expression_statement: variable.ParseFromString(raw)
        call: variable.ParseFromString(raw)
         attribute: variable.ParseFromString
          identifier: variable
          .: .
          identifier: ParseFromString
         argument_list: (raw)
          (: (
          identifier: raw
          ): )
       expression_statement: variable.ClearField("initializer_name")
        call: variable.ClearField("initializer_name")
         attribute: variable.ClearField
          identifier: variable
          .: .
          identifier: ClearField
         argument_list: ("initializer_name")
          (: (
          string: "initializer_name"
           string_start: "
           string_content: initializer_name
           string_end: "
          ): )
       expression_statement: raw_list.append(variable.SerializeToString())
        call: raw_list.append(variable.SerializeToString())
         attribute: raw_list.append
          identifier: raw_list
          .: .
          identifier: append
         argument_list: (variable.SerializeToString())
          (: (
          call: variable.SerializeToString()
           attribute: variable.SerializeToString
            identifier: variable
            .: .
            identifier: SerializeToString
           argument_list: ()
            (: (
            ): )
          ): )
     expression_statement: meta_graph.collection_def[name].bytes_list.value[:] = raw_list
      assignment: meta_graph.collection_def[name].bytes_list.value[:] = raw_list
       subscript: meta_graph.collection_def[name].bytes_list.value[:]
        attribute: meta_graph.collection_def[name].bytes_list.value
         attribute: meta_graph.collection_def[name].bytes_list
          subscript: meta_graph.collection_def[name]
           attribute: meta_graph.collection_def
            identifier: meta_graph
            .: .
            identifier: collection_def
           [: [
           identifier: name
           ]: ]
          .: .
          identifier: bytes_list
         .: .
         identifier: value
        [: [
        slice: :
         :: :
        ]: ]
       =: =
       identifier: raw_list
   comment: # Add a collection 'train_op' so that Grappler knows the outputs.
   expression_statement: fetch_collection = meta_graph_pb2.CollectionDef()
    assignment: fetch_collection = meta_graph_pb2.CollectionDef()
     identifier: fetch_collection
     =: =
     call: meta_graph_pb2.CollectionDef()
      attribute: meta_graph_pb2.CollectionDef
       identifier: meta_graph_pb2
       .: .
       identifier: CollectionDef
      argument_list: ()
       (: (
       ): )
   for_statement: for array in func.inputs + func.outputs:
    fetch_collection.node_list.value.append(array.name)
    for: for
    identifier: array
    in: in
    binary_operator: func.inputs + func.outputs
     attribute: func.inputs
      identifier: func
      .: .
      identifier: inputs
     +: +
     attribute: func.outputs
      identifier: func
      .: .
      identifier: outputs
    :: :
    block: fetch_collection.node_list.value.append(array.name)
     expression_statement: fetch_collection.node_list.value.append(array.name)
      call: fetch_collection.node_list.value.append(array.name)
       attribute: fetch_collection.node_list.value.append
        attribute: fetch_collection.node_list.value
         attribute: fetch_collection.node_list
          identifier: fetch_collection
          .: .
          identifier: node_list
         .: .
         identifier: value
        .: .
        identifier: append
       argument_list: (array.name)
        (: (
        attribute: array.name
         identifier: array
         .: .
         identifier: name
        ): )
   expression_statement: meta_graph.collection_def["train_op"].CopyFrom(fetch_collection)
    call: meta_graph.collection_def["train_op"].CopyFrom(fetch_collection)
     attribute: meta_graph.collection_def["train_op"].CopyFrom
      subscript: meta_graph.collection_def["train_op"]
       attribute: meta_graph.collection_def
        identifier: meta_graph
        .: .
        identifier: collection_def
       [: [
       string: "train_op"
        string_start: "
        string_content: train_op
        string_end: "
       ]: ]
      .: .
      identifier: CopyFrom
     argument_list: (fetch_collection)
      (: (
      identifier: fetch_collection
      ): )
   comment: # Initialize RewriterConfig with everything disabled except function inlining.
   expression_statement: config = config_pb2.ConfigProto()
    assignment: config = config_pb2.ConfigProto()
     identifier: config
     =: =
     call: config_pb2.ConfigProto()
      attribute: config_pb2.ConfigProto
       identifier: config_pb2
       .: .
       identifier: ConfigProto
      argument_list: ()
       (: (
       ): )
   expression_statement: rewrite_options = config.graph_options.rewrite_options
    assignment: rewrite_options = config.graph_options.rewrite_options
     identifier: rewrite_options
     =: =
     attribute: config.graph_options.rewrite_options
      attribute: config.graph_options
       identifier: config
       .: .
       identifier: graph_options
      .: .
      identifier: rewrite_options
   expression_statement: rewrite_options.min_graph_nodes = -1
    assignment: rewrite_options.min_graph_nodes = -1
     attribute: rewrite_options.min_graph_nodes
      identifier: rewrite_options
      .: .
      identifier: min_graph_nodes
     =: =
     unary_operator: -1
      -: -
      integer: 1
   comment: # do not skip small graphs
   expression_statement: rewrite_options.optimizers.append("function")
    call: rewrite_options.optimizers.append("function")
     attribute: rewrite_options.optimizers.append
      attribute: rewrite_options.optimizers
       identifier: rewrite_options
       .: .
       identifier: optimizers
      .: .
      identifier: append
     argument_list: ("function")
      (: (
      string: "function"
       string_start: "
       string_content: function
       string_end: "
      ): )
   expression_statement: new_graph_def = tf_optimizer.OptimizeGraph(config, meta_graph)
    assignment: new_graph_def = tf_optimizer.OptimizeGraph(config, meta_graph)
     identifier: new_graph_def
     =: =
     call: tf_optimizer.OptimizeGraph(config, meta_graph)
      attribute: tf_optimizer.OptimizeGraph
       identifier: tf_optimizer
       .: .
       identifier: OptimizeGraph
      argument_list: (config, meta_graph)
       (: (
       identifier: config
       ,: ,
       identifier: meta_graph
       ): )
   return_statement: return new_graph_def
    return: return
    identifier: new_graph_def
 function_definition: def _annotate_variable_ops(func, graph_def):
  """Annotates variable operations with custom `_shape` attribute.

  This is required for the converters and shape inference. The graph
  definition is modified in-place.

  Args:
    func: Function represented by the graph definition.
    graph_def: Graph definition to be annotated in-place.

  Raises:
    RuntimeError: if some shapes cannot be annotated.
  """
  ph_shape_map = {}
  for ph, var in zip(func.graph.internal_captures, func.variables):
    ph_shape_map[ph.name] = var.shape
  # Construct a mapping of node names to nodes
  name_to_node = {node.name: node for node in graph_def.node}
  # Go through all the ReadVariableOp nodes in the graph def
  for node in graph_def.node:
    if node.op == "ReadVariableOp" or node.op == "ResourceGather":
      node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
  def: def
  identifier: _annotate_variable_ops
  parameters: (func, graph_def)
   (: (
   identifier: func
   ,: ,
   identifier: graph_def
   ): )
  :: :
  block: """Annotates variable operations with custom `_shape` attribute.

  This is required for the converters and shape inference. The graph
  definition is modified in-place.

  Args:
    func: Function represented by the graph definition.
    graph_def: Graph definition to be annotated in-place.

  Raises:
    RuntimeError: if some shapes cannot be annotated.
  """
  ph_shape_map = {}
  for ph, var in zip(func.graph.internal_captures, func.variables):
    ph_shape_map[ph.name] = var.shape
  # Construct a mapping of node names to nodes
  name_to_node = {node.name: node for node in graph_def.node}
  # Go through all the ReadVariableOp nodes in the graph def
  for node in graph_def.node:
    if node.op == "ReadVariableOp" or node.op == "ResourceGather":
      node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
   expression_statement: """Annotates variable operations with custom `_shape` attribute.

  This is required for the converters and shape inference. The graph
  definition is modified in-place.

  Args:
    func: Function represented by the graph definition.
    graph_def: Graph definition to be annotated in-place.

  Raises:
    RuntimeError: if some shapes cannot be annotated.
  """
    string: """Annotates variable operations with custom `_shape` attribute.

  This is required for the converters and shape inference. The graph
  definition is modified in-place.

  Args:
    func: Function represented by the graph definition.
    graph_def: Graph definition to be annotated in-place.

  Raises:
    RuntimeError: if some shapes cannot be annotated.
  """
     string_start: """
     string_content: Annotates variable operations with custom `_shape` attribute.

  This is required for the converters and shape inference. The graph
  definition is modified in-place.

  Args:
    func: Function represented by the graph definition.
    graph_def: Graph definition to be annotated in-place.

  Raises:
    RuntimeError: if some shapes cannot be annotated.
  
     string_end: """
   expression_statement: ph_shape_map = {}
    assignment: ph_shape_map = {}
     identifier: ph_shape_map
     =: =
     dictionary: {}
      {: {
      }: }
   for_statement: for ph, var in zip(func.graph.internal_captures, func.variables):
    ph_shape_map[ph.name] = var.shape
    for: for
    pattern_list: ph, var
     identifier: ph
     ,: ,
     identifier: var
    in: in
    call: zip(func.graph.internal_captures, func.variables)
     identifier: zip
     argument_list: (func.graph.internal_captures, func.variables)
      (: (
      attribute: func.graph.internal_captures
       attribute: func.graph
        identifier: func
        .: .
        identifier: graph
       .: .
       identifier: internal_captures
      ,: ,
      attribute: func.variables
       identifier: func
       .: .
       identifier: variables
      ): )
    :: :
    block: ph_shape_map[ph.name] = var.shape
     expression_statement: ph_shape_map[ph.name] = var.shape
      assignment: ph_shape_map[ph.name] = var.shape
       subscript: ph_shape_map[ph.name]
        identifier: ph_shape_map
        [: [
        attribute: ph.name
         identifier: ph
         .: .
         identifier: name
        ]: ]
       =: =
       attribute: var.shape
        identifier: var
        .: .
        identifier: shape
   comment: # Construct a mapping of node names to nodes
   expression_statement: name_to_node = {node.name: node for node in graph_def.node}
    assignment: name_to_node = {node.name: node for node in graph_def.node}
     identifier: name_to_node
     =: =
     dictionary_comprehension: {node.name: node for node in graph_def.node}
      {: {
      pair: node.name: node
       attribute: node.name
        identifier: node
        .: .
        identifier: name
       :: :
       identifier: node
      for_in_clause: for node in graph_def.node
       for: for
       identifier: node
       in: in
       attribute: graph_def.node
        identifier: graph_def
        .: .
        identifier: node
      }: }
   comment: # Go through all the ReadVariableOp nodes in the graph def
   for_statement: for node in graph_def.node:
    if node.op == "ReadVariableOp" or node.op == "ResourceGather":
      node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
    for: for
    identifier: node
    in: in
    attribute: graph_def.node
     identifier: graph_def
     .: .
     identifier: node
    :: :
    block: if node.op == "ReadVariableOp" or node.op == "ResourceGather":
      node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
     if_statement: if node.op == "ReadVariableOp" or node.op == "ResourceGather":
      node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
      if: if
      boolean_operator: node.op == "ReadVariableOp" or node.op == "ResourceGather"
       comparison_operator: node.op == "ReadVariableOp"
        attribute: node.op
         identifier: node
         .: .
         identifier: op
        ==: ==
        string: "ReadVariableOp"
         string_start: "
         string_content: ReadVariableOp
         string_end: "
       or: or
       comparison_operator: node.op == "ResourceGather"
        attribute: node.op
         identifier: node
         .: .
         identifier: op
        ==: ==
        string: "ResourceGather"
         string_start: "
         string_content: ResourceGather
         string_end: "
      :: :
      block: node_ = node
      # Go up the chain of identities to find a placeholder
      while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
      ph_name = node_.input[0] + ":0"
      if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
       expression_statement: node_ = node
        assignment: node_ = node
         identifier: node_
         =: =
         identifier: node
       comment: # Go up the chain of identities to find a placeholder
       while_statement: while name_to_node[node_.input[0]].op == "Identity":
        node_ = name_to_node[node_.input[0]]
        while: while
        comparison_operator: name_to_node[node_.input[0]].op == "Identity"
         attribute: name_to_node[node_.input[0]].op
          subscript: name_to_node[node_.input[0]]
           identifier: name_to_node
           [: [
           subscript: node_.input[0]
            attribute: node_.input
             identifier: node_
             .: .
             identifier: input
            [: [
            integer: 0
            ]: ]
           ]: ]
          .: .
          identifier: op
         ==: ==
         string: "Identity"
          string_start: "
          string_content: Identity
          string_end: "
        :: :
        block: node_ = name_to_node[node_.input[0]]
         expression_statement: node_ = name_to_node[node_.input[0]]
          assignment: node_ = name_to_node[node_.input[0]]
           identifier: node_
           =: =
           subscript: name_to_node[node_.input[0]]
            identifier: name_to_node
            [: [
            subscript: node_.input[0]
             attribute: node_.input
              identifier: node_
              .: .
              identifier: input
             [: [
             integer: 0
             ]: ]
            ]: ]
       expression_statement: ph_name = node_.input[0] + ":0"
        assignment: ph_name = node_.input[0] + ":0"
         identifier: ph_name
         =: =
         binary_operator: node_.input[0] + ":0"
          subscript: node_.input[0]
           attribute: node_.input
            identifier: node_
            .: .
            identifier: input
           [: [
           integer: 0
           ]: ]
          +: +
          string: ":0"
           string_start: "
           string_content: :0
           string_end: "
       if_statement: if ph_name in ph_shape_map:
        shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
      else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
        if: if
        comparison_operator: ph_name in ph_shape_map
         identifier: ph_name
         in: in
         identifier: ph_shape_map
        :: :
        block: shape = ph_shape_map[ph_name]
        node.attr["_shape"].shape.CopyFrom(shape.as_proto())
         expression_statement: shape = ph_shape_map[ph_name]
          assignment: shape = ph_shape_map[ph_name]
           identifier: shape
           =: =
           subscript: ph_shape_map[ph_name]
            identifier: ph_shape_map
            [: [
            identifier: ph_name
            ]: ]
         expression_statement: node.attr["_shape"].shape.CopyFrom(shape.as_proto())
          call: node.attr["_shape"].shape.CopyFrom(shape.as_proto())
           attribute: node.attr["_shape"].shape.CopyFrom
            attribute: node.attr["_shape"].shape
             subscript: node.attr["_shape"]
              attribute: node.attr
               identifier: node
               .: .
               identifier: attr
              [: [
              string: "_shape"
               string_start: "
               string_content: _shape
               string_end: "
              ]: ]
             .: .
             identifier: shape
            .: .
            identifier: CopyFrom
           argument_list: (shape.as_proto())
            (: (
            call: shape.as_proto()
             attribute: shape.as_proto
              identifier: shape
              .: .
              identifier: as_proto
             argument_list: ()
              (: (
              ): )
            ): )
        else_clause: else:
        raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
         else: else
         :: :
         block: raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
          raise_statement: raise RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
           raise: raise
           call: RuntimeError(
            "Not found in the function captures: {}".format(ph_name))
            identifier: RuntimeError
            argument_list: (
            "Not found in the function captures: {}".format(ph_name))
             (: (
             call: "Not found in the function captures: {}".format(ph_name)
              attribute: "Not found in the function captures: {}".format
               string: "Not found in the function captures: {}"
                string_start: "
                string_content: Not found in the function captures: {}
                string_end: "
               .: .
               identifier: format
              argument_list: (ph_name)
               (: (
               identifier: ph_name
               ): )
             ): )
 function_definition: def _save_calibration_table(node):
  try:
    calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
    node.attr["calibration_data"].s = calibration_table.numpy()
  except (errors.UnknownError, errors.NotFoundError):
    logging.warning("Warning calibration error for %s", node.name)
  def: def
  identifier: _save_calibration_table
  parameters: (node)
   (: (
   identifier: node
   ): )
  :: :
  block: try:
    calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
    node.attr["calibration_data"].s = calibration_table.numpy()
  except (errors.UnknownError, errors.NotFoundError):
    logging.warning("Warning calibration error for %s", node.name)
   try_statement: try:
    calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
    node.attr["calibration_data"].s = calibration_table.numpy()
  except (errors.UnknownError, errors.NotFoundError):
    logging.warning("Warning calibration error for %s", node.name)
    try: try
    :: :
    block: calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
    node.attr["calibration_data"].s = calibration_table.numpy()
     expression_statement: calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
      assignment: calibration_table = gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
       identifier: calibration_table
       =: =
       call: gen_trt_ops.get_calibration_data_op(
        _get_canonical_engine_name(node.name))
        attribute: gen_trt_ops.get_calibration_data_op
         identifier: gen_trt_ops
         .: .
         identifier: get_calibration_data_op
        argument_list: (
        _get_canonical_engine_name(node.name))
         (: (
         call: _get_canonical_engine_name(node.name)
          identifier: _get_canonical_engine_name
          argument_list: (node.name)
           (: (
           attribute: node.name
            identifier: node
            .: .
            identifier: name
           ): )
         ): )
     expression_statement: node.attr["calibration_data"].s = calibration_table.numpy()
      assignment: node.attr["calibration_data"].s = calibration_table.numpy()
       attribute: node.attr["calibration_data"].s
        subscript: node.attr["calibration_data"]
         attribute: node.attr
          identifier: node
          .: .
          identifier: attr
         [: [
         string: "calibration_data"
          string_start: "
          string_content: calibration_data
          string_end: "
         ]: ]
        .: .
        identifier: s
       =: =
       call: calibration_table.numpy()
        attribute: calibration_table.numpy
         identifier: calibration_table
         .: .
         identifier: numpy
        argument_list: ()
         (: (
         ): )
    except_clause: except (errors.UnknownError, errors.NotFoundError):
    logging.warning("Warning calibration error for %s", node.name)
     except: except
     tuple: (errors.UnknownError, errors.NotFoundError)
      (: (
      attribute: errors.UnknownError
       identifier: errors
       .: .
       identifier: UnknownError
      ,: ,
      attribute: errors.NotFoundError
       identifier: errors
       .: .
       identifier: NotFoundError
      ): )
     :: :
     block: logging.warning("Warning calibration error for %s", node.name)
      expression_statement: logging.warning("Warning calibration error for %s", node.name)
       call: logging.warning("Warning calibration error for %s", node.name)
        attribute: logging.warning
         identifier: logging
         .: .
         identifier: warning
        argument_list: ("Warning calibration error for %s", node.name)
         (: (
         string: "Warning calibration error for %s"
          string_start: "
          string_content: Warning calibration error for %s
          string_end: "
         ,: ,
         attribute: node.name
          identifier: node
          .: .
          identifier: name
         ): )
 function_definition: def _convert_to_tensor(inp):
  try:
    if isinstance(inp, dict):
      args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
    else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
  except:
    error_msg = "Failed to convert input to tensor."
    logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
    raise RuntimeError(error_msg)

  return args, kwargs
  def: def
  identifier: _convert_to_tensor
  parameters: (inp)
   (: (
   identifier: inp
   ): )
  :: :
  block: try:
    if isinstance(inp, dict):
      args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
    else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
  except:
    error_msg = "Failed to convert input to tensor."
    logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
    raise RuntimeError(error_msg)

  return args, kwargs
   try_statement: try:
    if isinstance(inp, dict):
      args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
    else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
  except:
    error_msg = "Failed to convert input to tensor."
    logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
    raise RuntimeError(error_msg)
    try: try
    :: :
    block: if isinstance(inp, dict):
      args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
    else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
     if_statement: if isinstance(inp, dict):
      args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
    else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
      if: if
      call: isinstance(inp, dict)
       identifier: isinstance
       argument_list: (inp, dict)
        (: (
        identifier: inp
        ,: ,
        identifier: dict
        ): )
      :: :
      block: args = []
      kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
       expression_statement: args = []
        assignment: args = []
         identifier: args
         =: =
         list: []
          [: [
          ]: ]
       expression_statement: kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
        assignment: kwargs = {k: ops.convert_to_tensor(v) for k, v in inp.items()}
         identifier: kwargs
         =: =
         dictionary_comprehension: {k: ops.convert_to_tensor(v) for k, v in inp.items()}
          {: {
          pair: k: ops.convert_to_tensor(v)
           identifier: k
           :: :
           call: ops.convert_to_tensor(v)
            attribute: ops.convert_to_tensor
             identifier: ops
             .: .
             identifier: convert_to_tensor
            argument_list: (v)
             (: (
             identifier: v
             ): )
          for_in_clause: for k, v in inp.items()
           for: for
           pattern_list: k, v
            identifier: k
            ,: ,
            identifier: v
           in: in
           call: inp.items()
            attribute: inp.items
             identifier: inp
             .: .
             identifier: items
            argument_list: ()
             (: (
             ): )
          }: }
      else_clause: else:
      kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
       else: else
       :: :
       block: kwargs = {}
      if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
        expression_statement: kwargs = {}
         assignment: kwargs = {}
          identifier: kwargs
          =: =
          dictionary: {}
           {: {
           }: }
        if_statement: if isinstance(inp, (list, tuple)):
        args = map(ops.convert_to_tensor, inp)
      else:
        args = [ops.convert_to_tensor(inp)]
         if: if
         call: isinstance(inp, (list, tuple))
          identifier: isinstance
          argument_list: (inp, (list, tuple))
           (: (
           identifier: inp
           ,: ,
           tuple: (list, tuple)
            (: (
            identifier: list
            ,: ,
            identifier: tuple
            ): )
           ): )
         :: :
         block: args = map(ops.convert_to_tensor, inp)
          expression_statement: args = map(ops.convert_to_tensor, inp)
           assignment: args = map(ops.convert_to_tensor, inp)
            identifier: args
            =: =
            call: map(ops.convert_to_tensor, inp)
             identifier: map
             argument_list: (ops.convert_to_tensor, inp)
              (: (
              attribute: ops.convert_to_tensor
               identifier: ops
               .: .
               identifier: convert_to_tensor
              ,: ,
              identifier: inp
              ): )
         else_clause: else:
        args = [ops.convert_to_tensor(inp)]
          else: else
          :: :
          block: args = [ops.convert_to_tensor(inp)]
           expression_statement: args = [ops.convert_to_tensor(inp)]
            assignment: args = [ops.convert_to_tensor(inp)]
             identifier: args
             =: =
             list: [ops.convert_to_tensor(inp)]
              [: [
              call: ops.convert_to_tensor(inp)
               attribute: ops.convert_to_tensor
                identifier: ops
                .: .
                identifier: convert_to_tensor
               argument_list: (inp)
                (: (
                identifier: inp
                ): )
              ]: ]
    except_clause: except:
    error_msg = "Failed to convert input to tensor."
    logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
    raise RuntimeError(error_msg)
     except: except
     :: :
     block: error_msg = "Failed to convert input to tensor."
    logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
    raise RuntimeError(error_msg)
      expression_statement: error_msg = "Failed to convert input to tensor."
       assignment: error_msg = "Failed to convert input to tensor."
        identifier: error_msg
        =: =
        string: "Failed to convert input to tensor."
         string_start: "
         string_content: Failed to convert input to tensor.
         string_end: "
      expression_statement: logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
       call: logging.error(error_msg + "\ninp = `{0}`\n".format(inp))
        attribute: logging.error
         identifier: logging
         .: .
         identifier: error
        argument_list: (error_msg + "\ninp = `{0}`\n".format(inp))
         (: (
         binary_operator: error_msg + "\ninp = `{0}`\n".format(inp)
          identifier: error_msg
          +: +
          call: "\ninp = `{0}`\n".format(inp)
           attribute: "\ninp = `{0}`\n".format
            string: "\ninp = `{0}`\n"
             string_start: "
             string_content: \ninp = `{0}`\n
              escape_sequence: \n
              escape_sequence: \n
             string_end: "
            .: .
            identifier: format
           argument_list: (inp)
            (: (
            identifier: inp
            ): )
         ): )
      raise_statement: raise RuntimeError(error_msg)
       raise: raise
       call: RuntimeError(error_msg)
        identifier: RuntimeError
        argument_list: (error_msg)
         (: (
         identifier: error_msg
         ): )
   return_statement: return args, kwargs
    return: return
    expression_list: args, kwargs
     identifier: args
     ,: ,
     identifier: kwargs
 decorated_definition: @tf_export("experimental.tensorrt.Converter", v1=[])
class TrtGraphConverterV2(object):
  """An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  """

  def _verify_profile_strategy(self, strategy):
    supported_strategies = [s.lower() for s in supported_profile_strategies()]
    if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
    if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")

  @deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None):
    """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False

  def _need_trt_profiles(self):
    return self._use_dynamic_shape

  def _run_conversion(self, meta_graph_def):
    """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
    grappler_session_config = config_pb2.ConfigProto()
    # Always set `allow_build_at_runtime` for offline TensorRT engine building.
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
    return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")

  def _for_each_trt_node(self, graph_def, fn):
    """Helper method to manipulate all TRTEngineOps in a GraphDef."""
    for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
    for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)

  def _execute_calibration(self, calibration_input_fn):
    """Run INT8 calibration with the provided input generator function."""
    for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)

    self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)

    # Rebuild the function since calibration has changed the graph.
    self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
    self._calibrated = True

  # TODO(laigd): provide a utility function to optimize a ConcreteFunction and
  # use it here (b/124792963).
  def convert(self, calibration_input_fn=None):
    """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
    assert not self._converted

    # Creating an empty tensor to fetch queried device
    device_requested = array_ops.zeros([]).device

    if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")

    if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")

    if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
    if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")

    self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
    func = self._saved_model.signatures[self._input_saved_model_signature_key]
    if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
    frozen_graph_def = frozen_func.graph.as_graph_def()

    # Clear any prior device assignments
    logging.info("Clearing prior device assignments in loaded saved model")
    for node in frozen_graph_def.node:
      node.device = ""

    if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)

    # Add a collection 'train_op' so that Grappler knows the outputs.
    fetch_collection = meta_graph_pb2.CollectionDef()
    for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
    grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)

    # Run TRT optimizer in Grappler to convert the graph.
    self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
    self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)

    if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn

    self._converted = True

    graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
    if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )

    return self._converted_func

  def build(self, input_fn):
    """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
    if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
    if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
    if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
    if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")

    def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value

    if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func

    first_input = None
    # Run inference:
    #   Builds TRT engines if self._need_trt_profiles is False.
    #   Builds TRT optimization profiles if self._need_trt_profiles is True.
    for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)

    if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))

    # Run calibration if required, this would have been skipped in
    # the convert step
    if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)

    self._build_called_once = True

  def save(self,
           output_saved_model_dir,
           save_gpu_specific_engines=True,
           options=None):
    """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
    assert self._converted

    # 'remove_native_segments': setting this value to True removes native segments
    # associated with each TRT engine. This option can be used to reduce the size
    # of the converted model. Please note that a converted model without native
    # segments can't be used for collecting profiles, building or re-converting.
    # The reduced model can only be used for inference when no native segments
    # are required for computation. When remove_native_segments flag is set to
    # True, the converted_graph_def needs to be reduced before saved_model
    # function serialization.
    if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()

    if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
    # Serialize the TRT engines in the cache if any, and create trackable
    # resource to track them.
    engine_asset_dir = tempfile.mkdtemp()
    resource_map = {}

    def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)

    self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
    # If the graph is frozen, tracked variables are not needed by the converted model.
    trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
    trackable.trt_engine_resources = resource_map

    # Set allow_build_at_runtime=False if asked by user.
    #
    # This attribute is set here because build() needs it to be True in order to
    # build engines.
    if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func

    # Rewrite the signature map using the optimized ConcreteFunction.
    signatures = {self._input_saved_model_signature_key: self._converted_func}
    save.save(trackable, output_saved_model_dir, signatures, options=options)

  def summary(self, line_length=160, detailed=True, print_fn=None):
    """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
    if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")

    if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")

    if print_fn is None:
      print_fn = print

    # positions are percentage of `line_length`. positions[i]+1 is the starting
    # position for (i+1)th field. We also make sure that the last char printed
    # for each field is a space.
    columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]

    positions = [int(line_length * p) for _, p in columns]
    positions = np.cumsum(positions).tolist()
    headers = [h for h, _ in columns]

    _print_row(headers, positions, print_fn=print_fn)
    print_fn("=" * line_length)

    n_engines = 0
    n_ops_converted = 0
    n_ops_not_converted = 0

    graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)

    trtengineops_dict = dict()
    for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1

    for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")

    print_fn(f"\n{'='*line_length}")
    print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
    total_ops = n_ops_not_converted + n_ops_converted
    conversion_ratio = n_ops_converted / total_ops * 100
    print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
  decorator: @tf_export("experimental.tensorrt.Converter", v1=[])
   @: @
   call: tf_export("experimental.tensorrt.Converter", v1=[])
    identifier: tf_export
    argument_list: ("experimental.tensorrt.Converter", v1=[])
     (: (
     string: "experimental.tensorrt.Converter"
      string_start: "
      string_content: experimental.tensorrt.Converter
      string_end: "
     ,: ,
     keyword_argument: v1=[]
      identifier: v1
      =: =
      list: []
       [: [
       ]: ]
     ): )
  class_definition: class TrtGraphConverterV2(object):
  """An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  """

  def _verify_profile_strategy(self, strategy):
    supported_strategies = [s.lower() for s in supported_profile_strategies()]
    if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
    if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")

  @deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None):
    """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False

  def _need_trt_profiles(self):
    return self._use_dynamic_shape

  def _run_conversion(self, meta_graph_def):
    """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
    grappler_session_config = config_pb2.ConfigProto()
    # Always set `allow_build_at_runtime` for offline TensorRT engine building.
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
    return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")

  def _for_each_trt_node(self, graph_def, fn):
    """Helper method to manipulate all TRTEngineOps in a GraphDef."""
    for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
    for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)

  def _execute_calibration(self, calibration_input_fn):
    """Run INT8 calibration with the provided input generator function."""
    for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)

    self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)

    # Rebuild the function since calibration has changed the graph.
    self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
    self._calibrated = True

  # TODO(laigd): provide a utility function to optimize a ConcreteFunction and
  # use it here (b/124792963).
  def convert(self, calibration_input_fn=None):
    """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
    assert not self._converted

    # Creating an empty tensor to fetch queried device
    device_requested = array_ops.zeros([]).device

    if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")

    if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")

    if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
    if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")

    self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
    func = self._saved_model.signatures[self._input_saved_model_signature_key]
    if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
    frozen_graph_def = frozen_func.graph.as_graph_def()

    # Clear any prior device assignments
    logging.info("Clearing prior device assignments in loaded saved model")
    for node in frozen_graph_def.node:
      node.device = ""

    if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)

    # Add a collection 'train_op' so that Grappler knows the outputs.
    fetch_collection = meta_graph_pb2.CollectionDef()
    for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
    grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)

    # Run TRT optimizer in Grappler to convert the graph.
    self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
    self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)

    if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn

    self._converted = True

    graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
    if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )

    return self._converted_func

  def build(self, input_fn):
    """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
    if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
    if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
    if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
    if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")

    def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value

    if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func

    first_input = None
    # Run inference:
    #   Builds TRT engines if self._need_trt_profiles is False.
    #   Builds TRT optimization profiles if self._need_trt_profiles is True.
    for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)

    if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))

    # Run calibration if required, this would have been skipped in
    # the convert step
    if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)

    self._build_called_once = True

  def save(self,
           output_saved_model_dir,
           save_gpu_specific_engines=True,
           options=None):
    """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
    assert self._converted

    # 'remove_native_segments': setting this value to True removes native segments
    # associated with each TRT engine. This option can be used to reduce the size
    # of the converted model. Please note that a converted model without native
    # segments can't be used for collecting profiles, building or re-converting.
    # The reduced model can only be used for inference when no native segments
    # are required for computation. When remove_native_segments flag is set to
    # True, the converted_graph_def needs to be reduced before saved_model
    # function serialization.
    if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()

    if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
    # Serialize the TRT engines in the cache if any, and create trackable
    # resource to track them.
    engine_asset_dir = tempfile.mkdtemp()
    resource_map = {}

    def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)

    self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
    # If the graph is frozen, tracked variables are not needed by the converted model.
    trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
    trackable.trt_engine_resources = resource_map

    # Set allow_build_at_runtime=False if asked by user.
    #
    # This attribute is set here because build() needs it to be True in order to
    # build engines.
    if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func

    # Rewrite the signature map using the optimized ConcreteFunction.
    signatures = {self._input_saved_model_signature_key: self._converted_func}
    save.save(trackable, output_saved_model_dir, signatures, options=options)

  def summary(self, line_length=160, detailed=True, print_fn=None):
    """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
    if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")

    if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")

    if print_fn is None:
      print_fn = print

    # positions are percentage of `line_length`. positions[i]+1 is the starting
    # position for (i+1)th field. We also make sure that the last char printed
    # for each field is a space.
    columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]

    positions = [int(line_length * p) for _, p in columns]
    positions = np.cumsum(positions).tolist()
    headers = [h for h, _ in columns]

    _print_row(headers, positions, print_fn=print_fn)
    print_fn("=" * line_length)

    n_engines = 0
    n_ops_converted = 0
    n_ops_not_converted = 0

    graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)

    trtengineops_dict = dict()
    for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1

    for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")

    print_fn(f"\n{'='*line_length}")
    print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
    total_ops = n_ops_not_converted + n_ops_converted
    conversion_ratio = n_ops_converted / total_ops * 100
    print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
   class: class
   identifier: TrtGraphConverterV2
   argument_list: (object)
    (: (
    identifier: object
    ): )
   :: :
   block: """An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  """

  def _verify_profile_strategy(self, strategy):
    supported_strategies = [s.lower() for s in supported_profile_strategies()]
    if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
    if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")

  @deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None):
    """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False

  def _need_trt_profiles(self):
    return self._use_dynamic_shape

  def _run_conversion(self, meta_graph_def):
    """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
    grappler_session_config = config_pb2.ConfigProto()
    # Always set `allow_build_at_runtime` for offline TensorRT engine building.
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
    return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")

  def _for_each_trt_node(self, graph_def, fn):
    """Helper method to manipulate all TRTEngineOps in a GraphDef."""
    for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
    for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)

  def _execute_calibration(self, calibration_input_fn):
    """Run INT8 calibration with the provided input generator function."""
    for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)

    self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)

    # Rebuild the function since calibration has changed the graph.
    self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
    self._calibrated = True

  # TODO(laigd): provide a utility function to optimize a ConcreteFunction and
  # use it here (b/124792963).
  def convert(self, calibration_input_fn=None):
    """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
    assert not self._converted

    # Creating an empty tensor to fetch queried device
    device_requested = array_ops.zeros([]).device

    if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")

    if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")

    if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
    if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")

    self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
    func = self._saved_model.signatures[self._input_saved_model_signature_key]
    if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
    frozen_graph_def = frozen_func.graph.as_graph_def()

    # Clear any prior device assignments
    logging.info("Clearing prior device assignments in loaded saved model")
    for node in frozen_graph_def.node:
      node.device = ""

    if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)

    # Add a collection 'train_op' so that Grappler knows the outputs.
    fetch_collection = meta_graph_pb2.CollectionDef()
    for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
    grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)

    # Run TRT optimizer in Grappler to convert the graph.
    self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
    self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)

    if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn

    self._converted = True

    graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
    if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )

    return self._converted_func

  def build(self, input_fn):
    """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
    if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
    if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
    if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
    if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")

    def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value

    if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func

    first_input = None
    # Run inference:
    #   Builds TRT engines if self._need_trt_profiles is False.
    #   Builds TRT optimization profiles if self._need_trt_profiles is True.
    for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)

    if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))

    # Run calibration if required, this would have been skipped in
    # the convert step
    if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)

    self._build_called_once = True

  def save(self,
           output_saved_model_dir,
           save_gpu_specific_engines=True,
           options=None):
    """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
    assert self._converted

    # 'remove_native_segments': setting this value to True removes native segments
    # associated with each TRT engine. This option can be used to reduce the size
    # of the converted model. Please note that a converted model without native
    # segments can't be used for collecting profiles, building or re-converting.
    # The reduced model can only be used for inference when no native segments
    # are required for computation. When remove_native_segments flag is set to
    # True, the converted_graph_def needs to be reduced before saved_model
    # function serialization.
    if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()

    if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
    # Serialize the TRT engines in the cache if any, and create trackable
    # resource to track them.
    engine_asset_dir = tempfile.mkdtemp()
    resource_map = {}

    def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)

    self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
    # If the graph is frozen, tracked variables are not needed by the converted model.
    trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
    trackable.trt_engine_resources = resource_map

    # Set allow_build_at_runtime=False if asked by user.
    #
    # This attribute is set here because build() needs it to be True in order to
    # build engines.
    if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func

    # Rewrite the signature map using the optimized ConcreteFunction.
    signatures = {self._input_saved_model_signature_key: self._converted_func}
    save.save(trackable, output_saved_model_dir, signatures, options=options)

  def summary(self, line_length=160, detailed=True, print_fn=None):
    """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
    if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")

    if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")

    if print_fn is None:
      print_fn = print

    # positions are percentage of `line_length`. positions[i]+1 is the starting
    # position for (i+1)th field. We also make sure that the last char printed
    # for each field is a space.
    columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]

    positions = [int(line_length * p) for _, p in columns]
    positions = np.cumsum(positions).tolist()
    headers = [h for h, _ in columns]

    _print_row(headers, positions, print_fn=print_fn)
    print_fn("=" * line_length)

    n_engines = 0
    n_ops_converted = 0
    n_ops_not_converted = 0

    graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)

    trtengineops_dict = dict()
    for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1

    for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")

    print_fn(f"\n{'='*line_length}")
    print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
    total_ops = n_ops_not_converted + n_ops_converted
    conversion_ratio = n_ops_converted / total_ops * 100
    print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
    expression_statement: """An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  """
     string: """An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  """
      string_start: """
      string_content: An offline converter for TF-TRT transformation for TF 2.0 SavedModels.

  Windows support is provided experimentally. No guarantee is made regarding
  functionality or engineering support. Use at your own risk.

  There are several ways to run the conversion:

  1. FP32/FP16 precision

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16')
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()
     converter.save(output_saved_model_dir)
     ```

     In this case, no TRT engines will be built or saved in the converted
     SavedModel. But if input data is available during conversion, we can still
     build and save the TRT engines to reduce the cost during inference (see
     option 2 below).

  2. FP32/FP16 precision with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='FP16',
         # Set this to a large enough number so it can cache all the engines.
         maximum_cached_engines=16)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)
     converter.convert()

     # Define a generator function that yields input data, and use it to execute
     # the graph to build TRT engines.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
     converter.save(output_saved_model_dir)  # Generated engines will be saved.
     ```

     In this way, one engine will be built/saved for each unique input shapes of
     the TRTEngineOp. This is good for applications that cannot afford building
     engines during inference but have access to input data that is similar to
     the one used in production (for example, that has the same input shapes).
     Also, the generated TRT engines is platform dependent, so we need to run
     `build()` in an environment that is similar to production (e.g. with
     same type of GPU).

  3. INT8 precision and calibration with pre-built engines

     ```python
     params = tf.experimental.tensorrt.ConversionParams(
         precision_mode='INT8',
         # Currently only one INT8 engine is supported in this mode.
         maximum_cached_engines=1,
         use_calibration=True)
     converter = tf.experimental.tensorrt.Converter(
         input_saved_model_dir="my_dir", conversion_params=params)

     # Define a generator function that yields input data, and run INT8
     # calibration with the data. All input data should have the same shape.
     # At the end of convert(), the calibration stats (e.g. range information)
     # will be saved and can be used to generate more TRT engines with different
     # shapes. Also, one TRT engine will be generated (with the same shape as
     # the calibration data) for save later.
     def my_calibration_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2

     converter.convert(calibration_input_fn=my_calibration_input_fn)

     # (Optional) Generate more TRT engines offline (same as the previous
     # option), to avoid the cost of generating them during inference.
     def my_input_fn():
       for _ in range(num_runs):
         inp1, inp2 = ...
         yield inp1, inp2
     converter.build(input_fn=my_input_fn)

     # Save the TRT engine and the engines.
     converter.save(output_saved_model_dir)
     ```
  4. To use dynamic shape, we need to call the build method with an input
     function to generate profiles. This step is similar to the INT8 calibration
     step described above. The converter also needs to be created with
     use_dynamic_shape=True and one of the following profile_strategies for
     creating profiles based on the inputs produced by the input function:
     * `Range`: create one profile that works for inputs with dimension values
       in the range of [min_dims, max_dims] where min_dims and max_dims are
       derived from the provided inputs.
     * `Optimal`: create one profile for each input. The profile only works for
       inputs with the same dimensions as the input it is created for. The GPU
       engine will be run with optimal performance with such inputs.
     * `Range+Optimal`: create the profiles for both `Range` and `Optimal`.
  
      string_end: """
    function_definition: def _verify_profile_strategy(self, strategy):
    supported_strategies = [s.lower() for s in supported_profile_strategies()]
    if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
    if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
     def: def
     identifier: _verify_profile_strategy
     parameters: (self, strategy)
      (: (
      identifier: self
      ,: ,
      identifier: strategy
      ): )
     :: :
     block: supported_strategies = [s.lower() for s in supported_profile_strategies()]
    if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
    if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
      expression_statement: supported_strategies = [s.lower() for s in supported_profile_strategies()]
       assignment: supported_strategies = [s.lower() for s in supported_profile_strategies()]
        identifier: supported_strategies
        =: =
        list_comprehension: [s.lower() for s in supported_profile_strategies()]
         [: [
         call: s.lower()
          attribute: s.lower
           identifier: s
           .: .
           identifier: lower
          argument_list: ()
           (: (
           ): )
         for_in_clause: for s in supported_profile_strategies()
          for: for
          identifier: s
          in: in
          call: supported_profile_strategies()
           identifier: supported_profile_strategies
           argument_list: ()
            (: (
            ): )
         ]: ]
      if_statement: if strategy.lower() not in supported_strategies:
      raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
       if: if
       comparison_operator: strategy.lower() not in supported_strategies
        call: strategy.lower()
         attribute: strategy.lower
          identifier: strategy
          .: .
          identifier: lower
         argument_list: ()
          (: (
          ): )
        not in: not in
         not: not
         in: in
        identifier: supported_strategies
       :: :
       block: raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
        raise_statement: raise ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
         raise: raise
         call: ValueError(
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
          identifier: ValueError
          argument_list: (
          ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies()))
           (: (
           call: ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format(strategy, supported_profile_strategies())
            attribute: ("profile_strategy '{}' is not supported. It should be one of {}"
          ).format
             parenthesized_expression: ("profile_strategy '{}' is not supported. It should be one of {}"
          )
              (: (
              string: "profile_strategy '{}' is not supported. It should be one of {}"
               string_start: "
               string_content: profile_strategy '{}' is not supported. It should be one of {}
               string_end: "
              ): )
             .: .
             identifier: format
            argument_list: (strategy, supported_profile_strategies())
             (: (
             identifier: strategy
             ,: ,
             call: supported_profile_strategies()
              identifier: supported_profile_strategies
              argument_list: ()
               (: (
               ): )
             ): )
           ): )
      if_statement: if strategy == "ImplicitBatchModeCompatible":
      logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
       if: if
       comparison_operator: strategy == "ImplicitBatchModeCompatible"
        identifier: strategy
        ==: ==
        string: "ImplicitBatchModeCompatible"
         string_start: "
         string_content: ImplicitBatchModeCompatible
         string_end: "
       :: :
       block: logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
        expression_statement: logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
         call: logging.warn(
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
          attribute: logging.warn
           identifier: logging
           .: .
           identifier: warn
          argument_list: (
          "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy.")
           (: (
           concatenated_string: "ImplicitBatchModeCompatible strategy is deprecated, and"
          " using it may result in errors during engine building. Please"
          " consider using a different profile strategy."
            string: "ImplicitBatchModeCompatible strategy is deprecated, and"
             string_start: "
             string_content: ImplicitBatchModeCompatible strategy is deprecated, and
             string_end: "
            string: " using it may result in errors during engine building. Please"
             string_start: "
             string_content:  using it may result in errors during engine building. Please
             string_end: "
            string: " consider using a different profile strategy."
             string_start: "
             string_content:  consider using a different profile strategy.
             string_end: "
           ): )
    decorated_definition: @deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
  def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None):
    """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False
     decorator: @deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
      @: @
      call: deprecation.deprecated_args(None,
                               "Use individual converter parameters instead",
                               "conversion_params")
       attribute: deprecation.deprecated_args
        identifier: deprecation
        .: .
        identifier: deprecated_args
       argument_list: (None,
                               "Use individual converter parameters instead",
                               "conversion_params")
        (: (
        none: None
        ,: ,
        string: "Use individual converter parameters instead"
         string_start: "
         string_content: Use individual converter parameters instead
         string_end: "
        ,: ,
        string: "conversion_params"
         string_start: "
         string_content: conversion_params
         string_end: "
        ): )
     function_definition: def __init__(self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None):
    """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False
      def: def
      identifier: __init__
      parameters: (self,
               input_saved_model_dir=None,
               input_saved_model_tags=None,
               input_saved_model_signature_key=None,
               use_dynamic_shape=None,
               dynamic_shape_profile_strategy=None,
               max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
               precision_mode=TrtPrecisionMode.FP32,
               minimum_segment_size=3,
               maximum_cached_engines=1,
               use_calibration=True,
               allow_build_at_runtime=True,
               conversion_params=None)
       (: (
       identifier: self
       ,: ,
       default_parameter: input_saved_model_dir=None
        identifier: input_saved_model_dir
        =: =
        none: None
       ,: ,
       default_parameter: input_saved_model_tags=None
        identifier: input_saved_model_tags
        =: =
        none: None
       ,: ,
       default_parameter: input_saved_model_signature_key=None
        identifier: input_saved_model_signature_key
        =: =
        none: None
       ,: ,
       default_parameter: use_dynamic_shape=None
        identifier: use_dynamic_shape
        =: =
        none: None
       ,: ,
       default_parameter: dynamic_shape_profile_strategy=None
        identifier: dynamic_shape_profile_strategy
        =: =
        none: None
       ,: ,
       default_parameter: max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
        identifier: max_workspace_size_bytes
        =: =
        identifier: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
       ,: ,
       default_parameter: precision_mode=TrtPrecisionMode.FP32
        identifier: precision_mode
        =: =
        attribute: TrtPrecisionMode.FP32
         identifier: TrtPrecisionMode
         .: .
         identifier: FP32
       ,: ,
       default_parameter: minimum_segment_size=3
        identifier: minimum_segment_size
        =: =
        integer: 3
       ,: ,
       default_parameter: maximum_cached_engines=1
        identifier: maximum_cached_engines
        =: =
        integer: 1
       ,: ,
       default_parameter: use_calibration=True
        identifier: use_calibration
        =: =
        true: True
       ,: ,
       default_parameter: allow_build_at_runtime=True
        identifier: allow_build_at_runtime
        =: =
        true: True
       ,: ,
       default_parameter: conversion_params=None
        identifier: conversion_params
        =: =
        none: None
       ): )
      :: :
      block: """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
    assert context.executing_eagerly()
    if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)

    _check_trt_version_compatibility()
    _check_conversion_params(conversion_params, is_v2=True)

    self._conversion_params = conversion_params
    self._input_saved_model_dir = input_saved_model_dir
    self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
    self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")

    self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)

    self._calibration_input_fn = None

    self._converted = False
    self._device = None
    self._build_called_once = False
    self._calibrated = False

    if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape

    if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True

    self._profile_strategy = "Unknown"
    if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy

    # Fields to support TF-TRT testing and shouldn't be used for other purpose.
    self._test_only_disable_non_trt_optimizers = False
       expression_statement: """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
        string: """Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    """
         string_start: """
         string_content: Initialize the converter.

    Args:
      input_saved_model_dir: the directory to load the SavedModel which contains
        the input graph to transforms. Required.
      input_saved_model_tags: list of tags to load the SavedModel.
      input_saved_model_signature_key: the key of the signature to optimize the
        graph for.
      use_dynamic_shape: whether to enable dynamic shape support. None is
        equivalent to False in the current implementation.
      dynamic_shape_profile_strategy: one of the strings in
        supported_profile_strategies(). None is equivalent to Range in the
        current implementation.
      max_workspace_size_bytes: the maximum GPU temporary memory that the TRT
        engine can use at execution time. This corresponds to the
        'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
      precision_mode: one of the strings in
        TrtPrecisionMode.supported_precision_modes().
      minimum_segment_size: the minimum number of nodes required for a subgraph
        to be replaced by TRTEngineOp.
      maximum_cached_engines: max number of cached TRT engines for dynamic TRT
        ops. Created TRT engines for a dynamic dimension are cached. If the
        number of cached engines is already at max but none of them supports the
        input shapes, the TRTEngineOp will fall back to run the original TF
        subgraph that corresponds to the TRTEngineOp.
      use_calibration: this argument is ignored if precision_mode is not INT8.
        If set to True, a calibration graph will be created to calibrate the
        missing ranges. The calibration graph must be converted to an inference
        graph by running calibration with calibrate(). If set to False,
        quantization nodes will be expected for every tensor in the graph
        (excluding those which will be fused). If a range is missing, an error
        will occur. Please note that accuracy may be negatively affected if
        there is a mismatch between which tensors TRT quantizes and which
        tensors were trained with fake quantization.
      allow_build_at_runtime: whether to allow building TensorRT engines during
        runtime if no prebuilt TensorRT engine can be found that can handle the
        given inputs during runtime, then a new TensorRT engine is built at
        runtime if allow_build_at_runtime=True, and otherwise native TF is used.
      conversion_params: a TrtConversionParams instance (deprecated).

    Raises:
      ValueError: if the combination of the parameters is invalid.
    
         string_end: """
       assert_statement: assert context.executing_eagerly()
        assert: assert
        call: context.executing_eagerly()
         attribute: context.executing_eagerly
          identifier: context
          .: .
          identifier: executing_eagerly
         argument_list: ()
          (: (
          ): )
       if_statement: if conversion_params is None:
      conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)
        if: if
        comparison_operator: conversion_params is None
         identifier: conversion_params
         is: is
         none: None
        :: :
        block: conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)
         expression_statement: conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)
          assignment: conversion_params = TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)
           identifier: conversion_params
           =: =
           call: TrtConversionParams(
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)
            identifier: TrtConversionParams
            argument_list: (
          max_workspace_size_bytes=max_workspace_size_bytes,
          precision_mode=precision_mode,
          minimum_segment_size=minimum_segment_size,
          maximum_cached_engines=maximum_cached_engines,
          use_calibration=use_calibration,
          allow_build_at_runtime=allow_build_at_runtime)
             (: (
             keyword_argument: max_workspace_size_bytes=max_workspace_size_bytes
              identifier: max_workspace_size_bytes
              =: =
              identifier: max_workspace_size_bytes
             ,: ,
             keyword_argument: precision_mode=precision_mode
              identifier: precision_mode
              =: =
              identifier: precision_mode
             ,: ,
             keyword_argument: minimum_segment_size=minimum_segment_size
              identifier: minimum_segment_size
              =: =
              identifier: minimum_segment_size
             ,: ,
             keyword_argument: maximum_cached_engines=maximum_cached_engines
              identifier: maximum_cached_engines
              =: =
              identifier: maximum_cached_engines
             ,: ,
             keyword_argument: use_calibration=use_calibration
              identifier: use_calibration
              =: =
              identifier: use_calibration
             ,: ,
             keyword_argument: allow_build_at_runtime=allow_build_at_runtime
              identifier: allow_build_at_runtime
              =: =
              identifier: allow_build_at_runtime
             ): )
       expression_statement: _check_trt_version_compatibility()
        call: _check_trt_version_compatibility()
         identifier: _check_trt_version_compatibility
         argument_list: ()
          (: (
          ): )
       expression_statement: _check_conversion_params(conversion_params, is_v2=True)
        call: _check_conversion_params(conversion_params, is_v2=True)
         identifier: _check_conversion_params
         argument_list: (conversion_params, is_v2=True)
          (: (
          identifier: conversion_params
          ,: ,
          keyword_argument: is_v2=True
           identifier: is_v2
           =: =
           true: True
          ): )
       expression_statement: self._conversion_params = conversion_params
        assignment: self._conversion_params = conversion_params
         attribute: self._conversion_params
          identifier: self
          .: .
          identifier: _conversion_params
         =: =
         identifier: conversion_params
       expression_statement: self._input_saved_model_dir = input_saved_model_dir
        assignment: self._input_saved_model_dir = input_saved_model_dir
         attribute: self._input_saved_model_dir
          identifier: self
          .: .
          identifier: _input_saved_model_dir
         =: =
         identifier: input_saved_model_dir
       expression_statement: self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
        assignment: self._input_saved_model_tags = (
        input_saved_model_tags or [tag_constants.SERVING])
         attribute: self._input_saved_model_tags
          identifier: self
          .: .
          identifier: _input_saved_model_tags
         =: =
         parenthesized_expression: (
        input_saved_model_tags or [tag_constants.SERVING])
          (: (
          boolean_operator: input_saved_model_tags or [tag_constants.SERVING]
           identifier: input_saved_model_tags
           or: or
           list: [tag_constants.SERVING]
            [: [
            attribute: tag_constants.SERVING
             identifier: tag_constants
             .: .
             identifier: SERVING
            ]: ]
          ): )
       expression_statement: self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
        assignment: self._input_saved_model_signature_key = (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
         attribute: self._input_saved_model_signature_key
          identifier: self
          .: .
          identifier: _input_saved_model_signature_key
         =: =
         parenthesized_expression: (
        input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
          (: (
          boolean_operator: input_saved_model_signature_key or
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
           identifier: input_saved_model_signature_key
           or: or
           attribute: signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
            identifier: signature_constants
            .: .
            identifier: DEFAULT_SERVING_SIGNATURE_DEF_KEY
          ): )
       expression_statement: self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")
        assignment: self.freeze = not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")
         attribute: self.freeze
          identifier: self
          .: .
          identifier: freeze
         =: =
         not_operator: not trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")
          not: not
          call: trt_utils.is_experimental_feature_activated(
        "disable_graph_freezing")
           attribute: trt_utils.is_experimental_feature_activated
            identifier: trt_utils
            .: .
            identifier: is_experimental_feature_activated
           argument_list: (
        "disable_graph_freezing")
            (: (
            string: "disable_graph_freezing"
             string_start: "
             string_content: disable_graph_freezing
             string_end: "
            ): )
       expression_statement: self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)
        assignment: self._need_calibration = ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)
         attribute: self._need_calibration
          identifier: self
          .: .
          identifier: _need_calibration
         =: =
         parenthesized_expression: ((
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration)
          (: (
          boolean_operator: (
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())) and
                              conversion_params.use_calibration
           parenthesized_expression: (
        (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower()))
            (: (
            boolean_operator: (conversion_params.precision_mode == TrtPrecisionMode.INT8) or
        (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())
             parenthesized_expression: (conversion_params.precision_mode == TrtPrecisionMode.INT8)
              (: (
              comparison_operator: conversion_params.precision_mode == TrtPrecisionMode.INT8
               attribute: conversion_params.precision_mode
                identifier: conversion_params
                .: .
                identifier: precision_mode
               ==: ==
               attribute: TrtPrecisionMode.INT8
                identifier: TrtPrecisionMode
                .: .
                identifier: INT8
              ): )
             or: or
             parenthesized_expression: (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower())
              (: (
              comparison_operator: conversion_params.precision_mode == TrtPrecisionMode.INT8.lower()
               attribute: conversion_params.precision_mode
                identifier: conversion_params
                .: .
                identifier: precision_mode
               ==: ==
               call: TrtPrecisionMode.INT8.lower()
                attribute: TrtPrecisionMode.INT8.lower
                 attribute: TrtPrecisionMode.INT8
                  identifier: TrtPrecisionMode
                  .: .
                  identifier: INT8
                 .: .
                 identifier: lower
                argument_list: ()
                 (: (
                 ): )
              ): )
            ): )
           and: and
           attribute: conversion_params.use_calibration
            identifier: conversion_params
            .: .
            identifier: use_calibration
          ): )
       expression_statement: self._calibration_input_fn = None
        assignment: self._calibration_input_fn = None
         attribute: self._calibration_input_fn
          identifier: self
          .: .
          identifier: _calibration_input_fn
         =: =
         none: None
       expression_statement: self._converted = False
        assignment: self._converted = False
         attribute: self._converted
          identifier: self
          .: .
          identifier: _converted
         =: =
         false: False
       expression_statement: self._device = None
        assignment: self._device = None
         attribute: self._device
          identifier: self
          .: .
          identifier: _device
         =: =
         none: None
       expression_statement: self._build_called_once = False
        assignment: self._build_called_once = False
         attribute: self._build_called_once
          identifier: self
          .: .
          identifier: _build_called_once
         =: =
         false: False
       expression_statement: self._calibrated = False
        assignment: self._calibrated = False
         attribute: self._calibrated
          identifier: self
          .: .
          identifier: _calibrated
         =: =
         false: False
       if_statement: if use_dynamic_shape is None:
      self._use_dynamic_shape = False
    else:
      self._use_dynamic_shape = use_dynamic_shape
        if: if
        comparison_operator: use_dynamic_shape is None
         identifier: use_dynamic_shape
         is: is
         none: None
        :: :
        block: self._use_dynamic_shape = False
         expression_statement: self._use_dynamic_shape = False
          assignment: self._use_dynamic_shape = False
           attribute: self._use_dynamic_shape
            identifier: self
            .: .
            identifier: _use_dynamic_shape
           =: =
           false: False
        else_clause: else:
      self._use_dynamic_shape = use_dynamic_shape
         else: else
         :: :
         block: self._use_dynamic_shape = use_dynamic_shape
          expression_statement: self._use_dynamic_shape = use_dynamic_shape
           assignment: self._use_dynamic_shape = use_dynamic_shape
            attribute: self._use_dynamic_shape
             identifier: self
             .: .
             identifier: _use_dynamic_shape
            =: =
            identifier: use_dynamic_shape
       if_statement: if not self.freeze and not self._use_dynamic_shape:
      logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True
        if: if
        boolean_operator: not self.freeze and not self._use_dynamic_shape
         not_operator: not self.freeze
          not: not
          attribute: self.freeze
           identifier: self
           .: .
           identifier: freeze
         and: and
         not_operator: not self._use_dynamic_shape
          not: not
          attribute: self._use_dynamic_shape
           identifier: self
           .: .
           identifier: _use_dynamic_shape
        :: :
        block: logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
      self.freeze = True
         expression_statement: logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
          call: logging.warn(
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
           attribute: logging.warn
            identifier: logging
            .: .
            identifier: warn
           argument_list: (
          "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen.")
            (: (
            concatenated_string: "Disabling graph freezing is only possible in dynamic shape mode."
          " The graph will be frozen."
             string: "Disabling graph freezing is only possible in dynamic shape mode."
              string_start: "
              string_content: Disabling graph freezing is only possible in dynamic shape mode.
              string_end: "
             string: " The graph will be frozen."
              string_start: "
              string_content:  The graph will be frozen.
              string_end: "
            ): )
         expression_statement: self.freeze = True
          assignment: self.freeze = True
           attribute: self.freeze
            identifier: self
            .: .
            identifier: freeze
           =: =
           true: True
       expression_statement: self._profile_strategy = "Unknown"
        assignment: self._profile_strategy = "Unknown"
         attribute: self._profile_strategy
          identifier: self
          .: .
          identifier: _profile_strategy
         =: =
         string: "Unknown"
          string_start: "
          string_content: Unknown
          string_end: "
       if_statement: if self._use_dynamic_shape:
      if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy
        if: if
        attribute: self._use_dynamic_shape
         identifier: self
         .: .
         identifier: _use_dynamic_shape
        :: :
        block: if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy
         if_statement: if dynamic_shape_profile_strategy is None:
        self._profile_strategy = PROFILE_STRATEGY_RANGE
      else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy
          if: if
          comparison_operator: dynamic_shape_profile_strategy is None
           identifier: dynamic_shape_profile_strategy
           is: is
           none: None
          :: :
          block: self._profile_strategy = PROFILE_STRATEGY_RANGE
           expression_statement: self._profile_strategy = PROFILE_STRATEGY_RANGE
            assignment: self._profile_strategy = PROFILE_STRATEGY_RANGE
             attribute: self._profile_strategy
              identifier: self
              .: .
              identifier: _profile_strategy
             =: =
             identifier: PROFILE_STRATEGY_RANGE
          else_clause: else:
        self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy
           else: else
           :: :
           block: self._verify_profile_strategy(dynamic_shape_profile_strategy)
        self._profile_strategy = dynamic_shape_profile_strategy
            expression_statement: self._verify_profile_strategy(dynamic_shape_profile_strategy)
             call: self._verify_profile_strategy(dynamic_shape_profile_strategy)
              attribute: self._verify_profile_strategy
               identifier: self
               .: .
               identifier: _verify_profile_strategy
              argument_list: (dynamic_shape_profile_strategy)
               (: (
               identifier: dynamic_shape_profile_strategy
               ): )
            expression_statement: self._profile_strategy = dynamic_shape_profile_strategy
             assignment: self._profile_strategy = dynamic_shape_profile_strategy
              attribute: self._profile_strategy
               identifier: self
               .: .
               identifier: _profile_strategy
              =: =
              identifier: dynamic_shape_profile_strategy
       comment: # Fields to support TF-TRT testing and shouldn't be used for other purpose.
       expression_statement: self._test_only_disable_non_trt_optimizers = False
        assignment: self._test_only_disable_non_trt_optimizers = False
         attribute: self._test_only_disable_non_trt_optimizers
          identifier: self
          .: .
          identifier: _test_only_disable_non_trt_optimizers
         =: =
         false: False
    function_definition: def _need_trt_profiles(self):
    return self._use_dynamic_shape
     def: def
     identifier: _need_trt_profiles
     parameters: (self)
      (: (
      identifier: self
      ): )
     :: :
     block: return self._use_dynamic_shape
      return_statement: return self._use_dynamic_shape
       return: return
       attribute: self._use_dynamic_shape
        identifier: self
        .: .
        identifier: _use_dynamic_shape
    function_definition: def _run_conversion(self, meta_graph_def):
    """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
    grappler_session_config = config_pb2.ConfigProto()
    # Always set `allow_build_at_runtime` for offline TensorRT engine building.
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
    return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")
     def: def
     identifier: _run_conversion
     parameters: (self, meta_graph_def)
      (: (
      identifier: self
      ,: ,
      identifier: meta_graph_def
      ): )
     :: :
     block: """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
    grappler_session_config = config_pb2.ConfigProto()
    # Always set `allow_build_at_runtime` for offline TensorRT engine building.
    custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
    grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
    return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")
      expression_statement: """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
       string: """Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    """
        string_start: """
        string_content: Run Grappler's OptimizeGraph() tool to convert the graph.

    Args:
      meta_graph_def: the MetaGraphDef instance to run the optimizations on.

    Returns:
      The optimized GraphDef.
    
        string_end: """
      expression_statement: grappler_session_config = config_pb2.ConfigProto()
       assignment: grappler_session_config = config_pb2.ConfigProto()
        identifier: grappler_session_config
        =: =
        call: config_pb2.ConfigProto()
         attribute: config_pb2.ConfigProto
          identifier: config_pb2
          .: .
          identifier: ConfigProto
         argument_list: ()
          (: (
          ): )
      comment: # Always set `allow_build_at_runtime` for offline TensorRT engine building.
      expression_statement: custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
       assignment: custom_rewriter_config = _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
        identifier: custom_rewriter_config
        =: =
        call: _get_tensorrt_rewriter_config(
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
         identifier: _get_tensorrt_rewriter_config
         argument_list: (
        conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True),
        is_dynamic_op=True,
        max_batch_size=None,
        disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers,
        use_implicit_batch=not self._use_dynamic_shape,
        profile_strategy=self._profile_strategy)
          (: (
          keyword_argument: conversion_params=self._conversion_params._replace(
            allow_build_at_runtime=True)
           identifier: conversion_params
           =: =
           call: self._conversion_params._replace(
            allow_build_at_runtime=True)
            attribute: self._conversion_params._replace
             attribute: self._conversion_params
              identifier: self
              .: .
              identifier: _conversion_params
             .: .
             identifier: _replace
            argument_list: (
            allow_build_at_runtime=True)
             (: (
             keyword_argument: allow_build_at_runtime=True
              identifier: allow_build_at_runtime
              =: =
              true: True
             ): )
          ,: ,
          keyword_argument: is_dynamic_op=True
           identifier: is_dynamic_op
           =: =
           true: True
          ,: ,
          keyword_argument: max_batch_size=None
           identifier: max_batch_size
           =: =
           none: None
          ,: ,
          keyword_argument: disable_non_trt_optimizers=self._test_only_disable_non_trt_optimizers
           identifier: disable_non_trt_optimizers
           =: =
           attribute: self._test_only_disable_non_trt_optimizers
            identifier: self
            .: .
            identifier: _test_only_disable_non_trt_optimizers
          ,: ,
          keyword_argument: use_implicit_batch=not self._use_dynamic_shape
           identifier: use_implicit_batch
           =: =
           not_operator: not self._use_dynamic_shape
            not: not
            attribute: self._use_dynamic_shape
             identifier: self
             .: .
             identifier: _use_dynamic_shape
          ,: ,
          keyword_argument: profile_strategy=self._profile_strategy
           identifier: profile_strategy
           =: =
           attribute: self._profile_strategy
            identifier: self
            .: .
            identifier: _profile_strategy
          ): )
      expression_statement: grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
       call: grappler_session_config.graph_options.rewrite_options.CopyFrom(
        custom_rewriter_config)
        attribute: grappler_session_config.graph_options.rewrite_options.CopyFrom
         attribute: grappler_session_config.graph_options.rewrite_options
          attribute: grappler_session_config.graph_options
           identifier: grappler_session_config
           .: .
           identifier: graph_options
          .: .
          identifier: rewrite_options
         .: .
         identifier: CopyFrom
        argument_list: (
        custom_rewriter_config)
         (: (
         identifier: custom_rewriter_config
         ): )
      return_statement: return tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")
       return: return
       call: tf_optimizer.OptimizeGraph(
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")
        attribute: tf_optimizer.OptimizeGraph
         identifier: tf_optimizer
         .: .
         identifier: OptimizeGraph
        argument_list: (
        grappler_session_config, meta_graph_def, graph_id=b"tf_graph")
         (: (
         identifier: grappler_session_config
         ,: ,
         identifier: meta_graph_def
         ,: ,
         keyword_argument: graph_id=b"tf_graph"
          identifier: graph_id
          =: =
          string: b"tf_graph"
           string_start: b"
           string_content: tf_graph
           string_end: "
         ): )
    function_definition: def _for_each_trt_node(self, graph_def, fn):
    """Helper method to manipulate all TRTEngineOps in a GraphDef."""
    for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
    for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
     def: def
     identifier: _for_each_trt_node
     parameters: (self, graph_def, fn)
      (: (
      identifier: self
      ,: ,
      identifier: graph_def
      ,: ,
      identifier: fn
      ): )
     :: :
     block: """Helper method to manipulate all TRTEngineOps in a GraphDef."""
    for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
    for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
      expression_statement: """Helper method to manipulate all TRTEngineOps in a GraphDef."""
       string: """Helper method to manipulate all TRTEngineOps in a GraphDef."""
        string_start: """
        string_content: Helper method to manipulate all TRTEngineOps in a GraphDef.
        string_end: """
      for_statement: for node in graph_def.node:
      if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
       for: for
       identifier: node
       in: in
       attribute: graph_def.node
        identifier: graph_def
        .: .
        identifier: node
       :: :
       block: if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
        if_statement: if node.op == _TRT_ENGINE_OP_NAME:
        fn(node)
         if: if
         comparison_operator: node.op == _TRT_ENGINE_OP_NAME
          attribute: node.op
           identifier: node
           .: .
           identifier: op
          ==: ==
          identifier: _TRT_ENGINE_OP_NAME
         :: :
         block: fn(node)
          expression_statement: fn(node)
           call: fn(node)
            identifier: fn
            argument_list: (node)
             (: (
             identifier: node
             ): )
      for_statement: for func in graph_def.library.function:
      for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
       for: for
       identifier: func
       in: in
       attribute: graph_def.library.function
        attribute: graph_def.library
         identifier: graph_def
         .: .
         identifier: library
        .: .
        identifier: function
       :: :
       block: for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
        for_statement: for node in func.node_def:
        if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
         for: for
         identifier: node
         in: in
         attribute: func.node_def
          identifier: func
          .: .
          identifier: node_def
         :: :
         block: if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
          if_statement: if node.op == _TRT_ENGINE_OP_NAME:
          fn(node)
           if: if
           comparison_operator: node.op == _TRT_ENGINE_OP_NAME
            attribute: node.op
             identifier: node
             .: .
             identifier: op
            ==: ==
            identifier: _TRT_ENGINE_OP_NAME
           :: :
           block: fn(node)
            expression_statement: fn(node)
             call: fn(node)
              identifier: fn
              argument_list: (node)
               (: (
               identifier: node
               ): )
    function_definition: def _execute_calibration(self, calibration_input_fn):
    """Run INT8 calibration with the provided input generator function."""
    for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)

    self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)

    # Rebuild the function since calibration has changed the graph.
    self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
    self._calibrated = True
     def: def
     identifier: _execute_calibration
     parameters: (self, calibration_input_fn)
      (: (
      identifier: self
      ,: ,
      identifier: calibration_input_fn
      ): )
     :: :
     block: """Run INT8 calibration with the provided input generator function."""
    for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)

    self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)

    # Rebuild the function since calibration has changed the graph.
    self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
    self._calibrated = True
      expression_statement: """Run INT8 calibration with the provided input generator function."""
       string: """Run INT8 calibration with the provided input generator function."""
        string_start: """
        string_content: Run INT8 calibration with the provided input generator function.
        string_end: """
      for_statement: for inp in calibration_input_fn():
      args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)
       for: for
       identifier: inp
       in: in
       call: calibration_input_fn()
        identifier: calibration_input_fn
        argument_list: ()
         (: (
         ): )
       :: :
       block: args, kwargs = _convert_to_tensor(inp)
      self._converted_func(*args, **kwargs)
        expression_statement: args, kwargs = _convert_to_tensor(inp)
         assignment: args, kwargs = _convert_to_tensor(inp)
          pattern_list: args, kwargs
           identifier: args
           ,: ,
           identifier: kwargs
          =: =
          call: _convert_to_tensor(inp)
           identifier: _convert_to_tensor
           argument_list: (inp)
            (: (
            identifier: inp
            ): )
        expression_statement: self._converted_func(*args, **kwargs)
         call: self._converted_func(*args, **kwargs)
          attribute: self._converted_func
           identifier: self
           .: .
           identifier: _converted_func
          argument_list: (*args, **kwargs)
           (: (
           list_splat: *args
            *: *
            identifier: args
           ,: ,
           dictionary_splat: **kwargs
            **: **
            identifier: kwargs
           ): )
      expression_statement: self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)
       call: self._for_each_trt_node(self._converted_graph_def, _save_calibration_table)
        attribute: self._for_each_trt_node
         identifier: self
         .: .
         identifier: _for_each_trt_node
        argument_list: (self._converted_graph_def, _save_calibration_table)
         (: (
         attribute: self._converted_graph_def
          identifier: self
          .: .
          identifier: _converted_graph_def
         ,: ,
         identifier: _save_calibration_table
         ): )
      comment: # Rebuild the function since calibration has changed the graph.
      expression_statement: self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
       assignment: self._converted_func = _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
        attribute: self._converted_func
         identifier: self
         .: .
         identifier: _converted_func
        =: =
        call: _construct_function_from_graph_def(
        self._converted_func, self._converted_graph_def)
         identifier: _construct_function_from_graph_def
         argument_list: (
        self._converted_func, self._converted_graph_def)
          (: (
          attribute: self._converted_func
           identifier: self
           .: .
           identifier: _converted_func
          ,: ,
          attribute: self._converted_graph_def
           identifier: self
           .: .
           identifier: _converted_graph_def
          ): )
      expression_statement: self._calibrated = True
       assignment: self._calibrated = True
        attribute: self._calibrated
         identifier: self
         .: .
         identifier: _calibrated
        =: =
        true: True
    comment: # TODO(laigd): provide a utility function to optimize a ConcreteFunction and
    comment: # use it here (b/124792963).
    function_definition: def convert(self, calibration_input_fn=None):
    """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
    assert not self._converted

    # Creating an empty tensor to fetch queried device
    device_requested = array_ops.zeros([]).device

    if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")

    if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")

    if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
    if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")

    self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
    func = self._saved_model.signatures[self._input_saved_model_signature_key]
    if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
    frozen_graph_def = frozen_func.graph.as_graph_def()

    # Clear any prior device assignments
    logging.info("Clearing prior device assignments in loaded saved model")
    for node in frozen_graph_def.node:
      node.device = ""

    if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)

    # Add a collection 'train_op' so that Grappler knows the outputs.
    fetch_collection = meta_graph_pb2.CollectionDef()
    for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
    grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)

    # Run TRT optimizer in Grappler to convert the graph.
    self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
    self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)

    if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn

    self._converted = True

    graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
    if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )

    return self._converted_func
     def: def
     identifier: convert
     parameters: (self, calibration_input_fn=None)
      (: (
      identifier: self
      ,: ,
      default_parameter: calibration_input_fn=None
       identifier: calibration_input_fn
       =: =
       none: None
      ): )
     :: :
     block: """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
    assert not self._converted

    # Creating an empty tensor to fetch queried device
    device_requested = array_ops.zeros([]).device

    if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")

    if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")

    if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
    if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")

    self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
    func = self._saved_model.signatures[self._input_saved_model_signature_key]
    if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
    frozen_graph_def = frozen_func.graph.as_graph_def()

    # Clear any prior device assignments
    logging.info("Clearing prior device assignments in loaded saved model")
    for node in frozen_graph_def.node:
      node.device = ""

    if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)

    # Add a collection 'train_op' so that Grappler knows the outputs.
    fetch_collection = meta_graph_pb2.CollectionDef()
    for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
    grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)

    # Run TRT optimizer in Grappler to convert the graph.
    self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
    self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)

    if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn

    self._converted = True

    graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
    if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )

    return self._converted_func
      expression_statement: """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
       string: """Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    """
        string_start: """
        string_content: Convert the input SavedModel in 2.0 format.

    Args:
      calibration_input_fn: a generator function that yields input data as a
        list or tuple or dict, which will be used to execute the converted
        signature for calibration. All the returned input data should have the
        same shape. Example: `def input_fn(): yield input1, input2, input3`

        If dynamic_shape_mode==False, (or if the graph has static input shapes)
        then we run calibration and build the calibrated engine during
        conversion.

        If dynamic_shape_mode==True (and the graph has any unknown input
        shape), then the reference to calibration_input_fn is stored, and the
        calibration is actually performed when we build the engine (see
        build()).

    Raises:
      ValueError: if the input combination is invalid.

    Returns:
      The TF-TRT converted Function.
    
        string_end: """
      assert_statement: assert not self._converted
       assert: assert
       not_operator: not self._converted
        not: not
        attribute: self._converted
         identifier: self
         .: .
         identifier: _converted
      comment: # Creating an empty tensor to fetch queried device
      expression_statement: device_requested = array_ops.zeros([]).device
       assignment: device_requested = array_ops.zeros([]).device
        identifier: device_requested
        =: =
        attribute: array_ops.zeros([]).device
         call: array_ops.zeros([])
          attribute: array_ops.zeros
           identifier: array_ops
           .: .
           identifier: zeros
          argument_list: ([])
           (: (
           list: []
            [: [
            ]: ]
           ): )
         .: .
         identifier: device
      if_statement: if "gpu" not in device_requested.lower():
      raise ValueError(f"Specified device is not a GPU: {device_requested}")
       if: if
       comparison_operator: "gpu" not in device_requested.lower()
        string: "gpu"
         string_start: "
         string_content: gpu
         string_end: "
        not in: not in
         not: not
         in: in
        call: device_requested.lower()
         attribute: device_requested.lower
          identifier: device_requested
          .: .
          identifier: lower
         argument_list: ()
          (: (
          ): )
       :: :
       block: raise ValueError(f"Specified device is not a GPU: {device_requested}")
        raise_statement: raise ValueError(f"Specified device is not a GPU: {device_requested}")
         raise: raise
         call: ValueError(f"Specified device is not a GPU: {device_requested}")
          identifier: ValueError
          argument_list: (f"Specified device is not a GPU: {device_requested}")
           (: (
           string: f"Specified device is not a GPU: {device_requested}"
            string_start: f"
            string_content: Specified device is not a GPU: 
            interpolation: {device_requested}
             {: {
             identifier: device_requested
             }: }
            string_end: "
           ): )
      if_statement: if "gpu:0" not in device_requested.lower():
      self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")
       if: if
       comparison_operator: "gpu:0" not in device_requested.lower()
        string: "gpu:0"
         string_start: "
         string_content: gpu:0
         string_end: "
        not in: not in
         not: not
         in: in
        call: device_requested.lower()
         attribute: device_requested.lower
          identifier: device_requested
          .: .
          identifier: lower
         argument_list: ()
          (: (
          ): )
       :: :
       block: self._device = device_requested
      logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")
        expression_statement: self._device = device_requested
         assignment: self._device = device_requested
          attribute: self._device
           identifier: self
           .: .
           identifier: _device
          =: =
          identifier: device_requested
        expression_statement: logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")
         call: logging.info(f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")
          attribute: logging.info
           identifier: logging
           .: .
           identifier: info
          argument_list: (f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}")
           (: (
           concatenated_string: f"Placing imported graph from "
                   f"`{self._input_saved_model_dir}` on device: {self._device}"
            string: f"Placing imported graph from "
             string_start: f"
             string_content: Placing imported graph from 
             string_end: "
            string: f"`{self._input_saved_model_dir}` on device: {self._device}"
             string_start: f"
             string_content: `
             interpolation: {self._input_saved_model_dir}
              {: {
              attribute: self._input_saved_model_dir
               identifier: self
               .: .
               identifier: _input_saved_model_dir
              }: }
             string_content: ` on device: 
             interpolation: {self._device}
              {: {
              attribute: self._device
               identifier: self
               .: .
               identifier: _device
              }: }
             string_end: "
           ): )
      if_statement: if (self._need_calibration and not calibration_input_fn):
      raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
       if: if
       parenthesized_expression: (self._need_calibration and not calibration_input_fn)
        (: (
        boolean_operator: self._need_calibration and not calibration_input_fn
         attribute: self._need_calibration
          identifier: self
          .: .
          identifier: _need_calibration
         and: and
         not_operator: not calibration_input_fn
          not: not
          identifier: calibration_input_fn
        ): )
       :: :
       block: raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
        raise_statement: raise ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
         raise: raise
         call: ValueError("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
          identifier: ValueError
          argument_list: ("Should specify calibration_input_fn because INT8 "
                       "calibration is needed")
           (: (
           concatenated_string: "Should specify calibration_input_fn because INT8 "
                       "calibration is needed"
            string: "Should specify calibration_input_fn because INT8 "
             string_start: "
             string_content: Should specify calibration_input_fn because INT8 
             string_end: "
            string: "calibration is needed"
             string_start: "
             string_content: calibration is needed
             string_end: "
           ): )
      if_statement: if (not self._need_calibration and calibration_input_fn):
      raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")
       if: if
       parenthesized_expression: (not self._need_calibration and calibration_input_fn)
        (: (
        boolean_operator: not self._need_calibration and calibration_input_fn
         not_operator: not self._need_calibration
          not: not
          attribute: self._need_calibration
           identifier: self
           .: .
           identifier: _need_calibration
         and: and
         identifier: calibration_input_fn
        ): )
       :: :
       block: raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")
        raise_statement: raise ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")
         raise: raise
         call: ValueError("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")
          identifier: ValueError
          argument_list: ("Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed")
           (: (
           concatenated_string: "Should not specify calibration_input_fn because INT8 "
                       "calibration is not needed"
            string: "Should not specify calibration_input_fn because INT8 "
             string_start: "
             string_content: Should not specify calibration_input_fn because INT8 
             string_end: "
            string: "calibration is not needed"
             string_start: "
             string_content: calibration is not needed
             string_end: "
           ): )
      expression_statement: self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
       assignment: self._saved_model = load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
        attribute: self._saved_model
         identifier: self
         .: .
         identifier: _saved_model
        =: =
        call: load.load(self._input_saved_model_dir,
                                  self._input_saved_model_tags)
         attribute: load.load
          identifier: load
          .: .
          identifier: load
         argument_list: (self._input_saved_model_dir,
                                  self._input_saved_model_tags)
          (: (
          attribute: self._input_saved_model_dir
           identifier: self
           .: .
           identifier: _input_saved_model_dir
          ,: ,
          attribute: self._input_saved_model_tags
           identifier: self
           .: .
           identifier: _input_saved_model_tags
          ): )
      expression_statement: func = self._saved_model.signatures[self._input_saved_model_signature_key]
       assignment: func = self._saved_model.signatures[self._input_saved_model_signature_key]
        identifier: func
        =: =
        subscript: self._saved_model.signatures[self._input_saved_model_signature_key]
         attribute: self._saved_model.signatures
          attribute: self._saved_model
           identifier: self
           .: .
           identifier: _saved_model
          .: .
          identifier: signatures
         [: [
         attribute: self._input_saved_model_signature_key
          identifier: self
          .: .
          identifier: _input_saved_model_signature_key
         ]: ]
      if_statement: if self.freeze:
      frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
    else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
       if: if
       attribute: self.freeze
        identifier: self
        .: .
        identifier: freeze
       :: :
       block: frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
        expression_statement: frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
         assignment: frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)
          identifier: frozen_func
          =: =
          call: convert_to_constants.convert_variables_to_constants_v2(func)
           attribute: convert_to_constants.convert_variables_to_constants_v2
            identifier: convert_to_constants
            .: .
            identifier: convert_variables_to_constants_v2
           argument_list: (func)
            (: (
            identifier: func
            ): )
       else_clause: else:
      inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
        else: else
        :: :
        block: inlined_graph_def = _apply_inlining(func)
      _annotate_variable_ops(func, inlined_graph_def)
      frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
         expression_statement: inlined_graph_def = _apply_inlining(func)
          assignment: inlined_graph_def = _apply_inlining(func)
           identifier: inlined_graph_def
           =: =
           call: _apply_inlining(func)
            identifier: _apply_inlining
            argument_list: (func)
             (: (
             identifier: func
             ): )
         expression_statement: _annotate_variable_ops(func, inlined_graph_def)
          call: _annotate_variable_ops(func, inlined_graph_def)
           identifier: _annotate_variable_ops
           argument_list: (func, inlined_graph_def)
            (: (
            identifier: func
            ,: ,
            identifier: inlined_graph_def
            ): )
         expression_statement: frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
          assignment: frozen_func = _construct_function_from_graph_def(func, inlined_graph_def)
           identifier: frozen_func
           =: =
           call: _construct_function_from_graph_def(func, inlined_graph_def)
            identifier: _construct_function_from_graph_def
            argument_list: (func, inlined_graph_def)
             (: (
             identifier: func
             ,: ,
             identifier: inlined_graph_def
             ): )
      expression_statement: frozen_graph_def = frozen_func.graph.as_graph_def()
       assignment: frozen_graph_def = frozen_func.graph.as_graph_def()
        identifier: frozen_graph_def
        =: =
        call: frozen_func.graph.as_graph_def()
         attribute: frozen_func.graph.as_graph_def
          attribute: frozen_func.graph
           identifier: frozen_func
           .: .
           identifier: graph
          .: .
          identifier: as_graph_def
         argument_list: ()
          (: (
          ): )
      comment: # Clear any prior device assignments
      expression_statement: logging.info("Clearing prior device assignments in loaded saved model")
       call: logging.info("Clearing prior device assignments in loaded saved model")
        attribute: logging.info
         identifier: logging
         .: .
         identifier: info
        argument_list: ("Clearing prior device assignments in loaded saved model")
         (: (
         string: "Clearing prior device assignments in loaded saved model"
          string_start: "
          string_content: Clearing prior device assignments in loaded saved model
          string_end: "
         ): )
      for_statement: for node in frozen_graph_def.node:
      node.device = ""
       for: for
       identifier: node
       in: in
       attribute: frozen_graph_def.node
        identifier: frozen_graph_def
        .: .
        identifier: node
       :: :
       block: node.device = ""
        expression_statement: node.device = ""
         assignment: node.device = ""
          attribute: node.device
           identifier: node
           .: .
           identifier: device
          =: =
          string: ""
           string_start: "
           string_end: "
      if_statement: if self._device is None:
      grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
    else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
       if: if
       comparison_operator: self._device is None
        attribute: self._device
         identifier: self
         .: .
         identifier: _device
        is: is
        none: None
       :: :
       block: grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
        expression_statement: grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
         assignment: grappler_meta_graph_def = saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
          identifier: grappler_meta_graph_def
          =: =
          call: saver.export_meta_graph(
          graph_def=frozen_graph_def, graph=frozen_func.graph)
           attribute: saver.export_meta_graph
            identifier: saver
            .: .
            identifier: export_meta_graph
           argument_list: (
          graph_def=frozen_graph_def, graph=frozen_func.graph)
            (: (
            keyword_argument: graph_def=frozen_graph_def
             identifier: graph_def
             =: =
             identifier: frozen_graph_def
            ,: ,
            keyword_argument: graph=frozen_func.graph
             identifier: graph
             =: =
             attribute: frozen_func.graph
              identifier: frozen_func
              .: .
              identifier: graph
            ): )
       else_clause: else:
      with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
        else: else
        :: :
        block: with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
         with_statement: with ops.Graph().as_default() as graph, ops.device(self._device):
        importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
          with: with
          with_clause: ops.Graph().as_default() as graph, ops.device(self._device)
           with_item: ops.Graph().as_default() as graph
            as_pattern: ops.Graph().as_default() as graph
             call: ops.Graph().as_default()
              attribute: ops.Graph().as_default
               call: ops.Graph()
                attribute: ops.Graph
                 identifier: ops
                 .: .
                 identifier: Graph
                argument_list: ()
                 (: (
                 ): )
               .: .
               identifier: as_default
              argument_list: ()
               (: (
               ): )
             as: as
             as_pattern_target: graph
              identifier: graph
           ,: ,
           with_item: ops.device(self._device)
            call: ops.device(self._device)
             attribute: ops.device
              identifier: ops
              .: .
              identifier: device
             argument_list: (self._device)
              (: (
              attribute: self._device
               identifier: self
               .: .
               identifier: _device
              ): )
          :: :
          block: importer.import_graph_def(frozen_graph_def, name="")
        grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
           expression_statement: importer.import_graph_def(frozen_graph_def, name="")
            call: importer.import_graph_def(frozen_graph_def, name="")
             attribute: importer.import_graph_def
              identifier: importer
              .: .
              identifier: import_graph_def
             argument_list: (frozen_graph_def, name="")
              (: (
              identifier: frozen_graph_def
              ,: ,
              keyword_argument: name=""
               identifier: name
               =: =
               string: ""
                string_start: "
                string_end: "
              ): )
           expression_statement: grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
            assignment: grappler_meta_graph_def = saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
             identifier: grappler_meta_graph_def
             =: =
             call: saver.export_meta_graph(
            graph_def=graph.as_graph_def(), graph=graph)
              attribute: saver.export_meta_graph
               identifier: saver
               .: .
               identifier: export_meta_graph
              argument_list: (
            graph_def=graph.as_graph_def(), graph=graph)
               (: (
               keyword_argument: graph_def=graph.as_graph_def()
                identifier: graph_def
                =: =
                call: graph.as_graph_def()
                 attribute: graph.as_graph_def
                  identifier: graph
                  .: .
                  identifier: as_graph_def
                 argument_list: ()
                  (: (
                  ): )
               ,: ,
               keyword_argument: graph=graph
                identifier: graph
                =: =
                identifier: graph
               ): )
      comment: # Add a collection 'train_op' so that Grappler knows the outputs.
      expression_statement: fetch_collection = meta_graph_pb2.CollectionDef()
       assignment: fetch_collection = meta_graph_pb2.CollectionDef()
        identifier: fetch_collection
        =: =
        call: meta_graph_pb2.CollectionDef()
         attribute: meta_graph_pb2.CollectionDef
          identifier: meta_graph_pb2
          .: .
          identifier: CollectionDef
         argument_list: ()
          (: (
          ): )
      for_statement: for array in frozen_func.inputs + frozen_func.outputs:
      fetch_collection.node_list.value.append(array.name)
       for: for
       identifier: array
       in: in
       binary_operator: frozen_func.inputs + frozen_func.outputs
        attribute: frozen_func.inputs
         identifier: frozen_func
         .: .
         identifier: inputs
        +: +
        attribute: frozen_func.outputs
         identifier: frozen_func
         .: .
         identifier: outputs
       :: :
       block: fetch_collection.node_list.value.append(array.name)
        expression_statement: fetch_collection.node_list.value.append(array.name)
         call: fetch_collection.node_list.value.append(array.name)
          attribute: fetch_collection.node_list.value.append
           attribute: fetch_collection.node_list.value
            attribute: fetch_collection.node_list
             identifier: fetch_collection
             .: .
             identifier: node_list
            .: .
            identifier: value
           .: .
           identifier: append
          argument_list: (array.name)
           (: (
           attribute: array.name
            identifier: array
            .: .
            identifier: name
           ): )
      expression_statement: grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)
       call: grappler_meta_graph_def.collection_def["train_op"].CopyFrom(
        fetch_collection)
        attribute: grappler_meta_graph_def.collection_def["train_op"].CopyFrom
         subscript: grappler_meta_graph_def.collection_def["train_op"]
          attribute: grappler_meta_graph_def.collection_def
           identifier: grappler_meta_graph_def
           .: .
           identifier: collection_def
          [: [
          string: "train_op"
           string_start: "
           string_content: train_op
           string_end: "
          ]: ]
         .: .
         identifier: CopyFrom
        argument_list: (
        fetch_collection)
         (: (
         identifier: fetch_collection
         ): )
      comment: # Run TRT optimizer in Grappler to convert the graph.
      expression_statement: self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
       assignment: self._converted_graph_def = self._run_conversion(grappler_meta_graph_def)
        attribute: self._converted_graph_def
         identifier: self
         .: .
         identifier: _converted_graph_def
        =: =
        call: self._run_conversion(grappler_meta_graph_def)
         attribute: self._run_conversion
          identifier: self
          .: .
          identifier: _run_conversion
         argument_list: (grappler_meta_graph_def)
          (: (
          identifier: grappler_meta_graph_def
          ): )
      expression_statement: self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)
       assignment: self._converted_func = _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)
        attribute: self._converted_func
         identifier: self
         .: .
         identifier: _converted_func
        =: =
        call: _construct_function_from_graph_def(
        func, self._converted_graph_def, frozen_func)
         identifier: _construct_function_from_graph_def
         argument_list: (
        func, self._converted_graph_def, frozen_func)
          (: (
          identifier: func
          ,: ,
          attribute: self._converted_graph_def
           identifier: self
           .: .
           identifier: _converted_graph_def
          ,: ,
          identifier: frozen_func
          ): )
      if_statement: if self._need_calibration:
      # Execute calibration here only if not in dynamic shape mode.
      if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn
       if: if
       attribute: self._need_calibration
        identifier: self
        .: .
        identifier: _need_calibration
       :: :
       comment: # Execute calibration here only if not in dynamic shape mode.
       block: if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn
        if_statement: if not self._need_trt_profiles():
        self._execute_calibration(calibration_input_fn)
      else:
        self._calibration_input_fn = calibration_input_fn
         if: if
         not_operator: not self._need_trt_profiles()
          not: not
          call: self._need_trt_profiles()
           attribute: self._need_trt_profiles
            identifier: self
            .: .
            identifier: _need_trt_profiles
           argument_list: ()
            (: (
            ): )
         :: :
         block: self._execute_calibration(calibration_input_fn)
          expression_statement: self._execute_calibration(calibration_input_fn)
           call: self._execute_calibration(calibration_input_fn)
            attribute: self._execute_calibration
             identifier: self
             .: .
             identifier: _execute_calibration
            argument_list: (calibration_input_fn)
             (: (
             identifier: calibration_input_fn
             ): )
         else_clause: else:
        self._calibration_input_fn = calibration_input_fn
          else: else
          :: :
          block: self._calibration_input_fn = calibration_input_fn
           expression_statement: self._calibration_input_fn = calibration_input_fn
            assignment: self._calibration_input_fn = calibration_input_fn
             attribute: self._calibration_input_fn
              identifier: self
              .: .
              identifier: _calibration_input_fn
             =: =
             identifier: calibration_input_fn
      expression_statement: self._converted = True
       assignment: self._converted = True
        attribute: self._converted
         identifier: self
         .: .
         identifier: _converted
        =: =
        true: True
      expression_statement: graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
       assignment: graphviz_path = os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
        identifier: graphviz_path
        =: =
        call: os.environ.get("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
         attribute: os.environ.get
          attribute: os.environ
           identifier: os
           .: .
           identifier: environ
          .: .
          identifier: get
         argument_list: ("TF_TRT_EXPORT_GRAPH_VIZ_PATH", default=None)
          (: (
          string: "TF_TRT_EXPORT_GRAPH_VIZ_PATH"
           string_start: "
           string_content: TF_TRT_EXPORT_GRAPH_VIZ_PATH
           string_end: "
          ,: ,
          keyword_argument: default=None
           identifier: default
           =: =
           none: None
          ): )
      if_statement: if graphviz_path is not None:
      try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
       if: if
       comparison_operator: graphviz_path is not None
        identifier: graphviz_path
        is not: is not
         is: is
         not: not
        none: None
       :: :
       block: try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
        try_statement: try:
        trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
      except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
         try: try
         :: :
         block: trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
          expression_statement: trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
           call: trt_utils.draw_graphdef_as_graphviz(
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
            attribute: trt_utils.draw_graphdef_as_graphviz
             identifier: trt_utils
             .: .
             identifier: draw_graphdef_as_graphviz
            argument_list: (
            graphdef=self._converted_func.graph.as_graph_def(add_shapes=True),
            dot_output_filename=graphviz_path)
             (: (
             keyword_argument: graphdef=self._converted_func.graph.as_graph_def(add_shapes=True)
              identifier: graphdef
              =: =
              call: self._converted_func.graph.as_graph_def(add_shapes=True)
               attribute: self._converted_func.graph.as_graph_def
                attribute: self._converted_func.graph
                 attribute: self._converted_func
                  identifier: self
                  .: .
                  identifier: _converted_func
                 .: .
                 identifier: graph
                .: .
                identifier: as_graph_def
               argument_list: (add_shapes=True)
                (: (
                keyword_argument: add_shapes=True
                 identifier: add_shapes
                 =: =
                 true: True
                ): )
             ,: ,
             keyword_argument: dot_output_filename=graphviz_path
              identifier: dot_output_filename
              =: =
              identifier: graphviz_path
             ): )
         except_clause: except Exception as e:
        logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
          except: except
          as_pattern: Exception as e
           identifier: Exception
           as: as
           as_pattern_target: e
            identifier: e
          :: :
          block: logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
           expression_statement: logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
            call: logging.error(
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
             attribute: logging.error
              identifier: logging
              .: .
              identifier: error
             argument_list: (
            "An Exception occurred during the export of the graph "
            f"visualization: {e}"
        )
              (: (
              concatenated_string: "An Exception occurred during the export of the graph "
            f"visualization: {e}"
               string: "An Exception occurred during the export of the graph "
                string_start: "
                string_content: An Exception occurred during the export of the graph 
                string_end: "
               string: f"visualization: {e}"
                string_start: f"
                string_content: visualization: 
                interpolation: {e}
                 {: {
                 identifier: e
                 }: }
                string_end: "
              ): )
      return_statement: return self._converted_func
       return: return
       attribute: self._converted_func
        identifier: self
        .: .
        identifier: _converted_func
    function_definition: def build(self, input_fn):
    """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
    if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
    if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
    if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
    if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")

    def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value

    if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func

    first_input = None
    # Run inference:
    #   Builds TRT engines if self._need_trt_profiles is False.
    #   Builds TRT optimization profiles if self._need_trt_profiles is True.
    for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)

    if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))

    # Run calibration if required, this would have been skipped in
    # the convert step
    if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)

    self._build_called_once = True
     def: def
     identifier: build
     parameters: (self, input_fn)
      (: (
      identifier: self
      ,: ,
      identifier: input_fn
      ): )
     :: :
     block: """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
    if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
    if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
    if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
    if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")

    def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value

    if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func

    first_input = None
    # Run inference:
    #   Builds TRT engines if self._need_trt_profiles is False.
    #   Builds TRT optimization profiles if self._need_trt_profiles is True.
    for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)

    if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))

    # Run calibration if required, this would have been skipped in
    # the convert step
    if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)

    self._build_called_once = True
      expression_statement: """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
       string: """Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    """
        string_start: """
        string_content: Run inference with converted graph in order to build TensorRT engines.

    If the conversion requires INT8 calibration, then a reference to the
    calibration function was stored during the call to convert(). Calibration
    will be performed while we build the TensorRT engines.

    Args:
      input_fn: a generator function that provides the input data as a single
        array, OR a list or tuple of the arrays OR a dict, which will be used
        to execute the converted signature to generate TRT engines.
        Example 1:
        `def input_fn():
             # Let's assume a network with 1 input tensor.
             # We generate 2 sets of dummy input data:
             input_shapes = [(1, 16),    # 1st shape
                             (2, 32)]    # 2nd shape
             for shapes in input_shapes:
                 # return an input tensor
                 yield np.zeros(shape).astype(np.float32)'

        Example 2:
        `def input_fn():
             # Let's assume a network with 2 input tensors.
             # We generate 3 sets of dummy input data:
             input_shapes = [[(1, 16), (2, 16)], # 1st input list
                             [(2, 32), (4, 32)], # 2nd list of two tensors
                             [(4, 32), (8, 32)]] # 3rd input list
             for shapes in input_shapes:
                 # return a list of input tensors
                 yield [np.zeros(x).astype(np.float32) for x in shapes]`

    Raises:
      NotImplementedError: build() is already called.
      RuntimeError: the input_fx is None.
    
        string_end: """
      if_statement: if self._build_called_once:
      raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
       if: if
       attribute: self._build_called_once
        identifier: self
        .: .
        identifier: _build_called_once
       :: :
       block: raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
        raise_statement: raise NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
         raise: raise
         call: NotImplementedError("build() is already called. It is not "
                                "supported to call build() more than once.")
          identifier: NotImplementedError
          argument_list: ("build() is already called. It is not "
                                "supported to call build() more than once.")
           (: (
           concatenated_string: "build() is already called. It is not "
                                "supported to call build() more than once."
            string: "build() is already called. It is not "
             string_start: "
             string_content: build() is already called. It is not 
             string_end: "
            string: "supported to call build() more than once."
             string_start: "
             string_content: supported to call build() more than once.
             string_end: "
           ): )
      if_statement: if not input_fn:
      raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
       if: if
       not_operator: not input_fn
        not: not
        identifier: input_fn
       :: :
       block: raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
        raise_statement: raise RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
         raise: raise
         call: RuntimeError("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
          identifier: RuntimeError
          argument_list: ("input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines")
           (: (
           concatenated_string: "input_fn is None. Method build() needs input_fn "
                         "to be specified in order to build TensorRT engines"
            string: "input_fn is None. Method build() needs input_fn "
             string_start: "
             string_content: input_fn is None. Method build() needs input_fn 
             string_end: "
            string: "to be specified in order to build TensorRT engines"
             string_start: "
             string_content: to be specified in order to build TensorRT engines
             string_end: "
           ): )
      if_statement: if not self._converted:
      raise RuntimeError("Need to call convert() before build()")
       if: if
       not_operator: not self._converted
        not: not
        attribute: self._converted
         identifier: self
         .: .
         identifier: _converted
       :: :
       block: raise RuntimeError("Need to call convert() before build()")
        raise_statement: raise RuntimeError("Need to call convert() before build()")
         raise: raise
         call: RuntimeError("Need to call convert() before build()")
          identifier: RuntimeError
          argument_list: ("Need to call convert() before build()")
           (: (
           string: "Need to call convert() before build()"
            string_start: "
            string_content: Need to call convert() before build()
            string_end: "
           ): )
      if_statement: if (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None):
      raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")
       if: if
       parenthesized_expression: (self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None)
        (: (
        boolean_operator: self._need_calibration and not self._calibrated and
        self._calibration_input_fn is None
         boolean_operator: self._need_calibration and not self._calibrated
          attribute: self._need_calibration
           identifier: self
           .: .
           identifier: _need_calibration
          and: and
          not_operator: not self._calibrated
           not: not
           attribute: self._calibrated
            identifier: self
            .: .
            identifier: _calibrated
         and: and
         comparison_operator: self._calibration_input_fn is None
          attribute: self._calibration_input_fn
           identifier: self
           .: .
           identifier: _calibration_input_fn
          is: is
          none: None
        ): )
       :: :
       block: raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")
        raise_statement: raise RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")
         raise: raise
         call: RuntimeError("Need to provide the calibration_input_fn arg while "
                         "calling convert().")
          identifier: RuntimeError
          argument_list: ("Need to provide the calibration_input_fn arg while "
                         "calling convert().")
           (: (
           concatenated_string: "Need to provide the calibration_input_fn arg while "
                         "calling convert()."
            string: "Need to provide the calibration_input_fn arg while "
             string_start: "
             string_content: Need to provide the calibration_input_fn arg while 
             string_end: "
            string: "calling convert()."
             string_start: "
             string_content: calling convert().
             string_end: "
           ): )
      function_definition: def _set_profile_generation_mode(value, node):
      node.attr["_profile_generation_mode"].b = value
       def: def
       identifier: _set_profile_generation_mode
       parameters: (value, node)
        (: (
        identifier: value
        ,: ,
        identifier: node
        ): )
       :: :
       block: node.attr["_profile_generation_mode"].b = value
        expression_statement: node.attr["_profile_generation_mode"].b = value
         assignment: node.attr["_profile_generation_mode"].b = value
          attribute: node.attr["_profile_generation_mode"].b
           subscript: node.attr["_profile_generation_mode"]
            attribute: node.attr
             identifier: node
             .: .
             identifier: attr
            [: [
            string: "_profile_generation_mode"
             string_start: "
             string_content: _profile_generation_mode
             string_end: "
            ]: ]
           .: .
           identifier: b
          =: =
          identifier: value
      if_statement: if self._need_trt_profiles():
      # Enable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
    else:
      func = self._converted_func
       if: if
       call: self._need_trt_profiles()
        attribute: self._need_trt_profiles
         identifier: self
         .: .
         identifier: _need_trt_profiles
        argument_list: ()
         (: (
         ): )
       :: :
       comment: # Enable profile generation.
       block: self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
      # Profile generation is enabled using the _profile_generation_mode
      # attribute of the TRTEngineOps. We need to rebuild the function to
      # change this attribute.
      func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
        expression_statement: self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
         call: self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
          attribute: self._for_each_trt_node
           identifier: self
           .: .
           identifier: _for_each_trt_node
          argument_list: (self._converted_graph_def,
                              partial(_set_profile_generation_mode, True))
           (: (
           attribute: self._converted_graph_def
            identifier: self
            .: .
            identifier: _converted_graph_def
           ,: ,
           call: partial(_set_profile_generation_mode, True)
            identifier: partial
            argument_list: (_set_profile_generation_mode, True)
             (: (
             identifier: _set_profile_generation_mode
             ,: ,
             true: True
             ): )
           ): )
        comment: # Profile generation is enabled using the _profile_generation_mode
        comment: # attribute of the TRTEngineOps. We need to rebuild the function to
        comment: # change this attribute.
        expression_statement: func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
         assignment: func = _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
          identifier: func
          =: =
          call: _construct_function_from_graph_def(self._converted_func,
                                                self._converted_graph_def)
           identifier: _construct_function_from_graph_def
           argument_list: (self._converted_func,
                                                self._converted_graph_def)
            (: (
            attribute: self._converted_func
             identifier: self
             .: .
             identifier: _converted_func
            ,: ,
            attribute: self._converted_graph_def
             identifier: self
             .: .
             identifier: _converted_graph_def
            ): )
       else_clause: else:
      func = self._converted_func
        else: else
        :: :
        block: func = self._converted_func
         expression_statement: func = self._converted_func
          assignment: func = self._converted_func
           identifier: func
           =: =
           attribute: self._converted_func
            identifier: self
            .: .
            identifier: _converted_func
      expression_statement: first_input = None
       assignment: first_input = None
        identifier: first_input
        =: =
        none: None
      comment: # Run inference:
      comment: #   Builds TRT engines if self._need_trt_profiles is False.
      comment: #   Builds TRT optimization profiles if self._need_trt_profiles is True.
      for_statement: for inp in input_fn():
      if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)
       for: for
       identifier: inp
       in: in
       call: input_fn()
        identifier: input_fn
        argument_list: ()
         (: (
         ): )
       :: :
       block: if first_input is None:
        first_input = inp
      args, kwargs = _convert_to_tensor(inp)
      func(*args, **kwargs)
        if_statement: if first_input is None:
        first_input = inp
         if: if
         comparison_operator: first_input is None
          identifier: first_input
          is: is
          none: None
         :: :
         block: first_input = inp
          expression_statement: first_input = inp
           assignment: first_input = inp
            identifier: first_input
            =: =
            identifier: inp
        expression_statement: args, kwargs = _convert_to_tensor(inp)
         assignment: args, kwargs = _convert_to_tensor(inp)
          pattern_list: args, kwargs
           identifier: args
           ,: ,
           identifier: kwargs
          =: =
          call: _convert_to_tensor(inp)
           identifier: _convert_to_tensor
           argument_list: (inp)
            (: (
            identifier: inp
            ): )
        expression_statement: func(*args, **kwargs)
         call: func(*args, **kwargs)
          identifier: func
          argument_list: (*args, **kwargs)
           (: (
           list_splat: *args
            *: *
            identifier: args
           ,: ,
           dictionary_splat: **kwargs
            **: **
            identifier: kwargs
           ): )
      if_statement: if self._need_trt_profiles():
      # Disable profile generation.
      self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))
       if: if
       call: self._need_trt_profiles()
        attribute: self._need_trt_profiles
         identifier: self
         .: .
         identifier: _need_trt_profiles
        argument_list: ()
         (: (
         ): )
       :: :
       comment: # Disable profile generation.
       block: self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))
        expression_statement: self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))
         call: self._for_each_trt_node(self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))
          attribute: self._for_each_trt_node
           identifier: self
           .: .
           identifier: _for_each_trt_node
          argument_list: (self._converted_graph_def,
                              partial(_set_profile_generation_mode, False))
           (: (
           attribute: self._converted_graph_def
            identifier: self
            .: .
            identifier: _converted_graph_def
           ,: ,
           call: partial(_set_profile_generation_mode, False)
            identifier: partial
            argument_list: (_set_profile_generation_mode, False)
             (: (
             identifier: _set_profile_generation_mode
             ,: ,
             false: False
             ): )
           ): )
      comment: # Run calibration if required, this would have been skipped in
      comment: # the convert step
      if_statement: if self._need_calibration and not self._calibrated:
      self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
    else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)
       if: if
       boolean_operator: self._need_calibration and not self._calibrated
        attribute: self._need_calibration
         identifier: self
         .: .
         identifier: _need_calibration
        and: and
        not_operator: not self._calibrated
         not: not
         attribute: self._calibrated
          identifier: self
          .: .
          identifier: _calibrated
       :: :
       block: self._execute_calibration(self._calibration_input_fn)
      # calibration also builds the engine
        expression_statement: self._execute_calibration(self._calibration_input_fn)
         call: self._execute_calibration(self._calibration_input_fn)
          attribute: self._execute_calibration
           identifier: self
           .: .
           identifier: _execute_calibration
          argument_list: (self._calibration_input_fn)
           (: (
           attribute: self._calibration_input_fn
            identifier: self
            .: .
            identifier: _calibration_input_fn
           ): )
        comment: # calibration also builds the engine
       else_clause: else:
      # Use the first input in explicit batch mode to build TensorRT engines
      # after generating all the profiles. The first input is used but any of
      # the inputs can be used because the shape of this input does not
      # determine the engine and instead the shapes collected in profiles
      # determine the engine.
      args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)
        else: else
        :: :
        comment: # Use the first input in explicit batch mode to build TensorRT engines
        comment: # after generating all the profiles. The first input is used but any of
        comment: # the inputs can be used because the shape of this input does not
        comment: # determine the engine and instead the shapes collected in profiles
        comment: # determine the engine.
        block: args, kwargs = _convert_to_tensor(first_input)
      self._converted_func(*args, **kwargs)
         expression_statement: args, kwargs = _convert_to_tensor(first_input)
          assignment: args, kwargs = _convert_to_tensor(first_input)
           pattern_list: args, kwargs
            identifier: args
            ,: ,
            identifier: kwargs
           =: =
           call: _convert_to_tensor(first_input)
            identifier: _convert_to_tensor
            argument_list: (first_input)
             (: (
             identifier: first_input
             ): )
         expression_statement: self._converted_func(*args, **kwargs)
          call: self._converted_func(*args, **kwargs)
           attribute: self._converted_func
            identifier: self
            .: .
            identifier: _converted_func
           argument_list: (*args, **kwargs)
            (: (
            list_splat: *args
             *: *
             identifier: args
            ,: ,
            dictionary_splat: **kwargs
             **: **
             identifier: kwargs
            ): )
      expression_statement: self._build_called_once = True
       assignment: self._build_called_once = True
        attribute: self._build_called_once
         identifier: self
         .: .
         identifier: _build_called_once
        =: =
        true: True
    function_definition: def save(self,
           output_saved_model_dir,
           save_gpu_specific_engines=True,
           options=None):
    """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
    assert self._converted

    # 'remove_native_segments': setting this value to True removes native segments
    # associated with each TRT engine. This option can be used to reduce the size
    # of the converted model. Please note that a converted model without native
    # segments can't be used for collecting profiles, building or re-converting.
    # The reduced model can only be used for inference when no native segments
    # are required for computation. When remove_native_segments flag is set to
    # True, the converted_graph_def needs to be reduced before saved_model
    # function serialization.
    if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()

    if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
    # Serialize the TRT engines in the cache if any, and create trackable
    # resource to track them.
    engine_asset_dir = tempfile.mkdtemp()
    resource_map = {}

    def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)

    self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
    # If the graph is frozen, tracked variables are not needed by the converted model.
    trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
    trackable.trt_engine_resources = resource_map

    # Set allow_build_at_runtime=False if asked by user.
    #
    # This attribute is set here because build() needs it to be True in order to
    # build engines.
    if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func

    # Rewrite the signature map using the optimized ConcreteFunction.
    signatures = {self._input_saved_model_signature_key: self._converted_func}
    save.save(trackable, output_saved_model_dir, signatures, options=options)
     def: def
     identifier: save
     parameters: (self,
           output_saved_model_dir,
           save_gpu_specific_engines=True,
           options=None)
      (: (
      identifier: self
      ,: ,
      identifier: output_saved_model_dir
      ,: ,
      default_parameter: save_gpu_specific_engines=True
       identifier: save_gpu_specific_engines
       =: =
       true: True
      ,: ,
      default_parameter: options=None
       identifier: options
       =: =
       none: None
      ): )
     :: :
     block: """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
    assert self._converted

    # 'remove_native_segments': setting this value to True removes native segments
    # associated with each TRT engine. This option can be used to reduce the size
    # of the converted model. Please note that a converted model without native
    # segments can't be used for collecting profiles, building or re-converting.
    # The reduced model can only be used for inference when no native segments
    # are required for computation. When remove_native_segments flag is set to
    # True, the converted_graph_def needs to be reduced before saved_model
    # function serialization.
    if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()

    if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
    # Serialize the TRT engines in the cache if any, and create trackable
    # resource to track them.
    engine_asset_dir = tempfile.mkdtemp()
    resource_map = {}

    def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)

    self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
    # If the graph is frozen, tracked variables are not needed by the converted model.
    trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
    trackable.trt_engine_resources = resource_map

    # Set allow_build_at_runtime=False if asked by user.
    #
    # This attribute is set here because build() needs it to be True in order to
    # build engines.
    if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func

    # Rewrite the signature map using the optimized ConcreteFunction.
    signatures = {self._input_saved_model_signature_key: self._converted_func}
    save.save(trackable, output_saved_model_dir, signatures, options=options)
      expression_statement: """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
       string: """Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    """
        string_start: """
        string_content: Save the converted SavedModel.

    Args:
      output_saved_model_dir: directory to saved the converted SavedModel.
      save_gpu_specific_engines: whether to save TRT engines that have been
        built. When True, all engines are saved and when False, the engines
        are not saved and will be rebuilt at inference time. By using
        save_gpu_specific_engines=False after doing INT8 calibration, inference
        can be done on different GPUs than the GPU that the model was calibrated
        and saved on.
      options: `tf.saved_model.SaveOptions` object for configuring save options.
    Raises:
      RuntimeError: if the needed calibration hasn't been done.
    
        string_end: """
      assert_statement: assert self._converted
       assert: assert
       attribute: self._converted
        identifier: self
        .: .
        identifier: _converted
      comment: # 'remove_native_segments': setting this value to True removes native segments
      comment: # associated with each TRT engine. This option can be used to reduce the size
      comment: # of the converted model. Please note that a converted model without native
      comment: # segments can't be used for collecting profiles, building or re-converting.
      comment: # The reduced model can only be used for inference when no native segments
      comment: # are required for computation. When remove_native_segments flag is set to
      comment: # True, the converted_graph_def needs to be reduced before saved_model
      comment: # function serialization.
      if_statement: if trt_utils.is_experimental_feature_activated("remove_native_segments"):
      logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()
       if: if
       call: trt_utils.is_experimental_feature_activated("remove_native_segments")
        attribute: trt_utils.is_experimental_feature_activated
         identifier: trt_utils
         .: .
         identifier: is_experimental_feature_activated
        argument_list: ("remove_native_segments")
         (: (
         string: "remove_native_segments"
          string_start: "
          string_content: remove_native_segments
          string_end: "
         ): )
       :: :
       block: logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
      self._converted_func = _remove_native_segments(self._converted_func)
      self._converted_graph_def = self._converted_func.graph.as_graph_def()
        expression_statement: logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
         call: logging.info(
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
          attribute: logging.info
           identifier: logging
           .: .
           identifier: info
          argument_list: (
          "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
      )
           (: (
           concatenated_string: "'remove_native_segments' experimental feature is enabled"
          " during saving of converted SavedModel."
            string: "'remove_native_segments' experimental feature is enabled"
             string_start: "
             string_content: 'remove_native_segments' experimental feature is enabled
             string_end: "
            string: " during saving of converted SavedModel."
             string_start: "
             string_content:  during saving of converted SavedModel.
             string_end: "
           ): )
        expression_statement: self._converted_func = _remove_native_segments(self._converted_func)
         assignment: self._converted_func = _remove_native_segments(self._converted_func)
          attribute: self._converted_func
           identifier: self
           .: .
           identifier: _converted_func
          =: =
          call: _remove_native_segments(self._converted_func)
           identifier: _remove_native_segments
           argument_list: (self._converted_func)
            (: (
            attribute: self._converted_func
             identifier: self
             .: .
             identifier: _converted_func
            ): )
        expression_statement: self._converted_graph_def = self._converted_func.graph.as_graph_def()
         assignment: self._converted_graph_def = self._converted_func.graph.as_graph_def()
          attribute: self._converted_graph_def
           identifier: self
           .: .
           identifier: _converted_graph_def
          =: =
          call: self._converted_func.graph.as_graph_def()
           attribute: self._converted_func.graph.as_graph_def
            attribute: self._converted_func.graph
             attribute: self._converted_func
              identifier: self
              .: .
              identifier: _converted_func
             .: .
             identifier: graph
            .: .
            identifier: as_graph_def
           argument_list: ()
            (: (
            ): )
      if_statement: if self._need_calibration and not self._calibrated:
      raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
       if: if
       boolean_operator: self._need_calibration and not self._calibrated
        attribute: self._need_calibration
         identifier: self
         .: .
         identifier: _need_calibration
        and: and
        not_operator: not self._calibrated
         not: not
         attribute: self._calibrated
          identifier: self
          .: .
          identifier: _calibrated
       :: :
       block: raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
        raise_statement: raise RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
         raise: raise
         call: RuntimeError("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
          identifier: RuntimeError
          argument_list: ("A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines.")
           (: (
           concatenated_string: "A model that requires INT8 calibration has to be "
                         "built before saving it. Call build() to build and "
                         "calibrate the TensorRT engines."
            string: "A model that requires INT8 calibration has to be "
             string_start: "
             string_content: A model that requires INT8 calibration has to be 
             string_end: "
            string: "built before saving it. Call build() to build and "
             string_start: "
             string_content: built before saving it. Call build() to build and 
             string_end: "
            string: "calibrate the TensorRT engines."
             string_start: "
             string_content: calibrate the TensorRT engines.
             string_end: "
           ): )
      comment: # Serialize the TRT engines in the cache if any, and create trackable
      comment: # resource to track them.
      expression_statement: engine_asset_dir = tempfile.mkdtemp()
       assignment: engine_asset_dir = tempfile.mkdtemp()
        identifier: engine_asset_dir
        =: =
        call: tempfile.mkdtemp()
         attribute: tempfile.mkdtemp
          identifier: tempfile
          .: .
          identifier: mkdtemp
         argument_list: ()
          (: (
          ): )
      expression_statement: resource_map = {}
       assignment: resource_map = {}
        identifier: resource_map
        =: =
        dictionary: {}
         {: {
         }: }
      function_definition: def _serialize_and_track_engine(node):
      """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)
       def: def
       identifier: _serialize_and_track_engine
       parameters: (node)
        (: (
        identifier: node
        ): )
       :: :
       block: """Serialize TRT engines in the cache and track them."""
      # Don't dump the same cache twice.
      canonical_engine_name = _get_canonical_engine_name(node.name)
      if canonical_engine_name in resource_map:
        return

      filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)

      try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return

      # TODO(laigd): add an option for the user to choose the device.
      resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)
        expression_statement: """Serialize TRT engines in the cache and track them."""
         string: """Serialize TRT engines in the cache and track them."""
          string_start: """
          string_content: Serialize TRT engines in the cache and track them.
          string_end: """
        comment: # Don't dump the same cache twice.
        expression_statement: canonical_engine_name = _get_canonical_engine_name(node.name)
         assignment: canonical_engine_name = _get_canonical_engine_name(node.name)
          identifier: canonical_engine_name
          =: =
          call: _get_canonical_engine_name(node.name)
           identifier: _get_canonical_engine_name
           argument_list: (node.name)
            (: (
            attribute: node.name
             identifier: node
             .: .
             identifier: name
            ): )
        if_statement: if canonical_engine_name in resource_map:
        return
         if: if
         comparison_operator: canonical_engine_name in resource_map
          identifier: canonical_engine_name
          in: in
          identifier: resource_map
         :: :
         block: return
          return_statement: return
           return: return
        expression_statement: filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)
         assignment: filename = os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)
          identifier: filename
          =: =
          call: os.path.join(engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)
           attribute: os.path.join
            attribute: os.path
             identifier: os
             .: .
             identifier: path
            .: .
            identifier: join
           argument_list: (engine_asset_dir,
                              "trt-serialized-engine." + canonical_engine_name)
            (: (
            identifier: engine_asset_dir
            ,: ,
            binary_operator: "trt-serialized-engine." + canonical_engine_name
             string: "trt-serialized-engine."
              string_start: "
              string_content: trt-serialized-engine.
              string_end: "
             +: +
             identifier: canonical_engine_name
            ): )
        try_statement: try:
        gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
      except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return
         try: try
         :: :
         block: gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
          expression_statement: gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
           call: gen_trt_ops.serialize_trt_resource(
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
            attribute: gen_trt_ops.serialize_trt_resource
             identifier: gen_trt_ops
             .: .
             identifier: serialize_trt_resource
            argument_list: (
            resource_name=canonical_engine_name,
            filename=filename,
            delete_resource=True,
            save_gpu_specific_engines=save_gpu_specific_engines)
             (: (
             keyword_argument: resource_name=canonical_engine_name
              identifier: resource_name
              =: =
              identifier: canonical_engine_name
             ,: ,
             keyword_argument: filename=filename
              identifier: filename
              =: =
              identifier: filename
             ,: ,
             keyword_argument: delete_resource=True
              identifier: delete_resource
              =: =
              true: True
             ,: ,
             keyword_argument: save_gpu_specific_engines=save_gpu_specific_engines
              identifier: save_gpu_specific_engines
              =: =
              identifier: save_gpu_specific_engines
             ): )
         except_clause: except errors.NotFoundError:
        logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return
          except: except
          attribute: errors.NotFoundError
           identifier: errors
           .: .
           identifier: NotFoundError
          :: :
          block: logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
        return
           expression_statement: logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
            call: logging.info(
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
             attribute: logging.info
              identifier: logging
              .: .
              identifier: info
             argument_list: (
            "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime.", canonical_engine_name)
              (: (
              concatenated_string: "Could not find %s in TF-TRT cache. "
            "This can happen if build() is not called, "
            "which means TensorRT engines will be built "
            "and cached at runtime."
               string: "Could not find %s in TF-TRT cache. "
                string_start: "
                string_content: Could not find %s in TF-TRT cache. 
                string_end: "
               string: "This can happen if build() is not called, "
                string_start: "
                string_content: This can happen if build() is not called, 
                string_end: "
               string: "which means TensorRT engines will be built "
                string_start: "
                string_content: which means TensorRT engines will be built 
                string_end: "
               string: "and cached at runtime."
                string_start: "
                string_content: and cached at runtime.
                string_end: "
              ,: ,
              identifier: canonical_engine_name
              ): )
           return_statement: return
            return: return
        comment: # TODO(laigd): add an option for the user to choose the device.
        expression_statement: resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)
         assignment: resource_map[canonical_engine_name] = _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)
          subscript: resource_map[canonical_engine_name]
           identifier: resource_map
           [: [
           identifier: canonical_engine_name
           ]: ]
          =: =
          call: _TRTEngineResource(
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)
           identifier: _TRTEngineResource
           argument_list: (
          canonical_engine_name, filename,
          self._conversion_params.maximum_cached_engines)
            (: (
            identifier: canonical_engine_name
            ,: ,
            identifier: filename
            ,: ,
            attribute: self._conversion_params.maximum_cached_engines
             attribute: self._conversion_params
              identifier: self
              .: .
              identifier: _conversion_params
             .: .
             identifier: maximum_cached_engines
            ): )
      expression_statement: self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
       call: self._for_each_trt_node(self._converted_graph_def,
                            _serialize_and_track_engine)
        attribute: self._for_each_trt_node
         identifier: self
         .: .
         identifier: _for_each_trt_node
        argument_list: (self._converted_graph_def,
                            _serialize_and_track_engine)
         (: (
         attribute: self._converted_graph_def
          identifier: self
          .: .
          identifier: _converted_graph_def
         ,: ,
         identifier: _serialize_and_track_engine
         ): )
      comment: # If the graph is frozen, tracked variables are not needed by the converted model.
      expression_statement: trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
       assignment: trackable = autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
        identifier: trackable
        =: =
        conditional_expression: autotrackable.AutoTrackable(
    ) if self.freeze else self._saved_model
         call: autotrackable.AutoTrackable(
    )
          attribute: autotrackable.AutoTrackable
           identifier: autotrackable
           .: .
           identifier: AutoTrackable
          argument_list: (
    )
           (: (
           ): )
         if: if
         attribute: self.freeze
          identifier: self
          .: .
          identifier: freeze
         else: else
         attribute: self._saved_model
          identifier: self
          .: .
          identifier: _saved_model
      expression_statement: trackable.trt_engine_resources = resource_map
       assignment: trackable.trt_engine_resources = resource_map
        attribute: trackable.trt_engine_resources
         identifier: trackable
         .: .
         identifier: trt_engine_resources
        =: =
        identifier: resource_map
      comment: # Set allow_build_at_runtime=False if asked by user.
      comment: #
      comment: # This attribute is set here because build() needs it to be True in order to
      comment: # build engines.
      if_statement: if not self._conversion_params.allow_build_at_runtime:

      def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func
       if: if
       not_operator: not self._conversion_params.allow_build_at_runtime
        not: not
        attribute: self._conversion_params.allow_build_at_runtime
         attribute: self._conversion_params
          identifier: self
          .: .
          identifier: _conversion_params
         .: .
         identifier: allow_build_at_runtime
       :: :
       block: def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False

      self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
      # Rebuild the function since a node attribute changed above
      reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
      reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
      reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
      self._converted_func = reset_converted_func
        function_definition: def _reset_allow_build_at_runtime(node):
        node.attr["_allow_build_at_runtime"].b = False
         def: def
         identifier: _reset_allow_build_at_runtime
         parameters: (node)
          (: (
          identifier: node
          ): )
         :: :
         block: node.attr["_allow_build_at_runtime"].b = False
          expression_statement: node.attr["_allow_build_at_runtime"].b = False
           assignment: node.attr["_allow_build_at_runtime"].b = False
            attribute: node.attr["_allow_build_at_runtime"].b
             subscript: node.attr["_allow_build_at_runtime"]
              attribute: node.attr
               identifier: node
               .: .
               identifier: attr
              [: [
              string: "_allow_build_at_runtime"
               string_start: "
               string_content: _allow_build_at_runtime
               string_end: "
              ]: ]
             .: .
             identifier: b
            =: =
            false: False
        expression_statement: self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
         call: self._for_each_trt_node(self._converted_graph_def,
                              _reset_allow_build_at_runtime)
          attribute: self._for_each_trt_node
           identifier: self
           .: .
           identifier: _for_each_trt_node
          argument_list: (self._converted_graph_def,
                              _reset_allow_build_at_runtime)
           (: (
           attribute: self._converted_graph_def
            identifier: self
            .: .
            identifier: _converted_graph_def
           ,: ,
           identifier: _reset_allow_build_at_runtime
           ): )
        comment: # Rebuild the function since a node attribute changed above
        expression_statement: reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
         assignment: reset_converted_func = wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
          identifier: reset_converted_func
          =: =
          call: wrap_function.function_from_graph_def(
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
           attribute: wrap_function.function_from_graph_def
            identifier: wrap_function
            .: .
            identifier: function_from_graph_def
           argument_list: (
          self._converted_graph_def,
          [tensor.name for tensor in self._converted_func.inputs],
          [tensor.name for tensor in self._converted_func.outputs])
            (: (
            attribute: self._converted_graph_def
             identifier: self
             .: .
             identifier: _converted_graph_def
            ,: ,
            list_comprehension: [tensor.name for tensor in self._converted_func.inputs]
             [: [
             attribute: tensor.name
              identifier: tensor
              .: .
              identifier: name
             for_in_clause: for tensor in self._converted_func.inputs
              for: for
              identifier: tensor
              in: in
              attribute: self._converted_func.inputs
               attribute: self._converted_func
                identifier: self
                .: .
                identifier: _converted_func
               .: .
               identifier: inputs
             ]: ]
            ,: ,
            list_comprehension: [tensor.name for tensor in self._converted_func.outputs]
             [: [
             attribute: tensor.name
              identifier: tensor
              .: .
              identifier: name
             for_in_clause: for tensor in self._converted_func.outputs
              for: for
              identifier: tensor
              in: in
              attribute: self._converted_func.outputs
               attribute: self._converted_func
                identifier: self
                .: .
                identifier: _converted_func
               .: .
               identifier: outputs
             ]: ]
            ): )
        expression_statement: reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
         assignment: reset_converted_func.graph.structured_outputs = nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
          attribute: reset_converted_func.graph.structured_outputs
           attribute: reset_converted_func.graph
            identifier: reset_converted_func
            .: .
            identifier: graph
           .: .
           identifier: structured_outputs
          =: =
          call: nest.pack_sequence_as(
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
           attribute: nest.pack_sequence_as
            identifier: nest
            .: .
            identifier: pack_sequence_as
           argument_list: (
          self._converted_func.graph.structured_outputs,
          reset_converted_func.graph.structured_outputs)
            (: (
            attribute: self._converted_func.graph.structured_outputs
             attribute: self._converted_func.graph
              attribute: self._converted_func
               identifier: self
               .: .
               identifier: _converted_func
              .: .
              identifier: graph
             .: .
             identifier: structured_outputs
            ,: ,
            attribute: reset_converted_func.graph.structured_outputs
             attribute: reset_converted_func.graph
              identifier: reset_converted_func
              .: .
              identifier: graph
             .: .
             identifier: structured_outputs
            ): )
        expression_statement: reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
         assignment: reset_converted_func.graph.structured_input_signature = (
          self._converted_func.structured_input_signature)
          attribute: reset_converted_func.graph.structured_input_signature
           attribute: reset_converted_func.graph
            identifier: reset_converted_func
            .: .
            identifier: graph
           .: .
           identifier: structured_input_signature
          =: =
          parenthesized_expression: (
          self._converted_func.structured_input_signature)
           (: (
           attribute: self._converted_func.structured_input_signature
            attribute: self._converted_func
             identifier: self
             .: .
             identifier: _converted_func
            .: .
            identifier: structured_input_signature
           ): )
        expression_statement: self._converted_func = reset_converted_func
         assignment: self._converted_func = reset_converted_func
          attribute: self._converted_func
           identifier: self
           .: .
           identifier: _converted_func
          =: =
          identifier: reset_converted_func
      comment: # Rewrite the signature map using the optimized ConcreteFunction.
      expression_statement: signatures = {self._input_saved_model_signature_key: self._converted_func}
       assignment: signatures = {self._input_saved_model_signature_key: self._converted_func}
        identifier: signatures
        =: =
        dictionary: {self._input_saved_model_signature_key: self._converted_func}
         {: {
         pair: self._input_saved_model_signature_key: self._converted_func
          attribute: self._input_saved_model_signature_key
           identifier: self
           .: .
           identifier: _input_saved_model_signature_key
          :: :
          attribute: self._converted_func
           identifier: self
           .: .
           identifier: _converted_func
         }: }
      expression_statement: save.save(trackable, output_saved_model_dir, signatures, options=options)
       call: save.save(trackable, output_saved_model_dir, signatures, options=options)
        attribute: save.save
         identifier: save
         .: .
         identifier: save
        argument_list: (trackable, output_saved_model_dir, signatures, options=options)
         (: (
         identifier: trackable
         ,: ,
         identifier: output_saved_model_dir
         ,: ,
         identifier: signatures
         ,: ,
         keyword_argument: options=options
          identifier: options
          =: =
          identifier: options
         ): )
    function_definition: def summary(self, line_length=160, detailed=True, print_fn=None):
    """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
    if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")

    if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")

    if print_fn is None:
      print_fn = print

    # positions are percentage of `line_length`. positions[i]+1 is the starting
    # position for (i+1)th field. We also make sure that the last char printed
    # for each field is a space.
    columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]

    positions = [int(line_length * p) for _, p in columns]
    positions = np.cumsum(positions).tolist()
    headers = [h for h, _ in columns]

    _print_row(headers, positions, print_fn=print_fn)
    print_fn("=" * line_length)

    n_engines = 0
    n_ops_converted = 0
    n_ops_not_converted = 0

    graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)

    trtengineops_dict = dict()
    for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1

    for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")

    print_fn(f"\n{'='*line_length}")
    print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
    total_ops = n_ops_not_converted + n_ops_converted
    conversion_ratio = n_ops_converted / total_ops * 100
    print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
     def: def
     identifier: summary
     parameters: (self, line_length=160, detailed=True, print_fn=None)
      (: (
      identifier: self
      ,: ,
      default_parameter: line_length=160
       identifier: line_length
       =: =
       integer: 160
      ,: ,
      default_parameter: detailed=True
       identifier: detailed
       =: =
       true: True
      ,: ,
      default_parameter: print_fn=None
       identifier: print_fn
       =: =
       none: None
      ): )
     :: :
     block: """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
    if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")

    if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")

    if print_fn is None:
      print_fn = print

    # positions are percentage of `line_length`. positions[i]+1 is the starting
    # position for (i+1)th field. We also make sure that the last char printed
    # for each field is a space.
    columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]

    positions = [int(line_length * p) for _, p in columns]
    positions = np.cumsum(positions).tolist()
    headers = [h for h, _ in columns]

    _print_row(headers, positions, print_fn=print_fn)
    print_fn("=" * line_length)

    n_engines = 0
    n_ops_converted = 0
    n_ops_not_converted = 0

    graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)

    trtengineops_dict = dict()
    for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1

    for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")

    print_fn(f"\n{'='*line_length}")
    print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
    total_ops = n_ops_not_converted + n_ops_converted
    conversion_ratio = n_ops_converted / total_ops * 100
    print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
      expression_statement: """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
       string: """This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    """
        string_start: """
        string_content: This method describes the results of the conversion by TF-TRT.

    It includes information such as the name of the engine, the number of nodes
    per engine, the input and output dtype, along with the input shape of each
    TRTEngineOp.

    Args:
      line_length: Default line length when printing on the console. Minimum 160
        characters long.
      detailed: Whether or not to show the nodes inside each TRTEngineOp.
      print_fn: Print function to use. Defaults to `print`. It will be called on
        each line of the summary. You can set it to a custom function in order
        to capture the string summary.

    Raises:
      RuntimeError: if the graph is not converted.
    
        string_end: """
      if_statement: if not self._converted:
      raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")
       if: if
       not_operator: not self._converted
        not: not
        attribute: self._converted
         identifier: self
         .: .
         identifier: _converted
       :: :
       block: raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")
        raise_statement: raise RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")
         raise: raise
         call: RuntimeError(
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")
          identifier: RuntimeError
          argument_list: (
          f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`.")
           (: (
           concatenated_string: f"Impossible to call `{self.__class__.__name__}.summary()` before "
          f"calling {self.__class__.__name__}.convert()`."
            string: f"Impossible to call `{self.__class__.__name__}.summary()` before "
             string_start: f"
             string_content: Impossible to call `
             interpolation: {self.__class__.__name__}
              {: {
              attribute: self.__class__.__name__
               attribute: self.__class__
                identifier: self
                .: .
                identifier: __class__
               .: .
               identifier: __name__
              }: }
             string_content: .summary()` before 
             string_end: "
            string: f"calling {self.__class__.__name__}.convert()`."
             string_start: f"
             string_content: calling 
             interpolation: {self.__class__.__name__}
              {: {
              attribute: self.__class__.__name__
               attribute: self.__class__
                identifier: self
                .: .
                identifier: __class__
               .: .
               identifier: __name__
              }: }
             string_content: .convert()`.
             string_end: "
           ): )
      if_statement: if line_length < 160:
      raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")
       if: if
       comparison_operator: line_length < 160
        identifier: line_length
        <: <
        integer: 160
       :: :
       block: raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")
        raise_statement: raise ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")
         raise: raise
         call: ValueError(f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")
          identifier: ValueError
          argument_list: (f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160.")
           (: (
           concatenated_string: f"Invalid `line_length` value has been received: "
                       f"{line_length}. Minimum: 160."
            string: f"Invalid `line_length` value has been received: "
             string_start: f"
             string_content: Invalid `line_length` value has been received: 
             string_end: "
            string: f"{line_length}. Minimum: 160."
             string_start: f"
             interpolation: {line_length}
              {: {
              identifier: line_length
              }: }
             string_content: . Minimum: 160.
             string_end: "
           ): )
      if_statement: if print_fn is None:
      print_fn = print
       if: if
       comparison_operator: print_fn is None
        identifier: print_fn
        is: is
        none: None
       :: :
       block: print_fn = print
        expression_statement: print_fn = print
         assignment: print_fn = print
          identifier: print_fn
          =: =
          identifier: print
      comment: # positions are percentage of `line_length`. positions[i]+1 is the starting
      comment: # position for (i+1)th field. We also make sure that the last char printed
      comment: # for each field is a space.
      expression_statement: columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]
       assignment: columns = [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]
        identifier: columns
        =: =
        list: [
        # (column name, column size in % of line)
        ("TRTEngineOP Name", .20),  # 20%
        ("Device", .09),  # 29%
        ("# Nodes", .05),  # 34%
        ("# Inputs", .09),  # 43%
        ("# Outputs", .09),  # 52%
        ("Input DTypes", .12),  # 64%
        ("Output Dtypes", .12),  # 76%
        ("Input Shapes", .12),  # 88%
        ("Output Shapes", .12)  # 100%
    ]
         [: [
         comment: # (column name, column size in % of line)
         tuple: ("TRTEngineOP Name", .20)
          (: (
          string: "TRTEngineOP Name"
           string_start: "
           string_content: TRTEngineOP Name
           string_end: "
          ,: ,
          float: .20
          ): )
         ,: ,
         comment: # 20%
         tuple: ("Device", .09)
          (: (
          string: "Device"
           string_start: "
           string_content: Device
           string_end: "
          ,: ,
          float: .09
          ): )
         ,: ,
         comment: # 29%
         tuple: ("# Nodes", .05)
          (: (
          string: "# Nodes"
           string_start: "
           string_content: # Nodes
           string_end: "
          ,: ,
          float: .05
          ): )
         ,: ,
         comment: # 34%
         tuple: ("# Inputs", .09)
          (: (
          string: "# Inputs"
           string_start: "
           string_content: # Inputs
           string_end: "
          ,: ,
          float: .09
          ): )
         ,: ,
         comment: # 43%
         tuple: ("# Outputs", .09)
          (: (
          string: "# Outputs"
           string_start: "
           string_content: # Outputs
           string_end: "
          ,: ,
          float: .09
          ): )
         ,: ,
         comment: # 52%
         tuple: ("Input DTypes", .12)
          (: (
          string: "Input DTypes"
           string_start: "
           string_content: Input DTypes
           string_end: "
          ,: ,
          float: .12
          ): )
         ,: ,
         comment: # 64%
         tuple: ("Output Dtypes", .12)
          (: (
          string: "Output Dtypes"
           string_start: "
           string_content: Output Dtypes
           string_end: "
          ,: ,
          float: .12
          ): )
         ,: ,
         comment: # 76%
         tuple: ("Input Shapes", .12)
          (: (
          string: "Input Shapes"
           string_start: "
           string_content: Input Shapes
           string_end: "
          ,: ,
          float: .12
          ): )
         ,: ,
         comment: # 88%
         tuple: ("Output Shapes", .12)
          (: (
          string: "Output Shapes"
           string_start: "
           string_content: Output Shapes
           string_end: "
          ,: ,
          float: .12
          ): )
         comment: # 100%
         ]: ]
      expression_statement: positions = [int(line_length * p) for _, p in columns]
       assignment: positions = [int(line_length * p) for _, p in columns]
        identifier: positions
        =: =
        list_comprehension: [int(line_length * p) for _, p in columns]
         [: [
         call: int(line_length * p)
          identifier: int
          argument_list: (line_length * p)
           (: (
           binary_operator: line_length * p
            identifier: line_length
            *: *
            identifier: p
           ): )
         for_in_clause: for _, p in columns
          for: for
          pattern_list: _, p
           identifier: _
           ,: ,
           identifier: p
          in: in
          identifier: columns
         ]: ]
      expression_statement: positions = np.cumsum(positions).tolist()
       assignment: positions = np.cumsum(positions).tolist()
        identifier: positions
        =: =
        call: np.cumsum(positions).tolist()
         attribute: np.cumsum(positions).tolist
          call: np.cumsum(positions)
           attribute: np.cumsum
            identifier: np
            .: .
            identifier: cumsum
           argument_list: (positions)
            (: (
            identifier: positions
            ): )
          .: .
          identifier: tolist
         argument_list: ()
          (: (
          ): )
      expression_statement: headers = [h for h, _ in columns]
       assignment: headers = [h for h, _ in columns]
        identifier: headers
        =: =
        list_comprehension: [h for h, _ in columns]
         [: [
         identifier: h
         for_in_clause: for h, _ in columns
          for: for
          pattern_list: h, _
           identifier: h
           ,: ,
           identifier: _
          in: in
          identifier: columns
         ]: ]
      expression_statement: _print_row(headers, positions, print_fn=print_fn)
       call: _print_row(headers, positions, print_fn=print_fn)
        identifier: _print_row
        argument_list: (headers, positions, print_fn=print_fn)
         (: (
         identifier: headers
         ,: ,
         identifier: positions
         ,: ,
         keyword_argument: print_fn=print_fn
          identifier: print_fn
          =: =
          identifier: print_fn
         ): )
      expression_statement: print_fn("=" * line_length)
       call: print_fn("=" * line_length)
        identifier: print_fn
        argument_list: ("=" * line_length)
         (: (
         binary_operator: "=" * line_length
          string: "="
           string_start: "
           string_content: =
           string_end: "
          *: *
          identifier: line_length
         ): )
      expression_statement: n_engines = 0
       assignment: n_engines = 0
        identifier: n_engines
        =: =
        integer: 0
      expression_statement: n_ops_converted = 0
       assignment: n_ops_converted = 0
        identifier: n_ops_converted
        =: =
        integer: 0
      expression_statement: n_ops_not_converted = 0
       assignment: n_ops_not_converted = 0
        identifier: n_ops_not_converted
        =: =
        integer: 0
      expression_statement: graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)
       assignment: graphdef = self._converted_func.graph.as_graph_def(add_shapes=True)
        identifier: graphdef
        =: =
        call: self._converted_func.graph.as_graph_def(add_shapes=True)
         attribute: self._converted_func.graph.as_graph_def
          attribute: self._converted_func.graph
           attribute: self._converted_func
            identifier: self
            .: .
            identifier: _converted_func
           .: .
           identifier: graph
          .: .
          identifier: as_graph_def
         argument_list: (add_shapes=True)
          (: (
          keyword_argument: add_shapes=True
           identifier: add_shapes
           =: =
           true: True
          ): )
      expression_statement: trtengineops_dict = dict()
       assignment: trtengineops_dict = dict()
        identifier: trtengineops_dict
        =: =
        call: dict()
         identifier: dict
         argument_list: ()
          (: (
          ): )
      for_statement: for node in graphdef.node:
      if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1
       for: for
       identifier: node
       in: in
       attribute: graphdef.node
        identifier: graphdef
        .: .
        identifier: node
       :: :
       block: if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1
        if_statement: if node.op != "TRTEngineOp":
        n_ops_not_converted += 1
        continue
      else:
        trtengineops_dict[node.name] = node
        n_engines += 1
         if: if
         comparison_operator: node.op != "TRTEngineOp"
          attribute: node.op
           identifier: node
           .: .
           identifier: op
          !=: !=
          string: "TRTEngineOp"
           string_start: "
           string_content: TRTEngineOp
           string_end: "
         :: :
         block: n_ops_not_converted += 1
        continue
          expression_statement: n_ops_not_converted += 1
           augmented_assignment: n_ops_not_converted += 1
            identifier: n_ops_not_converted
            +=: +=
            integer: 1
          continue_statement: continue
           continue: continue
         else_clause: else:
        trtengineops_dict[node.name] = node
        n_engines += 1
          else: else
          :: :
          block: trtengineops_dict[node.name] = node
        n_engines += 1
           expression_statement: trtengineops_dict[node.name] = node
            assignment: trtengineops_dict[node.name] = node
             subscript: trtengineops_dict[node.name]
              identifier: trtengineops_dict
              [: [
              attribute: node.name
               identifier: node
               .: .
               identifier: name
              ]: ]
             =: =
             identifier: node
           expression_statement: n_engines += 1
            augmented_assignment: n_engines += 1
             identifier: n_engines
             +=: +=
             integer: 1
      for_statement: for name, node in sorted(trtengineops_dict.items()):
      node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")
       for: for
       pattern_list: name, node
        identifier: name
        ,: ,
        identifier: node
       in: in
       call: sorted(trtengineops_dict.items())
        identifier: sorted
        argument_list: (trtengineops_dict.items())
         (: (
         call: trtengineops_dict.items()
          attribute: trtengineops_dict.items
           identifier: trtengineops_dict
           .: .
           identifier: items
          argument_list: ()
           (: (
           ): )
         ): )
       :: :
       block: node_device = node.device.split("/")[-1]
      in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
      out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
      in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
      out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
      in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
      out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
      node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)

      n_ops_converted += node_count

      if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")

      _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)

      if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")
        expression_statement: node_device = node.device.split("/")[-1]
         assignment: node_device = node.device.split("/")[-1]
          identifier: node_device
          =: =
          subscript: node.device.split("/")[-1]
           call: node.device.split("/")
            attribute: node.device.split
             attribute: node.device
              identifier: node
              .: .
              identifier: device
             .: .
             identifier: split
            argument_list: ("/")
             (: (
             string: "/"
              string_start: "
              string_content: /
              string_end: "
             ): )
           [: [
           unary_operator: -1
            -: -
            integer: 1
           ]: ]
        expression_statement: in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
         assignment: in_shapes = trt_utils.get_node_io_shapes(node, "input_shapes")
          identifier: in_shapes
          =: =
          call: trt_utils.get_node_io_shapes(node, "input_shapes")
           attribute: trt_utils.get_node_io_shapes
            identifier: trt_utils
            .: .
            identifier: get_node_io_shapes
           argument_list: (node, "input_shapes")
            (: (
            identifier: node
            ,: ,
            string: "input_shapes"
             string_start: "
             string_content: input_shapes
             string_end: "
            ): )
        expression_statement: out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
         assignment: out_shapes = trt_utils.get_node_io_shapes(node, "_output_shapes")
          identifier: out_shapes
          =: =
          call: trt_utils.get_node_io_shapes(node, "_output_shapes")
           attribute: trt_utils.get_node_io_shapes
            identifier: trt_utils
            .: .
            identifier: get_node_io_shapes
           argument_list: (node, "_output_shapes")
            (: (
            identifier: node
            ,: ,
            string: "_output_shapes"
             string_start: "
             string_content: _output_shapes
             string_end: "
            ): )
        expression_statement: in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
         assignment: in_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "InT")
          identifier: in_dtypes
          =: =
          call: trt_utils.get_trtengineop_io_dtypes(node, "InT")
           attribute: trt_utils.get_trtengineop_io_dtypes
            identifier: trt_utils
            .: .
            identifier: get_trtengineop_io_dtypes
           argument_list: (node, "InT")
            (: (
            identifier: node
            ,: ,
            string: "InT"
             string_start: "
             string_content: InT
             string_end: "
            ): )
        expression_statement: out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
         assignment: out_dtypes = trt_utils.get_trtengineop_io_dtypes(node, "OutT")
          identifier: out_dtypes
          =: =
          call: trt_utils.get_trtengineop_io_dtypes(node, "OutT")
           attribute: trt_utils.get_trtengineop_io_dtypes
            identifier: trt_utils
            .: .
            identifier: get_trtengineop_io_dtypes
           argument_list: (node, "OutT")
            (: (
            identifier: node
            ,: ,
            string: "OutT"
             string_start: "
             string_content: OutT
             string_end: "
            ): )
        expression_statement: in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
         assignment: in_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "InT")
          identifier: in_nodes_count
          =: =
          call: trt_utils.get_trtengineop_io_nodes_count(node, "InT")
           attribute: trt_utils.get_trtengineop_io_nodes_count
            identifier: trt_utils
            .: .
            identifier: get_trtengineop_io_nodes_count
           argument_list: (node, "InT")
            (: (
            identifier: node
            ,: ,
            string: "InT"
             string_start: "
             string_content: InT
             string_end: "
            ): )
        expression_statement: out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
         assignment: out_nodes_count = trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
          identifier: out_nodes_count
          =: =
          call: trt_utils.get_trtengineop_io_nodes_count(node, "OutT")
           attribute: trt_utils.get_trtengineop_io_nodes_count
            identifier: trt_utils
            .: .
            identifier: get_trtengineop_io_nodes_count
           argument_list: (node, "OutT")
            (: (
            identifier: node
            ,: ,
            string: "OutT"
             string_start: "
             string_content: OutT
             string_end: "
            ): )
        expression_statement: node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)
         assignment: node_count, converted_ops_dict = trt_utils.get_trtengineop_node_op_count(
          graphdef, name)
          pattern_list: node_count, converted_ops_dict
           identifier: node_count
           ,: ,
           identifier: converted_ops_dict
          =: =
          call: trt_utils.get_trtengineop_node_op_count(
          graphdef, name)
           attribute: trt_utils.get_trtengineop_node_op_count
            identifier: trt_utils
            .: .
            identifier: get_trtengineop_node_op_count
           argument_list: (
          graphdef, name)
            (: (
            identifier: graphdef
            ,: ,
            identifier: name
            ): )
        expression_statement: n_ops_converted += node_count
         augmented_assignment: n_ops_converted += node_count
          identifier: n_ops_converted
          +=: +=
          identifier: node_count
        if_statement: if n_engines != 1:
        print_fn(f"\n{'-'*40}\n")
         if: if
         comparison_operator: n_engines != 1
          identifier: n_engines
          !=: !=
          integer: 1
         :: :
         block: print_fn(f"\n{'-'*40}\n")
          expression_statement: print_fn(f"\n{'-'*40}\n")
           call: print_fn(f"\n{'-'*40}\n")
            identifier: print_fn
            argument_list: (f"\n{'-'*40}\n")
             (: (
             string: f"\n{'-'*40}\n"
              string_start: f"
              string_content: \n
               escape_sequence: \n
              interpolation: {'-'*40}
               {: {
               binary_operator: '-'*40
                string: '-'
                 string_start: '
                 string_content: -
                 string_end: '
                *: *
                integer: 40
               }: }
              string_content: \n
               escape_sequence: \n
              string_end: "
             ): )
        expression_statement: _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)
         call: _print_row(
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)
          identifier: _print_row
          argument_list: (
          fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ],
          positions=positions,
          print_fn=print_fn)
           (: (
           keyword_argument: fields=[
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ]
            identifier: fields
            =: =
            list: [
              name, node_device, node_count, in_nodes_count, out_nodes_count,
              in_dtypes, out_dtypes, in_shapes, out_shapes
          ]
             [: [
             identifier: name
             ,: ,
             identifier: node_device
             ,: ,
             identifier: node_count
             ,: ,
             identifier: in_nodes_count
             ,: ,
             identifier: out_nodes_count
             ,: ,
             identifier: in_dtypes
             ,: ,
             identifier: out_dtypes
             ,: ,
             identifier: in_shapes
             ,: ,
             identifier: out_shapes
             ]: ]
           ,: ,
           keyword_argument: positions=positions
            identifier: positions
            =: =
            identifier: positions
           ,: ,
           keyword_argument: print_fn=print_fn
            identifier: print_fn
            =: =
            identifier: print_fn
           ): )
        if_statement: if detailed:
        print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")
         if: if
         identifier: detailed
         :: :
         block: print_fn()
        for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")
          expression_statement: print_fn()
           call: print_fn()
            identifier: print_fn
            argument_list: ()
             (: (
             ): )
          for_statement: for key, value in sorted(dict(converted_ops_dict).items()):
          print_fn(f"\t- {key}: {value}x")
           for: for
           pattern_list: key, value
            identifier: key
            ,: ,
            identifier: value
           in: in
           call: sorted(dict(converted_ops_dict).items())
            identifier: sorted
            argument_list: (dict(converted_ops_dict).items())
             (: (
             call: dict(converted_ops_dict).items()
              attribute: dict(converted_ops_dict).items
               call: dict(converted_ops_dict)
                identifier: dict
                argument_list: (converted_ops_dict)
                 (: (
                 identifier: converted_ops_dict
                 ): )
               .: .
               identifier: items
              argument_list: ()
               (: (
               ): )
             ): )
           :: :
           block: print_fn(f"\t- {key}: {value}x")
            expression_statement: print_fn(f"\t- {key}: {value}x")
             call: print_fn(f"\t- {key}: {value}x")
              identifier: print_fn
              argument_list: (f"\t- {key}: {value}x")
               (: (
               string: f"\t- {key}: {value}x"
                string_start: f"
                string_content: \t- 
                 escape_sequence: \t
                interpolation: {key}
                 {: {
                 identifier: key
                 }: }
                string_content: : 
                interpolation: {value}
                 {: {
                 identifier: value
                 }: }
                string_content: x
                string_end: "
               ): )
      expression_statement: print_fn(f"\n{'='*line_length}")
       call: print_fn(f"\n{'='*line_length}")
        identifier: print_fn
        argument_list: (f"\n{'='*line_length}")
         (: (
         string: f"\n{'='*line_length}"
          string_start: f"
          string_content: \n
           escape_sequence: \n
          interpolation: {'='*line_length}
           {: {
           binary_operator: '='*line_length
            string: '='
             string_start: '
             string_content: =
             string_end: '
            *: *
            identifier: line_length
           }: }
          string_end: "
         ): )
      expression_statement: print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
       call: print_fn(f"[*] Total number of TensorRT engines: {n_engines}")
        identifier: print_fn
        argument_list: (f"[*] Total number of TensorRT engines: {n_engines}")
         (: (
         string: f"[*] Total number of TensorRT engines: {n_engines}"
          string_start: f"
          string_content: [*] Total number of TensorRT engines: 
          interpolation: {n_engines}
           {: {
           identifier: n_engines
           }: }
          string_end: "
         ): )
      expression_statement: total_ops = n_ops_not_converted + n_ops_converted
       assignment: total_ops = n_ops_not_converted + n_ops_converted
        identifier: total_ops
        =: =
        binary_operator: n_ops_not_converted + n_ops_converted
         identifier: n_ops_not_converted
         +: +
         identifier: n_ops_converted
      expression_statement: conversion_ratio = n_ops_converted / total_ops * 100
       assignment: conversion_ratio = n_ops_converted / total_ops * 100
        identifier: conversion_ratio
        =: =
        binary_operator: n_ops_converted / total_ops * 100
         binary_operator: n_ops_converted / total_ops
          identifier: n_ops_converted
          /: /
          identifier: total_ops
         *: *
         integer: 100
      expression_statement: print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
       call: print_fn(f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
        identifier: print_fn
        argument_list: (f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n")
         (: (
         concatenated_string: f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
             f"[{n_ops_converted}/{total_ops}]\n"
          string: f"[*] % of OPs Converted: {conversion_ratio:.2f}% "
           string_start: f"
           string_content: [*] % of OPs Converted: 
           interpolation: {conversion_ratio:.2f}
            {: {
            identifier: conversion_ratio
            format_specifier: :.2f
             :: :
            }: }
           string_content: % 
           string_end: "
          string: f"[{n_ops_converted}/{total_ops}]\n"
           string_start: f"
           string_content: [
           interpolation: {n_ops_converted}
            {: {
            identifier: n_ops_converted
            }: }
           string_content: /
           interpolation: {total_ops}
            {: {
            identifier: total_ops
            }: }
           string_content: ]\n
            escape_sequence: \n
           string_end: "
         ): )
 comment: # TODO(laigd): use TrtConversionParams here.
 function_definition: def create_inference_graph(
    input_graph_def,
    outputs,
    max_batch_size=1,
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32,
    minimum_segment_size=3,
    is_dynamic_op=False,
    maximum_cached_engines=1,
    input_saved_model_dir=None,
    input_saved_model_tags=None,
    input_saved_model_signature_key=None,
    output_saved_model_dir=None):
  """Python wrapper for the TRT transformation.

  Args:
    input_graph_def: a GraphDef object containing a model to be transformed. If
      set to None, the graph will be read from the SavedModel loaded from
      input_saved_model_dir.
    outputs: list of tensors or node names for the model outputs. Only used when
      input_graph_def is not None.
    max_batch_size: max size for the input batch.
    max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
      engine can use at execution time. This corresponds to the 'workspaceSize'
      parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph to
      be replaced by TRTEngineOp.
    is_dynamic_op: whether to generate dynamic TRT ops which will build the TRT
      network and engine at run time.
    maximum_cached_engines: max number of cached TRT engines in dynamic TRT ops.
      If the number of cached engines is already at max but none of them can
      serve the input, the TRTEngineOp will fall back to run the TF function
      based on which the TRTEngineOp is created.
    input_saved_model_dir: the directory to load the SavedModel which contains
      the input graph to transforms. Used only when input_graph_def is None.
    input_saved_model_tags: list of tags to load the SavedModel.
    input_saved_model_signature_key: the key of the signature to optimize the
      graph for.
    output_saved_model_dir: if not None, construct a SavedModel using the
      returned GraphDef and save it to the specified directory. This option only
      works when the input graph is loaded from a SavedModel, i.e. when
      input_saved_model_dir is specified and input_graph_def is None.

  Returns:
    A GraphDef transformed from input_graph_def (or the SavedModel graph def
    loaded from input_saved_model_dir, if input_graph_def is not present), where
    all TRT compatible subgraphs are replaced with TRTEngineOps, and a TF
    function is added for each of the subgraphs.

    If is_dynamic_op is True, each TRTEngineOp will contain a serialized
    subgraph GraphDef, which will be converted to a TRT engine at execution time
    and the TRT engine will be cached for future usage. A new TRT engine will be
    created each time when none of the cached engines match the input shapes. If
    it fails to execute the TRT engine or the number of cached engines reaches
    maximum_cached_engines, the op will fall back to call the corresponding TF
    function.

    If is_dynamic_op is False, each TRTEngineOp will contain a serialized TRT
    engine created from the corresponding subgraph. No more engines will be
    created on the fly, and the op will fall back to call the corresponding TF
    function when it fails to execute the engine.

  Raises:
    ValueError: if the combination of the parameters is invalid.
  """
  trt_converter = TrtGraphConverter(
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
  converted_graph_def = trt_converter.convert()
  if output_saved_model_dir:
    trt_converter.save(output_saved_model_dir)
  return converted_graph_def
  def: def
  identifier: create_inference_graph
  parameters: (
    input_graph_def,
    outputs,
    max_batch_size=1,
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32,
    minimum_segment_size=3,
    is_dynamic_op=False,
    maximum_cached_engines=1,
    input_saved_model_dir=None,
    input_saved_model_tags=None,
    input_saved_model_signature_key=None,
    output_saved_model_dir=None)
   (: (
   identifier: input_graph_def
   ,: ,
   identifier: outputs
   ,: ,
   default_parameter: max_batch_size=1
    identifier: max_batch_size
    =: =
    integer: 1
   ,: ,
   default_parameter: max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
    identifier: max_workspace_size_bytes
    =: =
    identifier: DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES
   ,: ,
   default_parameter: precision_mode=TrtPrecisionMode.FP32
    identifier: precision_mode
    =: =
    attribute: TrtPrecisionMode.FP32
     identifier: TrtPrecisionMode
     .: .
     identifier: FP32
   ,: ,
   default_parameter: minimum_segment_size=3
    identifier: minimum_segment_size
    =: =
    integer: 3
   ,: ,
   default_parameter: is_dynamic_op=False
    identifier: is_dynamic_op
    =: =
    false: False
   ,: ,
   default_parameter: maximum_cached_engines=1
    identifier: maximum_cached_engines
    =: =
    integer: 1
   ,: ,
   default_parameter: input_saved_model_dir=None
    identifier: input_saved_model_dir
    =: =
    none: None
   ,: ,
   default_parameter: input_saved_model_tags=None
    identifier: input_saved_model_tags
    =: =
    none: None
   ,: ,
   default_parameter: input_saved_model_signature_key=None
    identifier: input_saved_model_signature_key
    =: =
    none: None
   ,: ,
   default_parameter: output_saved_model_dir=None
    identifier: output_saved_model_dir
    =: =
    none: None
   ): )
  :: :
  block: """Python wrapper for the TRT transformation.

  Args:
    input_graph_def: a GraphDef object containing a model to be transformed. If
      set to None, the graph will be read from the SavedModel loaded from
      input_saved_model_dir.
    outputs: list of tensors or node names for the model outputs. Only used when
      input_graph_def is not None.
    max_batch_size: max size for the input batch.
    max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
      engine can use at execution time. This corresponds to the 'workspaceSize'
      parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph to
      be replaced by TRTEngineOp.
    is_dynamic_op: whether to generate dynamic TRT ops which will build the TRT
      network and engine at run time.
    maximum_cached_engines: max number of cached TRT engines in dynamic TRT ops.
      If the number of cached engines is already at max but none of them can
      serve the input, the TRTEngineOp will fall back to run the TF function
      based on which the TRTEngineOp is created.
    input_saved_model_dir: the directory to load the SavedModel which contains
      the input graph to transforms. Used only when input_graph_def is None.
    input_saved_model_tags: list of tags to load the SavedModel.
    input_saved_model_signature_key: the key of the signature to optimize the
      graph for.
    output_saved_model_dir: if not None, construct a SavedModel using the
      returned GraphDef and save it to the specified directory. This option only
      works when the input graph is loaded from a SavedModel, i.e. when
      input_saved_model_dir is specified and input_graph_def is None.

  Returns:
    A GraphDef transformed from input_graph_def (or the SavedModel graph def
    loaded from input_saved_model_dir, if input_graph_def is not present), where
    all TRT compatible subgraphs are replaced with TRTEngineOps, and a TF
    function is added for each of the subgraphs.

    If is_dynamic_op is True, each TRTEngineOp will contain a serialized
    subgraph GraphDef, which will be converted to a TRT engine at execution time
    and the TRT engine will be cached for future usage. A new TRT engine will be
    created each time when none of the cached engines match the input shapes. If
    it fails to execute the TRT engine or the number of cached engines reaches
    maximum_cached_engines, the op will fall back to call the corresponding TF
    function.

    If is_dynamic_op is False, each TRTEngineOp will contain a serialized TRT
    engine created from the corresponding subgraph. No more engines will be
    created on the fly, and the op will fall back to call the corresponding TF
    function when it fails to execute the engine.

  Raises:
    ValueError: if the combination of the parameters is invalid.
  """
  trt_converter = TrtGraphConverter(
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
  converted_graph_def = trt_converter.convert()
  if output_saved_model_dir:
    trt_converter.save(output_saved_model_dir)
  return converted_graph_def
   expression_statement: """Python wrapper for the TRT transformation.

  Args:
    input_graph_def: a GraphDef object containing a model to be transformed. If
      set to None, the graph will be read from the SavedModel loaded from
      input_saved_model_dir.
    outputs: list of tensors or node names for the model outputs. Only used when
      input_graph_def is not None.
    max_batch_size: max size for the input batch.
    max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
      engine can use at execution time. This corresponds to the 'workspaceSize'
      parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph to
      be replaced by TRTEngineOp.
    is_dynamic_op: whether to generate dynamic TRT ops which will build the TRT
      network and engine at run time.
    maximum_cached_engines: max number of cached TRT engines in dynamic TRT ops.
      If the number of cached engines is already at max but none of them can
      serve the input, the TRTEngineOp will fall back to run the TF function
      based on which the TRTEngineOp is created.
    input_saved_model_dir: the directory to load the SavedModel which contains
      the input graph to transforms. Used only when input_graph_def is None.
    input_saved_model_tags: list of tags to load the SavedModel.
    input_saved_model_signature_key: the key of the signature to optimize the
      graph for.
    output_saved_model_dir: if not None, construct a SavedModel using the
      returned GraphDef and save it to the specified directory. This option only
      works when the input graph is loaded from a SavedModel, i.e. when
      input_saved_model_dir is specified and input_graph_def is None.

  Returns:
    A GraphDef transformed from input_graph_def (or the SavedModel graph def
    loaded from input_saved_model_dir, if input_graph_def is not present), where
    all TRT compatible subgraphs are replaced with TRTEngineOps, and a TF
    function is added for each of the subgraphs.

    If is_dynamic_op is True, each TRTEngineOp will contain a serialized
    subgraph GraphDef, which will be converted to a TRT engine at execution time
    and the TRT engine will be cached for future usage. A new TRT engine will be
    created each time when none of the cached engines match the input shapes. If
    it fails to execute the TRT engine or the number of cached engines reaches
    maximum_cached_engines, the op will fall back to call the corresponding TF
    function.

    If is_dynamic_op is False, each TRTEngineOp will contain a serialized TRT
    engine created from the corresponding subgraph. No more engines will be
    created on the fly, and the op will fall back to call the corresponding TF
    function when it fails to execute the engine.

  Raises:
    ValueError: if the combination of the parameters is invalid.
  """
    string: """Python wrapper for the TRT transformation.

  Args:
    input_graph_def: a GraphDef object containing a model to be transformed. If
      set to None, the graph will be read from the SavedModel loaded from
      input_saved_model_dir.
    outputs: list of tensors or node names for the model outputs. Only used when
      input_graph_def is not None.
    max_batch_size: max size for the input batch.
    max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
      engine can use at execution time. This corresponds to the 'workspaceSize'
      parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph to
      be replaced by TRTEngineOp.
    is_dynamic_op: whether to generate dynamic TRT ops which will build the TRT
      network and engine at run time.
    maximum_cached_engines: max number of cached TRT engines in dynamic TRT ops.
      If the number of cached engines is already at max but none of them can
      serve the input, the TRTEngineOp will fall back to run the TF function
      based on which the TRTEngineOp is created.
    input_saved_model_dir: the directory to load the SavedModel which contains
      the input graph to transforms. Used only when input_graph_def is None.
    input_saved_model_tags: list of tags to load the SavedModel.
    input_saved_model_signature_key: the key of the signature to optimize the
      graph for.
    output_saved_model_dir: if not None, construct a SavedModel using the
      returned GraphDef and save it to the specified directory. This option only
      works when the input graph is loaded from a SavedModel, i.e. when
      input_saved_model_dir is specified and input_graph_def is None.

  Returns:
    A GraphDef transformed from input_graph_def (or the SavedModel graph def
    loaded from input_saved_model_dir, if input_graph_def is not present), where
    all TRT compatible subgraphs are replaced with TRTEngineOps, and a TF
    function is added for each of the subgraphs.

    If is_dynamic_op is True, each TRTEngineOp will contain a serialized
    subgraph GraphDef, which will be converted to a TRT engine at execution time
    and the TRT engine will be cached for future usage. A new TRT engine will be
    created each time when none of the cached engines match the input shapes. If
    it fails to execute the TRT engine or the number of cached engines reaches
    maximum_cached_engines, the op will fall back to call the corresponding TF
    function.

    If is_dynamic_op is False, each TRTEngineOp will contain a serialized TRT
    engine created from the corresponding subgraph. No more engines will be
    created on the fly, and the op will fall back to call the corresponding TF
    function when it fails to execute the engine.

  Raises:
    ValueError: if the combination of the parameters is invalid.
  """
     string_start: """
     string_content: Python wrapper for the TRT transformation.

  Args:
    input_graph_def: a GraphDef object containing a model to be transformed. If
      set to None, the graph will be read from the SavedModel loaded from
      input_saved_model_dir.
    outputs: list of tensors or node names for the model outputs. Only used when
      input_graph_def is not None.
    max_batch_size: max size for the input batch.
    max_workspace_size_bytes: the maximum GPU temporary memory which the TRT
      engine can use at execution time. This corresponds to the 'workspaceSize'
      parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().
    precision_mode: one of TrtPrecisionMode.supported_precision_modes().
    minimum_segment_size: the minimum number of nodes required for a subgraph to
      be replaced by TRTEngineOp.
    is_dynamic_op: whether to generate dynamic TRT ops which will build the TRT
      network and engine at run time.
    maximum_cached_engines: max number of cached TRT engines in dynamic TRT ops.
      If the number of cached engines is already at max but none of them can
      serve the input, the TRTEngineOp will fall back to run the TF function
      based on which the TRTEngineOp is created.
    input_saved_model_dir: the directory to load the SavedModel which contains
      the input graph to transforms. Used only when input_graph_def is None.
    input_saved_model_tags: list of tags to load the SavedModel.
    input_saved_model_signature_key: the key of the signature to optimize the
      graph for.
    output_saved_model_dir: if not None, construct a SavedModel using the
      returned GraphDef and save it to the specified directory. This option only
      works when the input graph is loaded from a SavedModel, i.e. when
      input_saved_model_dir is specified and input_graph_def is None.

  Returns:
    A GraphDef transformed from input_graph_def (or the SavedModel graph def
    loaded from input_saved_model_dir, if input_graph_def is not present), where
    all TRT compatible subgraphs are replaced with TRTEngineOps, and a TF
    function is added for each of the subgraphs.

    If is_dynamic_op is True, each TRTEngineOp will contain a serialized
    subgraph GraphDef, which will be converted to a TRT engine at execution time
    and the TRT engine will be cached for future usage. A new TRT engine will be
    created each time when none of the cached engines match the input shapes. If
    it fails to execute the TRT engine or the number of cached engines reaches
    maximum_cached_engines, the op will fall back to call the corresponding TF
    function.

    If is_dynamic_op is False, each TRTEngineOp will contain a serialized TRT
    engine created from the corresponding subgraph. No more engines will be
    created on the fly, and the op will fall back to call the corresponding TF
    function when it fails to execute the engine.

  Raises:
    ValueError: if the combination of the parameters is invalid.
  
     string_end: """
   expression_statement: trt_converter = TrtGraphConverter(
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
    assignment: trt_converter = TrtGraphConverter(
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
     identifier: trt_converter
     =: =
     call: TrtGraphConverter(
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
      identifier: TrtGraphConverter
      argument_list: (
      input_saved_model_dir=input_saved_model_dir,
      input_saved_model_tags=input_saved_model_tags,
      input_saved_model_signature_key=input_saved_model_signature_key,
      input_graph_def=input_graph_def,
      nodes_denylist=outputs,
      max_batch_size=max_batch_size,
      max_workspace_size_bytes=max_workspace_size_bytes,
      precision_mode=precision_mode,
      minimum_segment_size=minimum_segment_size,
      is_dynamic_op=is_dynamic_op,
      maximum_cached_engines=maximum_cached_engines,
      use_calibration=False)
       (: (
       keyword_argument: input_saved_model_dir=input_saved_model_dir
        identifier: input_saved_model_dir
        =: =
        identifier: input_saved_model_dir
       ,: ,
       keyword_argument: input_saved_model_tags=input_saved_model_tags
        identifier: input_saved_model_tags
        =: =
        identifier: input_saved_model_tags
       ,: ,
       keyword_argument: input_saved_model_signature_key=input_saved_model_signature_key
        identifier: input_saved_model_signature_key
        =: =
        identifier: input_saved_model_signature_key
       ,: ,
       keyword_argument: input_graph_def=input_graph_def
        identifier: input_graph_def
        =: =
        identifier: input_graph_def
       ,: ,
       keyword_argument: nodes_denylist=outputs
        identifier: nodes_denylist
        =: =
        identifier: outputs
       ,: ,
       keyword_argument: max_batch_size=max_batch_size
        identifier: max_batch_size
        =: =
        identifier: max_batch_size
       ,: ,
       keyword_argument: max_workspace_size_bytes=max_workspace_size_bytes
        identifier: max_workspace_size_bytes
        =: =
        identifier: max_workspace_size_bytes
       ,: ,
       keyword_argument: precision_mode=precision_mode
        identifier: precision_mode
        =: =
        identifier: precision_mode
       ,: ,
       keyword_argument: minimum_segment_size=minimum_segment_size
        identifier: minimum_segment_size
        =: =
        identifier: minimum_segment_size
       ,: ,
       keyword_argument: is_dynamic_op=is_dynamic_op
        identifier: is_dynamic_op
        =: =
        identifier: is_dynamic_op
       ,: ,
       keyword_argument: maximum_cached_engines=maximum_cached_engines
        identifier: maximum_cached_engines
        =: =
        identifier: maximum_cached_engines
       ,: ,
       keyword_argument: use_calibration=False
        identifier: use_calibration
        =: =
        false: False
       ): )
   expression_statement: converted_graph_def = trt_converter.convert()
    assignment: converted_graph_def = trt_converter.convert()
     identifier: converted_graph_def
     =: =
     call: trt_converter.convert()
      attribute: trt_converter.convert
       identifier: trt_converter
       .: .
       identifier: convert
      argument_list: ()
       (: (
       ): )
   if_statement: if output_saved_model_dir:
    trt_converter.save(output_saved_model_dir)
    if: if
    identifier: output_saved_model_dir
    :: :
    block: trt_converter.save(output_saved_model_dir)
     expression_statement: trt_converter.save(output_saved_model_dir)
      call: trt_converter.save(output_saved_model_dir)
       attribute: trt_converter.save
        identifier: trt_converter
        .: .
        identifier: save
       argument_list: (output_saved_model_dir)
        (: (
        identifier: output_saved_model_dir
        ): )
   return_statement: return converted_graph_def
    return: return
    identifier: converted_graph_def
